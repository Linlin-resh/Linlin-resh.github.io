---
title: "Reading Notes: Newman's Networks Chapter 1 - Introduction"
date: 2025-08-29
draft: false
description: "Comprehensive study notes for Chapter 1 of Newman's 'Networks: An Introduction' covering fundamental concepts, mathematical definitions, and real-world network examples"
tags: ["reading-notes", "network-theory", "mathematics", "introduction"]
showToc: true
TocOpen: true
---

## Introduction

Chapter 1 of Newman's seminal work *Networks: An Introduction* serves as the foundation for understanding network science. This chapter establishes the mathematical framework, introduces key concepts, and provides compelling real-world examples that demonstrate the ubiquity and importance of networks in our world.
我们现在把你贴的这一大段拆成三块来讲：

* **9.3.6 Example**：现实挖掘（reality mining）例子
* **9.3.7 Estimation of other quantities**：如何用 (Q_{ij}) 来算度、中心性等
* **9.3.8 Other error models**：独立边误差模型之外还能干嘛

我会继续连着前面 9.3.5 的逻辑讲，这样整段 9.3 你就有完整闭环了。

---

## 一、9.3.6 Example：现实挖掘（Reality Mining）实验

这是一个把 **“独立边误差 + EM 模型”** 真正用到数据上的例子。

### 1. 实验设计（Eagle & Pentland）

* 96 个 MIT 学生
* 每个人拿一个装了特制软件的手机
* 手机用 **蓝牙** 记录：什么时候与哪一个手机在几米之内
* 实验持续很长时间（2004–2005）

但是：

> “蓝牙靠近 ≠ 一定是社交关系”

* 他们可能只是同一楼里走过、在同一餐厅吃饭
* 真正的朋友或同学，会**反复地、多次地**出现在近距离内
* 所以：**“接近”是一个 noisy 的 proxy**，有假阳性和假阴性 → 正好适合我们刚才的 error model

### 2. 取了一个子集：8 个星期三

* 只看 2005 年 3–4 月 **连续 8 个星期三** 的数据
* 对每一天（一个 Wednesday）来说：

  * 对每人对 (i,j) ，记录当天是否 **至少一次**被观测到接近
  * 所以，一周是一轮测量，一共 **N = 8 次测量**
* 那么 (E_{ij}) 就是：这 8 天里，有多少天 (i,j) 这一对曾被观测到接近

  * (E_{ij} \in {0,1,\dots,8})

为什么只选星期三？
因为工作日/周末的社交模式差异巨大，把所有天混在一起，很难统一建模。
固定“每周的同一天”能减少这种周期性变化。

### 3. 把数据丢进第 9.3.5 节的 EM 模型

使用我们之前推过的独立边模型 + EM：

* E-step：用观测到的 (E_{ij}) 和当前参数，算出
  [
  Q_{ij} = P(A_{ij}=1 \mid E_{ij}, \alpha,\beta,\rho)
  ]
* M-step：用新的 (Q_{ij}) 更新 (\alpha,\beta,\rho)

迭代到收敛之后，他们得到：

* (\alpha = 0.4242)
  → true positive rate ≈ 0.42
  → false negative rate = 1−α ≈ 0.58（漏掉的真实边挺多的，>50%）

* (\beta = 0.0043)
  → false positive rate ≈ 0.4%（误判的假边非常少）

* (\rho = 0.0335)
  → 每对节点有边的先验概率 ≈ 3.35%，网络比较稀疏（很合理）

怎么解读？

* **小 β**：蓝牙随机偶发接近工作得还不错，假阳性不多
* **α 只有 0.42**：
  这说明就算两个人真的是朋友/同学，也只有约 42% 的概率在当天发生“足够接近”的蓝牙记录。换句话说：

  > 他们是真朋友，但大部分日子并不会刚好在那一天、那几米距离内被观测到
  > 这在现实中非常合理，不代表实验失败。

### 4. Q 矩阵和图 9.2：粗核心 + 周边节点

他们把 (Q_{ij}) 画成图（Fig. 9.2）：

* 点是人
* 线是**边存在的概率 (Q_{ij})**
* 线越粗，(Q_{ij}) 越接近 1

图上可以看到：

* 一个大约 20 人组成的核心 group，边的 (Q_{ij}) 很高 → 彼此关系强
* 其他人和核心之间的连边比较细 → 较弱或不确定关系

**这就是一个“软网络图”**：不是简单 0 / 1，而是概率权重。

### 5. 图 9.3：Q vs E 的关系

他们把 (Q_{ij}) 对 (E_{ij}) 画出来（Fig. 9.3）：

* 横轴：(E_{ij})（这对人被记录接近的天数，0–8）
* 纵轴：边存在概率 (Q_{ij})

结论非常清晰：

* 若 (E_{ij} = 0) 或 1：
  (Q_{ij} < 0.1)，大概率是“路人、偶遇”
* 若 (E_{ij} \ge 2)：
  (Q_{ij} > 0.9)，几乎肯定有社交关系

> 换成自然语言：
> 一次偶然靠近，基本可以认为是巧合；
> 两次及以上重复靠近，就很可能是真朋友/同学。

这就是用 **贝叶斯+EM** 把 noisy 的“接近数据”转成“朋友关系概率”的非常典型例子。

如果你做“某对纳米线交点之间，在多次采样/多张图中共出现几次连接”，完全可以用同样思路，把“多次观察中的连接次数”→“真实有连接的概率”。

---

## 二、9.3.7 Estimation of other quantities：用后验分布算度、中心性…

前面我们只是在估计 “哪条边在不在”。
但一旦你有了 **(P(A|\text{data},\theta))**，就可以对 **任意依赖于 A 的网络量** 做统计。

设：

* (X(A))：任何你关心的量（比如平均路径长度、某节点度、聚类系数等）

### 1. X 的分布和期望

* **X 取某个值 x 的概率**：
  [
  P(X = x)
  ========

  \sum_{A} \delta_{x, X(A)} P(A|\text{data},\theta) \tag{9.31}
  ]
  （用 Kronecker delta 限制 X(A)=x 的网络求和）

* **X 的期望**：
  [
  \langle X \rangle
  =================

  \sum_A X(A) P(A|\text{data},\theta) \tag{9.32}
  ]

* **X 的方差（σ²）**：
  [
  \sigma_X^2
  ==========

  \sum_A [X(A)-\langle X\rangle]^2 P(A|\text{data},\theta) \tag{9.33}
  ]

理念很简单：

> A 是一个随机变量（网络结构不确定），
> 任意网络指标 X(A) 也是随机变量，
> 你可以给出 “X 的分布/期望/方差”，而不是死板地给一个数。

### 2. 节点度的例子：期望度 = Σ Qij

节点 i 的度：
[
k_i = \sum_j A_{ij}
]

于是其期望值：

[
\langle k_i \rangle
===================

# \sum_A k_i(A) P(A|\text{data},\theta)

\sum_j \sum_A A_{ij} P(A|\text{data},\theta)
]

注意：
对任意 (i,j)，(\sum_A A_{ij} P(A|\text{data},\theta)) 就是 **(P(A_{ij}=1|\text{data},\theta))**。

这正是我们在独立边误差模型里叫的 (Q_{ij})。
于是：

[
\langle k_i \rangle
===================

\sum_j Q_{ij} \tag{9.34}
]

非常漂亮的结论：

> **把普通邻接矩阵 (A_{ij}) 换成软邻接矩阵 (Q_{ij})，
> 所有关于“度”的公式照抄，就得到各种指标的期望值**。

对你来说：

* 现在你不是“网络确定的 graph”，而是“概率图 (Q_{ij})”
* 想要 degree distribution、聚类系数、甚至 MMD 指标的 “带误差版本”，都可以用这种 “对后验平均” 的思想做。

---

## 三、9.3.8 Other error models：独立边误差之外的世界

作者说：独立边误差模型很简单、很好用，是不错的起点，但 **网络误差不像实数测量那样有一个“万能 Gaussian 模型”可以几乎一直用**。

### 1. 独立边误差模型可能的变体

可以做很多扩展，比如：

* (\alpha,\beta) 在不同节点对上不同

  * 比如某些区域的测量更可靠
* (\alpha,\beta) 依赖于节点属性或边属性

  * 比如“degree 超高的节点更容易被测量到”
* 边之间有相关性（correlated edges）

  * 一个边的存在提高另一个边被测量到的概率
* 不同边被测量的次数 (N_{ij}) 不同（不是所有 (E_{ij}) 都基于同样多的观测）
* 加上**边权重**和“多等级关系”

  * 例如社交网络：不认识 / 一般熟 / 很熟，有三种状态而不是 0/1

这些都可以通过重新写一个合适的 (P(\text{data}\mid A,\theta)) 来实现。

---

### 2. 一些完全不同类型的误差模型

例如：

* 固定上限的社交问卷（fixed choice surveys）
  → 每人最多只能填 k 个朋友
  → 这种“截断”会导致系统性的漏边，必须写进误差模型里。

* Butts 的模型 [90]：
  把受访者“回答准确度”当成参数，有的人报告很准、有的人乱填。
  → 多了一大堆“个体可靠性”参数，模型更复杂。

---

### 3. 单次测量也能估计误差吗？

前面的独立边模型需要多次重复测量同一个网络（N 次），这在现实中不常见。
作者说，**在某些特殊情况下，可以从单次测量推误差**，前提是：

* 网络中某些边**强相关**甚至“完美相关”
* 例如：我们知道两条边一定要么同时存在，要么同时不存在
* 那么测一次整个网络，实际上对这个“合并后的边”测了 2 次，有点像重复测量

现实中的边通常不完全相关，但只要有部分相关性，也可能挤出一点误差信息。
这类思路在一些论文中出现过，但理论还不成熟，是可以继续做研究的方向（Newman 在这点上有点“开放问题”的味道）。

---

## 四、和你研究的连接（简版）

你现在已经掌握了：

* 如何建 **误差模型** (P(\text{data}\mid A,\theta))
* 如何用 EM 估计 (\theta) 和 (Q_{ij})
* 如何用 (Q_{ij}) 计算：

  * 期望度：(\langle k_i\rangle = \sum_j Q_{ij})
  * 以及其他网络指标的期望和不确定性

如果你做：

* 多张同一位置 SEM 图像、或者不同参数跑多次 skeleton（得到多次“有边/无边”观测）
* 或者在数据增强、多视角下得到“重复观测”

就可以直接把这一整套搬到 **银纳米线图的 measurement error analysis** 里。

---

如果你愿意，下一步我可以帮你写一段**完全贴合你论文场景的“Network measurement error & EM estimation”方法学小节**，用类似 Newman's 语言，但对象换成：

* 纳米线交点作为节点
* 导电接触作为边
* 多次图像/算法变化作为重复测量

你可以直接贴进你的方法章节或 supplementary。

## 1.1 Why Networks Matter

### The Ubiquity of Networks

Networks are **everywhere** in both natural and artificial systems:

- **Biological systems**: Neural networks, protein interaction networks, metabolic pathways
- **Social systems**: Friendship networks, collaboration networks, communication networks  
- **Technological systems**: Internet, power grids, transportation networks
- **Information systems**: World Wide Web, citation networks, knowledge graphs

### Why Study Networks?

Understanding network structure helps us:
- **Predict system behavior** and evolution
- **Optimize performance** and efficiency
- **Identify critical components** and vulnerabilities
- **Design better systems** based on network principles

## 1.2 Real-World Network Examples

### The Internet

The Internet represents one of the most studied technological networks:

- **Nodes**: Routers, servers, and end-user devices
- **Edges**: Physical and logical connections
- **Scale**: Billions of nodes, trillions of connections
- **Properties**: High clustering, short path lengths, scale-free degree distribution

### Social Networks

Human social networks exhibit fascinating properties:

- **Six degrees of separation**: Any two people are connected by at most 6 steps
- **Small-world effect**: Short average path lengths despite large size
- **Homophily**: People tend to connect with similar others
- **Community structure**: Dense clusters with sparse interconnections

### Biological Networks

#### Protein-Protein Interaction Networks

- **Nodes**: Proteins
- **Edges**: Physical interactions between proteins
- **Scale**: ~20,000 human proteins, ~100,000 interactions
- **Properties**: Scale-free, modular structure, essential proteins are highly connected

#### Metabolic Networks

- **Nodes**: Metabolites (small molecules)
- **Edges**: Biochemical reactions
- **Scale**: Thousands of metabolites and reactions
- **Properties**: Hierarchical organization, conserved across species

## 1.3 Fundamental Network Properties

### Degree Distribution

The **degree** $k_i$ of node $i$ is the number of edges connected to it.

For a network with $n$ nodes, the **degree distribution** $P(k)$ gives the probability that a randomly chosen node has degree $k$:

$$P(k) = \frac{\text{Number of nodes with degree } k}{n}$$

#### Mathematical Properties

- **Normalization**: $\sum_{k=0}^{\infty} P(k) = 1$
- **Average degree**: $\langle k \rangle = \sum_{k=0}^{\infty} k P(k)$
- **Second moment**: $\langle k^2 \rangle = \sum_{k=0}^{\infty} k^2 P(k)$

### Clustering Coefficient

The **local clustering coefficient** $C_i$ measures how tightly connected the neighbors of node $i$ are:

$$C_i = \frac{\text{Number of triangles containing node } i}{\binom{k_i}{2}} = \frac{2e_i}{k_i(k_i-1)}$$

Where $e_i$ is the number of edges between neighbors of node $i$.

The **global clustering coefficient** is the average:

$$C = \frac{1}{n} \sum_{i=1}^{n} C_i$$

#### Physical Interpretation

- $C_i = 1$: All neighbors of $i$ are connected (complete subgraph)
- $C_i = 0$: No connections between neighbors of $i$
- High clustering indicates **local order** and **community structure**

### Path Length and Diameter

#### Shortest Path Length

The **shortest path length** $d_{ij}$ between nodes $i$ and $j$ is the minimum number of edges in any path connecting them.

#### Average Path Length

$$L = \frac{1}{\binom{n}{2}} \sum_{i<j} d_{ij} = \frac{2}{n(n-1)} \sum_{i<j} d_{ij}$$

#### Network Diameter

$$D = \max_{i,j} d_{ij}$$

### Small-World Effect

Many real networks exhibit the **small-world property**:

- **Short average path length**: $L \sim \log n$ (much smaller than $n$)
- **High clustering**: $C \gg C_{\text{random}}$ (much higher than random networks)

This combination is rare in random networks but common in real-world systems.

## 1.4 Scale-Free Networks

### Power-Law Degree Distribution

Many real networks follow a **power-law degree distribution**:

$$P(k) \sim k^{-\gamma}$$

Where $\gamma$ is the **exponent** (typically $2 < \gamma < 3$).

#### Mathematical Properties

- **Heavy-tailed**: Few high-degree nodes, many low-degree nodes
- **Scale-invariant**: No characteristic scale
- **Infinite variance**: When $\gamma \leq 3$, $\langle k^2 \rangle$ diverges

### Real-World Examples

#### World Wide Web
- **In-degree**: $\gamma \approx 2.1$ (pages linking to a page)
- **Out-degree**: $\gamma \approx 2.7$ (pages linked by a page)

#### Internet Router Network
- **Degree distribution**: $\gamma \approx 2.2$
- **Implication**: Few highly connected routers, many peripheral ones

#### Scientific Collaboration Networks
- **Degree distribution**: $\gamma \approx 2.1$
- **Implication**: Few highly collaborative scientists, many with few collaborators

## 1.5 Network Robustness and Vulnerability

### Attack Tolerance

Scale-free networks exhibit **robustness against random failures** but **vulnerability to targeted attacks**:

- **Random failures**: Removing random nodes rarely affects connectivity
- **Targeted attacks**: Removing high-degree nodes quickly fragments the network

### Mathematical Framework

The **percolation threshold** $p_c$ is the critical fraction of nodes that must be removed to fragment the network:

$$p_c = 1 - \frac{1}{\kappa - 1}$$

Where $\kappa = \frac{\langle k^2 \rangle}{\langle k \rangle}$ is the **degree ratio**.

## 1.6 Applications to Materials Science

### Silver Nanowire Networks

Network concepts apply directly to nanowire systems:

- **Nodes**: Nanowire junctions
- **Edges**: Nanowire segments
- **Properties**: Percolation threshold, electrical conductivity, mechanical strength

#### Percolation Theory

The **percolation probability** $P(p)$ gives the probability that a randomly chosen node belongs to the giant component:

$$P(p) = 1 - \sum_{k=0}^{\infty} P(k) (1-p)^k$$

### Partially Disordered Materials

Network analysis helps understand:

- **Local order parameters** in disordered regions
- **Defect clustering** and percolation
- **Phase transition** mechanisms
- **Property-structure relationships**

## Code Example: Basic Network Analysis

```python
import networkx as nx
import numpy as np
import matplotlib.pyplot as plt

def analyze_network_properties(G):
    """Analyze fundamental network properties"""
    
    # Basic statistics
    n = G.number_of_nodes()
    m = G.number_of_edges()
    
    # Degree distribution
    degrees = [d for n, d in G.degree()]
    avg_degree = np.mean(degrees)
    
    # Clustering coefficient
    clustering = nx.average_clustering(G)
    
    # Path length (for connected components)
    if nx.is_connected(G):
        avg_path_length = nx.average_shortest_path_length(G)
        diameter = nx.diameter(G)
    else:
        # For disconnected graphs, analyze largest component
        largest_cc = max(nx.connected_components(G), key=len)
        subgraph = G.subgraph(largest_cc)
        avg_path_length = nx.average_shortest_path_length(subgraph)
        diameter = nx.diameter(subgraph)
    
    return {
        'nodes': n,
        'edges': m,
        'avg_degree': avg_degree,
        'clustering': clustering,
        'avg_path_length': avg_path_length,
        'diameter': diameter
    }

# Example: Analyze a scale-free network
G = nx.barabasi_albert_graph(1000, 3)
properties = analyze_network_properties(G)

print(f"Network properties:")
for key, value in properties.items():
    print(f"{key}: {value:.4f}")
```

## Key Takeaways

1. **Networks are universal**: They appear in virtually every complex system
2. **Mathematical framework**: Degree distribution, clustering, and path length are fundamental measures
3. **Scale-free property**: Many real networks follow power-law degree distributions
4. **Small-world effect**: Short path lengths with high clustering are common
5. **Robustness-vulnerability trade-off**: Scale-free networks are robust to random failures but vulnerable to targeted attacks
6. **Materials applications**: Network concepts directly apply to nanowire systems and disordered materials

## References

1. Newman, M. E. J. (2010). Networks: An Introduction. Oxford University Press.
2. Barabási, A. L., & Albert, R. (1999). Emergence of scaling in random networks. Science, 286(5439), 509-512.
3. Watts, D. J., & Strogatz, S. H. (1998). Collective dynamics of 'small-world' networks. Nature, 393(6684), 440-442.

---

*This is the first in a series of chapter-by-chapter study notes for Newman's Networks textbook. Each chapter builds upon the previous ones, so understanding these fundamental concepts is crucial for the advanced topics to come.*
