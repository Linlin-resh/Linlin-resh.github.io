---
title: "Reading Notes: Newman's Networks Chapter 6 - Mathematical Foundations"
date: 2025-08-29
draft: false
description: "Study notes for Chapter 6 of Newman's 'Networks: An Introduction' covering graph theory, matrix representations, and mathematical foundations of network analysis"
tags: ["reading-notes", "network-theory", "mathematics", "graph-theory", "linear-algebra"]
showToc: true
TocOpen: true
---

## Introduction

Chapter 6 of Newman's *Networks: An Introduction* establishes the **mathematical foundations** of network analysis. This chapter provides the essential mathematical tools and concepts needed to understand and analyze complex networks, from basic graph theory to advanced matrix methods.

## 6.1 Graph Theory Fundamentals

### Basic Definitions

#### Graph Components

A **graph** $G = (V, E)$ consists of:

- **Vertices (nodes)**: $V = \{v_1, v_2, \ldots, v_n\}$
- **Edges (links)**: $E = \{(v_i, v_j) : v_i, v_j \in V\}$
- **Order**: $n = |V|$ (number of vertices)
- **Size**: $m = |E|$ (number of edges)

#### Graph Types

**Undirected graph**: Edges have no direction
- **Edge**: $(v_i, v_j) = (v_j, v_i)$
- **Degree**: $k_i = |\{j : (v_i, v_j) \in E\}|$

**Directed graph (digraph)**: Edges have direction
- **Edge**: $(v_i, v_j) \neq (v_j, v_i)$
- **In-degree**: $k_i^{\text{in}} = |\{j : (v_j, v_i) \in E\}|$
- **Out-degree**: $k_i^{\text{out}} = |\{j : (v_i, v_j) \in E\}|$

**Weighted graph**: Edges have weights
- **Weight**: $w_{ij} \in \mathbb{R}$ for edge $(v_i, v_j)$
- **Weighted degree**: $k_i^w = \sum_{j} w_{ij}$

### Graph Properties

#### Connectivity

**Path**: Sequence of vertices $(v_1, v_2, \ldots, v_k)$ where $(v_i, v_{i+1}) \in E$

**Walk**: Path where vertices can be repeated

**Trail**: Path where edges can be repeated

**Cycle**: Path where $v_1 = v_k$ and all other vertices are distinct

**Connected graph**: Path exists between any two vertices

**Strongly connected**: Directed path exists between any two vertices

#### Distance and Diameter

**Distance**: $d_{ij} = \min\{\text{length of path from } i \text{ to } j\}$

**Diameter**: $D = \max_{i,j} d_{ij}$

**Average path length**: $L = \frac{1}{n(n-1)} \sum_{i \neq j} d_{ij}$

#### Clustering

**Local clustering coefficient**:
$$C_i = \frac{2e_i}{k_i(k_i-1)}$$

Where $e_i$ is the number of edges between neighbors of vertex $i$.

**Global clustering coefficient**:
$$C = \frac{1}{n} \sum_{i=1}^n C_i$$

**Transitivity**:
$$T = \frac{3 \times \text{number of triangles}}{\text{number of connected triples}}$$

## 6.2 Matrix Representations

### Adjacency Matrix

#### Definition

For an undirected graph:
$$A_{ij} = \begin{cases} 
1 & \text{if } (v_i, v_j) \in E \\
0 & \text{otherwise}
\end{cases}$$

For a directed graph:
$$A_{ij} = \begin{cases} 
1 & \text{if } (v_i, v_j) \in E \\
0 & \text{otherwise}
\end{cases}$$

#### Properties

**Undirected graphs**:
- **Symmetric**: $A = A^T$
- **Trace**: $\text{tr}(A) = 0$ (no self-loops)
- **Sum of rows**: $\sum_j A_{ij} = k_i$ (degree of vertex $i$)

**Directed graphs**:
- **Asymmetric**: $A \neq A^T$ (in general)
- **Row sums**: $\sum_j A_{ij} = k_i^{\text{out}}$ (out-degree)
- **Column sums**: $\sum_i A_{ij} = k_j^{\text{in}}$ (in-degree)

#### Powers of Adjacency Matrix

**$A^2$ interpretation**:
$$(A^2)_{ij} = \sum_k A_{ik} A_{kj} = \text{number of paths of length 2 from } i \text{ to } j$$

**General power**:
$$(A^k)_{ij} = \text{number of paths of length } k \text{ from } i \text{ to } j$$

**Reachability matrix**:
$$R = \sum_{k=1}^{n-1} A^k$$

Where $R_{ij} = 1$ if there exists a path from $i$ to $j$.

### Laplacian Matrix

#### Definition

**Laplacian matrix**:
$$L = D - A$$

Where $D$ is the **degree matrix**:
$$D_{ij} = \begin{cases} 
k_i & \text{if } i = j \\
0 & \text{otherwise}
\end{cases}$$

#### Properties

**Eigenvalues**:
- **Smallest eigenvalue**: $\lambda_1 = 0$ (always)
- **Multiplicity of 0**: Number of connected components
- **Second smallest eigenvalue**: $\lambda_2 > 0$ if graph is connected

**Eigenvectors**:
- **First eigenvector**: $\mathbf{v}_1 = \frac{1}{\sqrt{n}} \mathbf{1}$ (constant vector)
- **Fiedler vector**: Second eigenvector (used for graph partitioning)

#### Normalized Laplacian

**Symmetric normalized Laplacian**:
$$L_{\text{sym}} = D^{-1/2} L D^{-1/2} = I - D^{-1/2} A D^{-1/2}$$

**Random walk normalized Laplacian**:
$$L_{\text{rw}} = D^{-1} L = I - D^{-1} A$$

### Incidence Matrix

#### Definition

**Incidence matrix** $B$:
$$B_{ie} = \begin{cases} 
1 & \text{if vertex } i \text{ is incident to edge } e \\
0 & \text{otherwise}
\end{cases}$$

#### Properties

**Dimensions**: $n \times m$ (vertices Ã— edges)

**Rank**: $\text{rank}(B) = n - c$ where $c$ is the number of connected components

**Laplacian relationship**:
$$L = B B^T$$

## 6.3 Spectral Graph Theory

### Eigenvalue Analysis

#### Adjacency Matrix Eigenvalues

**Spectral radius**: $\rho(A) = \max_i |\lambda_i|$

**Perron-Frobenius theorem**: For connected graphs, the largest eigenvalue is:
- **Real and positive**: $\lambda_1 > 0$
- **Simple**: Multiplicity 1
- **Positive eigenvector**: All components > 0

#### Laplacian Eigenvalues

**Spectral gap**: $\lambda_2 - \lambda_1 = \lambda_2$

**Cheeger's inequality**:
$$\frac{\lambda_2}{2} \leq h(G) \leq \sqrt{2\lambda_2}$$

Where $h(G)$ is the **Cheeger constant**:
$$h(G) = \min_{S} \frac{|\partial S|}{\min(|S|, |V \setminus S|)}$$

### Graph Partitioning

#### Spectral Partitioning

**Algorithm**:
1. Compute **Fiedler vector** (second eigenvector of Laplacian)
2. Sort vertices by Fiedler vector values
3. Cut at median value

**Mathematical foundation**:
$$\min_{\mathbf{x}} \mathbf{x}^T L \mathbf{x} \quad \text{subject to } \mathbf{x}^T \mathbf{1} = 0, \mathbf{x}^T \mathbf{x} = n$$

**Solution**: Fiedler vector

#### Ratio Cut

**Ratio cut**:
$$\text{RatioCut}(S, T) = \frac{\text{cut}(S, T)}{|S|} + \frac{\text{cut}(S, T)}{|T|}$$

**Spectral relaxation**:
$$\min_{\mathbf{x}} \mathbf{x}^T L \mathbf{x} \quad \text{subject to } \mathbf{x}^T \mathbf{1} = 0, \mathbf{x}^T \mathbf{x} = n$$

## 6.4 Random Walks

### Transition Matrix

#### Definition

**Transition matrix**:
$$P_{ij} = \frac{A_{ij}}{k_i}$$

**Properties**:
- **Row stochastic**: $\sum_j P_{ij} = 1$
- **Eigenvalues**: $1 = \lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n \geq -1$

#### Stationary Distribution

**Stationary distribution**:
$$\pi_i = \frac{k_i}{2m}$$

**Verification**:
$$\sum_i \pi_i P_{ij} = \sum_i \frac{k_i}{2m} \frac{A_{ij}}{k_i} = \frac{1}{2m} \sum_i A_{ij} = \frac{k_j}{2m} = \pi_j$$

### Mixing Time

#### Definition

**Mixing time**:
$$\tau_{\text{mix}} = \min\{t : \max_i ||P^t(i, \cdot) - \pi||_1 \leq \epsilon\}$$

**Spectral bound**:
$$\tau_{\text{mix}} \leq \frac{1}{1 - \lambda_2} \log\left(\frac{1}{\epsilon \pi_{\min}}\right)$$

Where $\pi_{\min} = \min_i \pi_i$.

### PageRank

#### Definition

**PageRank**:
$$\mathbf{PR} = (1-d) \frac{\mathbf{1}}{n} + d P^T \mathbf{PR}$$

**Matrix form**:
$$\mathbf{PR} = (1-d) \frac{\mathbf{1}}{n} + d \mathbf{M} \mathbf{PR}$$

Where $\mathbf{M}$ is the **stochastic matrix**:
$$M_{ij} = \frac{A_{ji}}{k_j^{\text{out}}}$$

#### Power Iteration

**Iterative solution**:
$$\mathbf{PR}^{(t+1)} = (1-d) \frac{\mathbf{1}}{n} + d \mathbf{M}^T \mathbf{PR}^{(t)}$$

**Convergence**:
$$||\mathbf{PR}^{(t+1)} - \mathbf{PR}^{(t)}||_1 < \epsilon$$

## 6.5 Centrality Measures

### Degree Centrality

**Definition**:
$$C_D(i) = \frac{k_i}{n-1}$$

**Normalized**: $0 \leq C_D(i) \leq 1$

### Betweenness Centrality

**Definition**:
$$C_B(i) = \sum_{s \neq i \neq t} \frac{\sigma_{st}(i)}{\sigma_{st}}$$

Where:
- $\sigma_{st}$: Number of shortest paths from $s$ to $t$
- $\sigma_{st}(i)$: Number of shortest paths from $s$ to $t$ passing through $i$

**Normalized**:
$$C_B^{\text{norm}}(i) = \frac{C_B(i)}{(n-1)(n-2)/2}$$

### Closeness Centrality

**Definition**:
$$C_C(i) = \frac{n-1}{\sum_{j \neq i} d_{ij}}$$

**Normalized**: $0 \leq C_C(i) \leq 1$

### Eigenvector Centrality

**Definition**:
$$x_i = \frac{1}{\lambda} \sum_j A_{ij} x_j$$

**Matrix form**:
$$A \mathbf{x} = \lambda \mathbf{x}$$

**Solution**: Principal eigenvector of adjacency matrix

### Katz Centrality

**Definition**:
$$x_i = \alpha \sum_j A_{ij} x_j + \beta$$

**Matrix form**:
$$\mathbf{x} = \alpha A \mathbf{x} + \beta \mathbf{1}$$

**Solution**:
$$\mathbf{x} = \beta (I - \alpha A)^{-1} \mathbf{1}$$

**Convergence**: Requires $\alpha < \frac{1}{\rho(A)}$

## 6.6 Applications to Materials Science

### Network Topology in Materials

#### Atomic Networks

**Network representation**:
- **Nodes**: Atoms
- **Edges**: Chemical bonds
- **Weights**: Bond strength, distance

**Mathematical analysis**:
- **Degree distribution**: Coordination number distribution
- **Clustering**: Local atomic environment
- **Path length**: Atomic connectivity

#### Defect Networks

**Network properties**:
- **Connectivity**: Defect percolation
- **Clustering**: Defect clustering
- **Centrality**: Critical defects

**Mathematical framework**:
$$P(\text{percolation}) = 1 - \exp\left(-\frac{\langle k^2 \rangle}{\langle k \rangle} p\right)$$

### Network-Based Materials Design

#### Structure-Property Relationships

**Network descriptors**:
- **Average degree**: $\langle k \rangle$
- **Clustering coefficient**: $C$
- **Path length**: $L$
- **Centrality measures**: $C_D, C_B, C_C$

**Property prediction**:
$$P = f(\langle k \rangle, C, L, \ldots)$$

#### Optimization

**Objective function**:
$$\min_{\text{network}} \sum_i w_i |P_i - P_i^{\text{target}}|^2$$

**Constraints**:
- **Connectivity**: Network must be connected
- **Degree bounds**: $k_{\min} \leq k_i \leq k_{\max}$
- **Clustering bounds**: $C_{\min} \leq C \leq C_{\max}$

## Code Example: Mathematical Foundations

```python
import networkx as nx
import numpy as np
import matplotlib.pyplot as plt
from scipy.linalg import eig
from scipy.sparse import csgraph

def analyze_graph_matrices(G):
    """Analyze graph using matrix representations"""
    
    # Adjacency matrix
    A = nx.adjacency_matrix(G).toarray()
    n = A.shape[0]
    
    # Degree matrix
    degrees = [d for n, d in G.degree()]
    D = np.diag(degrees)
    
    # Laplacian matrix
    L = D - A
    
    # Normalized Laplacian
    D_sqrt_inv = np.diag(1.0 / np.sqrt(degrees))
    L_norm = D_sqrt_inv @ L @ D_sqrt_inv
    
    # Eigenvalue analysis
    eigenvals_A, eigenvecs_A = eig(A)
    eigenvals_L, eigenvecs_L = eig(L)
    eigenvals_L_norm, eigenvecs_L_norm = eig(L_norm)
    
    # Sort eigenvalues
    eigenvals_A = np.real(eigenvals_A)
    eigenvals_L = np.real(eigenvals_L)
    eigenvals_L_norm = np.real(eigenvals_L_norm)
    
    idx_A = np.argsort(eigenvals_A)[::-1]
    idx_L = np.argsort(eigenvals_L)
    idx_L_norm = np.argsort(eigenvals_L_norm)
    
    eigenvals_A = eigenvals_A[idx_A]
    eigenvals_L = eigenvals_L[idx_L]
    eigenvals_L_norm = eigenvals_L_norm[idx_L_norm]
    
    return {
        'adjacency_matrix': A,
        'degree_matrix': D,
        'laplacian_matrix': L,
        'normalized_laplacian': L_norm,
        'adjacency_eigenvalues': eigenvals_A,
        'laplacian_eigenvalues': eigenvals_L,
        'normalized_laplacian_eigenvalues': eigenvals_L_norm
    }

def compute_centrality_measures(G):
    """Compute various centrality measures"""
    
    # Degree centrality
    degree_centrality = nx.degree_centrality(G)
    
    # Betweenness centrality
    betweenness_centrality = nx.betweenness_centrality(G)
    
    # Closeness centrality
    closeness_centrality = nx.closeness_centrality(G)
    
    # Eigenvector centrality
    eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)
    
    # Katz centrality
    katz_centrality = nx.katz_centrality(G, alpha=0.1)
    
    # PageRank
    pagerank = nx.pagerank(G, alpha=0.85)
    
    return {
        'degree_centrality': degree_centrality,
        'betweenness_centrality': betweenness_centrality,
        'closeness_centrality': closeness_centrality,
        'eigenvector_centrality': eigenvector_centrality,
        'katz_centrality': katz_centrality,
        'pagerank': pagerank
    }

def analyze_spectral_properties(G):
    """Analyze spectral properties of the graph"""
    
    # Matrix analysis
    matrix_results = analyze_graph_matrices(G)
    
    # Spectral gap
    laplacian_eigenvals = matrix_results['laplacian_eigenvalues']
    spectral_gap = laplacian_eigenvals[1] - laplacian_eigenvals[0]
    
    # Cheeger constant (approximation)
    cheeger_constant = spectral_gap / 2
    
    # Mixing time (approximation)
    if spectral_gap > 0:
        mixing_time = 1 / spectral_gap
    else:
        mixing_time = float('inf')
    
    # Fiedler vector
    L = matrix_results['laplacian_matrix']
    eigenvals, eigenvecs = eig(L)
    eigenvals = np.real(eigenvals)
    eigenvecs = np.real(eigenvecs)
    
    idx = np.argsort(eigenvals)
    fiedler_vector = eigenvecs[:, idx[1]]
    
    return {
        'spectral_gap': spectral_gap,
        'cheeger_constant': cheeger_constant,
        'mixing_time': mixing_time,
        'fiedler_vector': fiedler_vector
    }

def plot_spectral_analysis(G, title="Spectral Analysis"):
    """Plot spectral analysis results"""
    
    matrix_results = analyze_graph_matrices(G)
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
    
    # Adjacency matrix eigenvalues
    eigenvals_A = matrix_results['adjacency_eigenvalues']
    ax1.plot(eigenvals_A, 'bo-', markersize=8)
    ax1.set_xlabel('Index')
    ax1.set_ylabel('Eigenvalue')
    ax1.set_title('Adjacency Matrix Eigenvalues')
    ax1.grid(True, alpha=0.3)
    
    # Laplacian matrix eigenvalues
    eigenvals_L = matrix_results['laplacian_eigenvalues']
    ax2.plot(eigenvals_L, 'ro-', markersize=8)
    ax2.set_xlabel('Index')
    ax2.set_ylabel('Eigenvalue')
    ax2.set_title('Laplacian Matrix Eigenvalues')
    ax2.grid(True, alpha=0.3)
    
    # Normalized Laplacian eigenvalues
    eigenvals_L_norm = matrix_results['normalized_laplacian_eigenvalues']
    ax3.plot(eigenvals_L_norm, 'go-', markersize=8)
    ax3.set_xlabel('Index')
    ax3.set_ylabel('Eigenvalue')
    ax3.set_title('Normalized Laplacian Eigenvalues')
    ax3.grid(True, alpha=0.3)
    
    # Fiedler vector
    spectral_results = analyze_spectral_properties(G)
    fiedler_vector = spectral_results['fiedler_vector']
    
    ax4.plot(fiedler_vector, 'mo-', markersize=8)
    ax4.set_xlabel('Node Index')
    ax4.set_ylabel('Fiedler Vector Value')
    ax4.set_title('Fiedler Vector (Second Eigenvector)')
    ax4.grid(True, alpha=0.3)
    
    plt.suptitle(title)
    plt.tight_layout()
    plt.show()

# Example: Analyze a scale-free network
G = nx.barabasi_albert_graph(100, 3)

# Matrix analysis
matrix_results = analyze_graph_matrices(G)
print("Matrix Analysis Results:")
print(f"Spectral gap: {matrix_results['laplacian_eigenvalues'][1] - matrix_results['laplacian_eigenvalues'][0]:.4f}")

# Centrality measures
centrality_results = compute_centrality_measures(G)
print("\nCentrality Measures (top 5 nodes):")
for measure, values in centrality_results.items():
    sorted_nodes = sorted(values.items(), key=lambda x: x[1], reverse=True)[:5]
    print(f"{measure}: {sorted_nodes}")

# Spectral analysis
spectral_results = analyze_spectral_properties(G)
print(f"\nSpectral Properties:")
print(f"Cheeger constant: {spectral_results['cheeger_constant']:.4f}")
print(f"Mixing time: {spectral_results['mixing_time']:.4f}")

# Plot results
plot_spectral_analysis(G, "Scale-Free Network Spectral Analysis")
```

## Key Takeaways

1. **Matrix representations**: Adjacency, Laplacian, and incidence matrices provide different views of network structure
2. **Spectral analysis**: Eigenvalues and eigenvectors reveal important network properties
3. **Centrality measures**: Different measures capture different aspects of node importance
4. **Random walks**: Provide insights into network dynamics and mixing
5. **Graph partitioning**: Spectral methods are effective for community detection
6. **Mathematical rigor**: Solid mathematical foundation enables advanced network analysis
7. **Applications**: Matrix methods are essential for materials science applications

## References

1. Newman, M. E. J. (2010). Networks: An Introduction. Oxford University Press.
2. Chung, F. R. K. (1997). Spectral Graph Theory. American Mathematical Society.
3. Godsil, C., & Royle, G. (2001). Algebraic Graph Theory. Springer.
4. Horn, R. A., & Johnson, C. R. (2012). Matrix Analysis. Cambridge University Press.

---

*The mathematical foundations of network analysis provide the essential tools for understanding complex systems, with direct applications to materials science and engineering.*
