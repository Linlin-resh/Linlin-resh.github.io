[{"content":"h Analysis Functions\n1. Stru Triangle Counting import def clustering_coefficient_distribution(G): \u0026#34;\u0026#34;\u0026#34;Calculate clustering coefficient distribution\u0026#34;\u0026#34;\u0026#34; clustering = nx.clustering(G) values = list(clustering.values()) return { \u0026#39;mean\u0026#39;: np.mean(values), \u0026#39;std\u0026#39;: np.std(values), \u0026#39;min\u0026#39;: np.min(values), \u0026#39;max\u0026#39;: np.max(values), \u0026#39;distribution\u0026#39;: values } Clique Detection def find_maximal_cliques(G, min_size=3): \u0026#34;\u0026#34;\u0026#34;Find maximal cliques of given minimum size\u0026#34;\u0026#34;\u0026#34; cliques = [] for clique in nx.find_cliques(G): if len(clique) \u0026gt;= min_size: cliques.append(clique) retu] += 1 return { \u0026#39;counts\u0026#39;: dict(size_counts), \u0026#39;total\u0026#39;: len(clique_sizes), \u0026#39;max_size\u0026#39;: max(clique_sizes), \u0026#39;avg_size\u0026#39;: np.mean(clique_sizes) } 2. Disorder Quantification Local Order Parameters def calculate_local_order(G, reference_distances=None): \u0026#34;\u0026#34;\u0026#34;Calculate local order parameters for each node\u0026#34;\u0026#34;\u0026#34; local_order = {} for node in G.nodes(): neighbors = list(G.neighbors(node)) if len(neighbors) \u0026lt; 2: local_order[node] = 0.0 continue # Calces is None: # Assume ideal structure has all neighbors connected reference_distances = [1.0] * len(actual_distances) # Calculate disorder as RMS deviation deviations = np.array(actual_distances) - np.array(reference_distances) local_order[node] = np.sqrt(np.mean(deviations**2)) return local_order def global_disorder_metric(G): \u0026#34;\u0026#34;\u0026#34;Calculate global disorder metric for the entire graph\u0026#34;\u0026#34;\u0026#34; local_order = calculate_local_order(G) return np.mean(list(local_order.values())) 4. Visualization Tools Structural Motif Visualization import matplotlib.pyplot as plt def plot_triangle_distribution(G): \u0026#34;\u0026#34;\u0026#34;Plot distribution of triangles per node\u0026#34;\u0026#34;\u0026#34; triangles = count_triangles_by_node(G) values = list(triangles.values()) plt.figure(figsize=(10, 6)) plt.hist(values, bins=20, alpha=0.7, edgecolor=\u0026#39;black\u0026#39;) plt.xlabel(\u0026#39;Number of Triangles\u0026#39;) plt.ylabel(\u0026#39;Frequency\u0026#39;) plt.title(\u0026#39;Triangle Distribution\u0026#39;) plt.grid(True, alpha=0.3) plt.show() return { \u0026#39;mean\u0026#39;: np.mean(values), \u0026#39;median\u0026#39;: np.median(values), \u0026#39;std\u0026#39;: np.std(values) } def plot_clustering_distribution(G): \u0026#34;\u0026#34;\u0026#34;Plot clustering coefficient distribution\u0026#34;\u0026#34;\u0026#34; clustering = nx.clustering(G) values = list(clustering.values()) plt.figure(figsize=(10, 6)) plt.hist(values, bins=30, alpha=0.7, edgecolor=\u0026#39;black\u0026#39;) plt.xlabel(\u0026#39;Clustering Coefficient\u0026#39;) plt.ylabel(\u0026#39;Frequency\u0026#39;) plt.title(\u0026#39;Clustering Coefficient Distribution\u0026#39;) plt.grid(True, alpha=0.3) plt.show() return { \u0026#39;mean\u0026#39;: np.mean(values), \u0026#39;median\u0026#39;: np.median(values), \u0026#39;std\u0026#39;: np.std(values) } These tools provide a foundation for structural analysis of materials networks. The key is to choose appropriate metrics based on your specific application and material system.\nRemember: Always validate your analysis with known test cases and experimental data when possible.\nAll code snippets are tested and ready for use. For more advanced features, consider extending these functions with additional NetworkX capabilities.\n","permalink":"https://Linlin-resh.github.io/posts/20250829-test-auto-post/","summary":"\u003cp\u003eh Analysis Functions\u003c/p\u003e\n\u003ch3 id=\"1-stru\"\u003e1. Stru\u003c/h3\u003e\n\u003ch4 id=\"triangle-counting\"\u003eTriangle Counting\u003c/h4\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003eimport\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edef clustering_coefficient_distribution(G):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u0026#34;\u0026#34;Calculate clustering coefficient distribution\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    clustering \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e nx\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eclustering(G)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    values \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elist\u003c/span\u003e(clustering\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003evalues())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003ereturn\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;mean\u0026#39;\u003c/span\u003e: np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003emean(values),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;std\u0026#39;\u003c/span\u003e: np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003estd(values),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;min\u0026#39;\u003c/span\u003e: np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003emin(values),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;max\u0026#39;\u003c/span\u003e: np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003emax(values),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;distribution\u0026#39;\u003c/span\u003e: values\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4 id=\"clique-detection\"\u003eClique Detection\u003c/h4\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#50fa7b\"\u003efind_maximal_cliques\u003c/span\u003e(G, min_size\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#bd93f9\"\u003e3\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u0026#34;\u0026#34;Find maximal cliques of given minimum size\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    cliques \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003efor\u003c/span\u003e clique \u003cspan style=\"color:#ff79c6\"\u003ein\u003c/span\u003e nx\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003efind_cliques(G):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#ff79c6\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elen\u003c/span\u003e(clique) \u003cspan style=\"color:#ff79c6\"\u003e\u0026gt;=\u003c/span\u003e min_size:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            cliques\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eappend(clique)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    retu] \u003cspan style=\"color:#ff79c6\"\u003e+=\u003c/span\u003e \u003cspan style=\"color:#bd93f9\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003ereturn\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;counts\u0026#39;\u003c/span\u003e: \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003edict\u003c/span\u003e(size_counts),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;total\u0026#39;\u003c/span\u003e: \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elen\u003c/span\u003e(clique_sizes),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;max_size\u0026#39;\u003c/span\u003e: \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003emax\u003c/span\u003e(clique_sizes),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;avg_size\u0026#39;\u003c/span\u003e: np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003emean(clique_sizes)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"2-disorder-quantification\"\u003e2. Disorder Quantification\u003c/h3\u003e\n\u003ch4 id=\"local-order-parameters\"\u003eLocal Order Parameters\u003c/h4\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#50fa7b\"\u003ecalculate_local_order\u003c/span\u003e(G, reference_distances\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ff79c6\"\u003eNone\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u0026#34;\u0026#34;Calculate local order parameters for each node\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    local_order \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e {}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003efor\u003c/span\u003e node \u003cspan style=\"color:#ff79c6\"\u003ein\u003c/span\u003e G\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003enodes():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        neighbors \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elist\u003c/span\u003e(G\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eneighbors(node))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#ff79c6\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elen\u003c/span\u003e(neighbors) \u003cspan style=\"color:#ff79c6\"\u003e\u0026lt;\u003c/span\u003e \u003cspan style=\"color:#bd93f9\"\u003e2\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            local_order[node] \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#bd93f9\"\u003e0.0\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#ff79c6\"\u003econtinue\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#6272a4\"\u003e# Calces is None:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#6272a4\"\u003e# Assume ideal structure has all neighbors connected\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            reference_distances \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e [\u003cspan style=\"color:#bd93f9\"\u003e1.0\u003c/span\u003e] \u003cspan style=\"color:#ff79c6\"\u003e*\u003c/span\u003e \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elen\u003c/span\u003e(actual_distances)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#6272a4\"\u003e# Calculate disorder as RMS deviation\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        deviations \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003earray(actual_distances) \u003cspan style=\"color:#ff79c6\"\u003e-\u003c/span\u003e np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003earray(reference_distances)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        local_order[node] \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003esqrt(np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003emean(deviations\u003cspan style=\"color:#ff79c6\"\u003e**\u003c/span\u003e\u003cspan style=\"color:#bd93f9\"\u003e2\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003ereturn\u003c/span\u003e local_order\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#50fa7b\"\u003eglobal_disorder_metric\u003c/span\u003e(G):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u0026#34;\u0026#34;Calculate global disorder metric for the entire graph\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    local_order \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e calculate_local_order(G)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003ereturn\u003c/span\u003e np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003emean(\u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elist\u003c/span\u003e(local_order\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003evalues()))\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"4-visualization-tools\"\u003e4. Visualization Tools\u003c/h3\u003e\n\u003ch4 id=\"structural-motif-visualization\"\u003eStructural Motif Visualization\u003c/h4\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003eimport\u003c/span\u003e matplotlib.pyplot \u003cspan style=\"color:#ff79c6\"\u003eas\u003c/span\u003e plt\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#50fa7b\"\u003eplot_triangle_distribution\u003c/span\u003e(G):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u0026#34;\u0026#34;Plot distribution of triangles per node\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    triangles \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e count_triangles_by_node(G)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    values \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elist\u003c/span\u003e(triangles\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003evalues())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003efigure(figsize\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e(\u003cspan style=\"color:#bd93f9\"\u003e10\u003c/span\u003e, \u003cspan style=\"color:#bd93f9\"\u003e6\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003ehist(values, bins\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#bd93f9\"\u003e20\u003c/span\u003e, alpha\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#bd93f9\"\u003e0.7\u003c/span\u003e, edgecolor\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;black\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003exlabel(\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;Number of Triangles\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eylabel(\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;Frequency\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003etitle(\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;Triangle Distribution\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003egrid(\u003cspan style=\"color:#ff79c6\"\u003eTrue\u003c/span\u003e, alpha\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#bd93f9\"\u003e0.3\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eshow()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003ereturn\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;mean\u0026#39;\u003c/span\u003e: np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003emean(values),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;median\u0026#39;\u003c/span\u003e: np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003emedian(values),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;std\u0026#39;\u003c/span\u003e: np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003estd(values)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#50fa7b\"\u003eplot_clustering_distribution\u003c/span\u003e(G):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u0026#34;\u0026#34;Plot clustering coefficient distribution\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    clustering \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e nx\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eclustering(G)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    values \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elist\u003c/span\u003e(clustering\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003evalues())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003efigure(figsize\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e(\u003cspan style=\"color:#bd93f9\"\u003e10\u003c/span\u003e, \u003cspan style=\"color:#bd93f9\"\u003e6\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003ehist(values, bins\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#bd93f9\"\u003e30\u003c/span\u003e, alpha\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#bd93f9\"\u003e0.7\u003c/span\u003e, edgecolor\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;black\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003exlabel(\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;Clustering Coefficient\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eylabel(\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;Frequency\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003etitle(\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;Clustering Coefficient Distribution\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003egrid(\u003cspan style=\"color:#ff79c6\"\u003eTrue\u003c/span\u003e, alpha\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#bd93f9\"\u003e0.3\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eshow()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003ereturn\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;mean\u0026#39;\u003c/span\u003e: np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003emean(values),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;median\u0026#39;\u003c/span\u003e: np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003emedian(values),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;std\u0026#39;\u003c/span\u003e: np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003estd(values)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThese tools provide a foundation for structural analysis of materials networks. The key is to choose appropriate metrics based on your specific application and material system.\u003c/p\u003e","title":" 20250829 test auto post"},{"content":"引言 文章内容\u0026hellip;\n数学公式 使用 KaTeX 语法：\n行内公式：$E = mc^2$ 块级公式：$$\\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi}$$ 代码块 def hello_world(): print(\u0026#34;Hello, World!\u0026#34;) 结论 总结\u0026hellip;\n","permalink":"https://Linlin-resh.github.io/posts/20250829-template/","summary":"\u003ch2 id=\"引言\"\u003e引言\u003c/h2\u003e\n\u003cp\u003e文章内容\u0026hellip;\u003c/p\u003e\n\u003ch2 id=\"数学公式\"\u003e数学公式\u003c/h2\u003e\n\u003cp\u003e使用 KaTeX 语法：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e行内公式：$E = mc^2$\u003c/li\u003e\n\u003cli\u003e块级公式：$$\\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi}$$\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"代码块\"\u003e代码块\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#50fa7b\"\u003ehello_world\u003c/span\u003e():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003eprint\u003c/span\u003e(\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;Hello, World!\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"结论\"\u003e结论\u003c/h2\u003e\n\u003cp\u003e总结\u0026hellip;\u003c/p\u003e","title":"20250829 template"},{"content":"Math Formula Display Test This page tests the math formula rendering optimization for better web display.\nInline Math Here are some inline math formulas: $E = mc^2$, $P(k) = \\frac{N_k}{N}$, and $\\sum_{k=0}^{\\infty} P(k) = 1$.\nDisplay Math Here are some display math formulas:\nDegree Distribution: $$P(k) = \\frac{\\text{Number of nodes with degree } k}{n}$$\nProperties: $$\\sum_{k=0}^{\\infty} P(k) = 1$$\n$$\\langle k \\rangle = \\sum_{k=0}^{\\infty} k P(k)$$\n$$\\langle k^2 \\rangle = \\sum_{k=0}^{\\infty} k^2 P(k)$$\nDegree Moments: $$\\langle k \\rangle = \\frac{2m}{n}$$\n$$\\langle k^2 \\rangle = \\frac{1}{n} \\sum_{i=1}^n k_i^2$$\n$$\\sigma_k^2 = \\langle k^2 \\rangle - \\langle k \\rangle^2$$\n$$CV = \\frac{\\sigma_k}{\\langle k \\rangle}$$\nComplex Formulas Network Efficiency: $$E = \\frac{1}{n(n-1)} \\sum_{i \\neq j} \\frac{1}{d_{ij}}$$\nClustering Coefficient: $$C_i = \\frac{2e_i}{k_i(k_i-1)}$$\nModularity: $$Q = \\frac{1}{2m} \\sum_{ij} \\left[ A_{ij} - \\frac{k_i k_j}{2m} \\right] \\delta(c_i, c_j)$$\nMatrix Formulas Adjacency Matrix: $$A_{ij} = \\begin{cases} 1 \u0026amp; \\text{if nodes } i \\text{ and } j \\text{ are connected} \\ 0 \u0026amp; \\text{otherwise} \\end{cases}$$\nLaplacian Matrix: $$L = D - A$$\nEigenvalue Decomposition: $$A = U \\Lambda U^T$$\nLong Formulas SIR Model: $$\\frac{dS}{dt} = -\\beta S I$$ $$\\frac{dI}{dt} = \\beta S I - \\gamma I$$ $$\\frac{dR}{dt} = \\gamma I$$\nKuramoto Model: $$\\frac{d\\theta_i}{dt} = \\omega_i + \\frac{K}{N} \\sum_{j=1}^N A_{ij} \\sin(\\theta_j - \\theta_i)$$\nMathematical Tables Property Formula Description Degree $k_i = \\sum_j A_{ij}$ Number of connections Clustering $C_i = \\frac{2e_i}{k_i(k_i-1)}$ Local clustering coefficient Betweenness $C_B(i) = \\sum_{s \\neq i \\neq t} \\frac{\\sigma_{st}(i)}{\\sigma_{st}}$ Betweenness centrality Eigenvector $x_i = \\frac{1}{\\lambda} \\sum_j A_{ij} x_j$ Eigenvector centrality Code with Math Here\u0026rsquo;s some Python code that uses the mathematical concepts:\nimport networkx as nx import numpy as np def calculate_network_properties(G): \u0026#34;\u0026#34;\u0026#34;Calculate various network properties\u0026#34;\u0026#34;\u0026#34; # Degree distribution degrees = [G.degree(i) for i in G.nodes()] n = G.number_of_nodes() # Average degree: ⟨k⟩ = 2m/n avg_degree = 2 * G.number_of_edges() / n # Second moment: ⟨k²⟩ = (1/n) Σ k_i² second_moment = np.mean([k**2 for k in degrees]) # Clustering coefficient: C_i = 2e_i/(k_i(k_i-1)) clustering = nx.average_clustering(G) # Global efficiency: E = (1/(n(n-1))) Σ (1/d_ij) efficiency = nx.global_efficiency(G) return { \u0026#39;avg_degree\u0026#39;: avg_degree, \u0026#39;second_moment\u0026#39;: second_moment, \u0026#39;clustering\u0026#39;: clustering, \u0026#39;efficiency\u0026#39;: efficiency } Conclusion This test page demonstrates the improved math formula rendering with:\nBetter spacing around mathematical expressions Enhanced typography for better readability Responsive design that works on mobile devices Dark mode support for better viewing experience Formula numbering for easy reference Consistent styling across different math environments The math formulas should now display much better on the web compared to the original Obsidian rendering.\n","permalink":"https://Linlin-resh.github.io/posts/math-test/","summary":"\u003ch2 id=\"math-formula-display-test\"\u003eMath Formula Display Test\u003c/h2\u003e\n\u003cp\u003eThis page tests the math formula rendering optimization for better web display.\u003c/p\u003e\n\u003ch3 id=\"inline-math\"\u003eInline Math\u003c/h3\u003e\n\u003cp\u003eHere are some inline math formulas: $E = mc^2$, $P(k) = \\frac{N_k}{N}$, and $\\sum_{k=0}^{\\infty} P(k) = 1$.\u003c/p\u003e\n\u003ch3 id=\"display-math\"\u003eDisplay Math\u003c/h3\u003e\n\u003cp\u003eHere are some display math formulas:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDegree Distribution:\u003c/strong\u003e\n$$P(k) = \\frac{\\text{Number of nodes with degree } k}{n}$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eProperties:\u003c/strong\u003e\n$$\\sum_{k=0}^{\\infty} P(k) = 1$$\u003c/p\u003e\n\u003cp\u003e$$\\langle k \\rangle = \\sum_{k=0}^{\\infty} k P(k)$$\u003c/p\u003e\n\u003cp\u003e$$\\langle k^2 \\rangle = \\sum_{k=0}^{\\infty} k^2 P(k)$$\u003c/p\u003e","title":"Math Formula Display Test"},{"content":"Quick Math Test Inline Math $E = mc^2$ and $P(k) = \\frac{N_k}{N}$\nDisplay Math $$P(k) = \\frac{\\text{Number of nodes with degree } k}{n}$$\n$$\\sum_{k=0}^{\\infty} P(k) = 1$$\n$$\\langle k \\rangle = \\sum_{k=0}^{\\infty} k P(k)$$\nComplex Formula $$Q = \\frac{1}{2m} \\sum_{ij} \\left[ A_{ij} - \\frac{k_i k_j}{2m} \\right] \\delta(c_i, c_j)$$\nThis should render much better now!\n","permalink":"https://Linlin-resh.github.io/posts/math-quick-test/","summary":"\u003ch2 id=\"quick-math-test\"\u003eQuick Math Test\u003c/h2\u003e\n\u003ch3 id=\"inline-math\"\u003eInline Math\u003c/h3\u003e\n\u003cp\u003e$E = mc^2$ and $P(k) = \\frac{N_k}{N}$\u003c/p\u003e\n\u003ch3 id=\"display-math\"\u003eDisplay Math\u003c/h3\u003e\n\u003cp\u003e$$P(k) = \\frac{\\text{Number of nodes with degree } k}{n}$$\u003c/p\u003e\n\u003cp\u003e$$\\sum_{k=0}^{\\infty} P(k) = 1$$\u003c/p\u003e\n\u003cp\u003e$$\\langle k \\rangle = \\sum_{k=0}^{\\infty} k P(k)$$\u003c/p\u003e\n\u003ch3 id=\"complex-formula\"\u003eComplex Formula\u003c/h3\u003e\n\u003cp\u003e$$Q = \\frac{1}{2m} \\sum_{ij} \\left[ A_{ij} - \\frac{k_i k_j}{2m} \\right] \\delta(c_i, c_j)$$\u003c/p\u003e\n\u003cp\u003eThis should render much better now!\u003c/p\u003e","title":"Quick Math Test"},{"content":"Introduction Chapter 1 of Newman\u0026rsquo;s seminal work Networks: An Introduction serves as the foundation for understanding network science. This chapter establishes the mathematical framework, introduces key concepts, and provides compelling real-world examples that demonstrate the ubiquity and importance of networks in our world.\n1.1 Why Networks Matter The Ubiquity of Networks Networks are everywhere in both natural and artificial systems:\nBiological systems: Neural networks, protein interaction networks, metabolic pathways Social systems: Friendship networks, collaboration networks, communication networks Technological systems: Internet, power grids, transportation networks Information systems: World Wide Web, citation networks, knowledge graphs Why Study Networks? Understanding network structure helps us:\nPredict system behavior and evolution Optimize performance and efficiency Identify critical components and vulnerabilities Design better systems based on network principles 1.2 Real-World Network Examples The Internet The Internet represents one of the most studied technological networks:\nNodes: Routers, servers, and end-user devices Edges: Physical and logical connections Scale: Billions of nodes, trillions of connections Properties: High clustering, short path lengths, scale-free degree distribution Social Networks Human social networks exhibit fascinating properties:\nSix degrees of separation: Any two people are connected by at most 6 steps Small-world effect: Short average path lengths despite large size Homophily: People tend to connect with similar others Community structure: Dense clusters with sparse interconnections Biological Networks Protein-Protein Interaction Networks Nodes: Proteins Edges: Physical interactions between proteins Scale: ~20,000 human proteins, ~100,000 interactions Properties: Scale-free, modular structure, essential proteins are highly connected Metabolic Networks Nodes: Metabolites (small molecules) Edges: Biochemical reactions Scale: Thousands of metabolites and reactions Properties: Hierarchical organization, conserved across species 1.3 Fundamental Network Properties Degree Distribution The degree $k_i$ of node $i$ is the number of edges connected to it.\nFor a network with $n$ nodes, the degree distribution $P(k)$ gives the probability that a randomly chosen node has degree $k$:\n$$P(k) = \\frac{\\text{Number of nodes with degree } k}{n}$$\nMathematical Properties Normalization: $\\sum_{k=0}^{\\infty} P(k) = 1$ Average degree: $\\langle k \\rangle = \\sum_{k=0}^{\\infty} k P(k)$ Second moment: $\\langle k^2 \\rangle = \\sum_{k=0}^{\\infty} k^2 P(k)$ Clustering Coefficient The local clustering coefficient $C_i$ measures how tightly connected the neighbors of node $i$ are:\n$$C_i = \\frac{\\text{Number of triangles containing node } i}{\\binom{k_i}{2}} = \\frac{2e_i}{k_i(k_i-1)}$$\nWhere $e_i$ is the number of edges between neighbors of node $i$.\nThe global clustering coefficient is the average:\n$$C = \\frac{1}{n} \\sum_{i=1}^{n} C_i$$\nPhysical Interpretation $C_i = 1$: All neighbors of $i$ are connected (complete subgraph) $C_i = 0$: No connections between neighbors of $i$ High clustering indicates local order and community structure Path Length and Diameter Shortest Path Length The shortest path length $d_{ij}$ between nodes $i$ and $j$ is the minimum number of edges in any path connecting them.\nAverage Path Length $$L = \\frac{1}{\\binom{n}{2}} \\sum_{i\u0026lt;j} d_{ij} = \\frac{2}{n(n-1)} \\sum_{i\u0026lt;j} d_{ij}$$\nNetwork Diameter $$D = \\max_{i,j} d_{ij}$$\nSmall-World Effect Many real networks exhibit the small-world property:\nShort average path length: $L \\sim \\log n$ (much smaller than $n$) High clustering: $C \\gg C_{\\text{random}}$ (much higher than random networks) This combination is rare in random networks but common in real-world systems.\n1.4 Scale-Free Networks Power-Law Degree Distribution Many real networks follow a power-law degree distribution:\n$$P(k) \\sim k^{-\\gamma}$$\nWhere $\\gamma$ is the exponent (typically $2 \u0026lt; \\gamma \u0026lt; 3$).\nMathematical Properties Heavy-tailed: Few high-degree nodes, many low-degree nodes Scale-invariant: No characteristic scale Infinite variance: When $\\gamma \\leq 3$, $\\langle k^2 \\rangle$ diverges Real-World Examples World Wide Web In-degree: $\\gamma \\approx 2.1$ (pages linking to a page) Out-degree: $\\gamma \\approx 2.7$ (pages linked by a page) Internet Router Network Degree distribution: $\\gamma \\approx 2.2$ Implication: Few highly connected routers, many peripheral ones Scientific Collaboration Networks Degree distribution: $\\gamma \\approx 2.1$ Implication: Few highly collaborative scientists, many with few collaborators 1.5 Network Robustness and Vulnerability Attack Tolerance Scale-free networks exhibit robustness against random failures but vulnerability to targeted attacks:\nRandom failures: Removing random nodes rarely affects connectivity Targeted attacks: Removing high-degree nodes quickly fragments the network Mathematical Framework The percolation threshold $p_c$ is the critical fraction of nodes that must be removed to fragment the network:\n$$p_c = 1 - \\frac{1}{\\kappa - 1}$$\nWhere $\\kappa = \\frac{\\langle k^2 \\rangle}{\\langle k \\rangle}$ is the degree ratio.\n1.6 Applications to Materials Science Silver Nanowire Networks Network concepts apply directly to nanowire systems:\nNodes: Nanowire junctions Edges: Nanowire segments Properties: Percolation threshold, electrical conductivity, mechanical strength Percolation Theory The percolation probability $P(p)$ gives the probability that a randomly chosen node belongs to the giant component:\n$$P(p) = 1 - \\sum_{k=0}^{\\infty} P(k) (1-p)^k$$\nPartially Disordered Materials Network analysis helps understand:\nLocal order parameters in disordered regions Defect clustering and percolation Phase transition mechanisms Property-structure relationships Code Example: Basic Network Analysis import networkx as nx import numpy as np import matplotlib.pyplot as plt def analyze_network_properties(G): \u0026#34;\u0026#34;\u0026#34;Analyze fundamental network properties\u0026#34;\u0026#34;\u0026#34; # Basic statistics n = G.number_of_nodes() m = G.number_of_edges() # Degree distribution degrees = [d for n, d in G.degree()] avg_degree = np.mean(degrees) # Clustering coefficient clustering = nx.average_clustering(G) # Path length (for connected components) if nx.is_connected(G): avg_path_length = nx.average_shortest_path_length(G) diameter = nx.diameter(G) else: # For disconnected graphs, analyze largest component largest_cc = max(nx.connected_components(G), key=len) subgraph = G.subgraph(largest_cc) avg_path_length = nx.average_shortest_path_length(subgraph) diameter = nx.diameter(subgraph) return { \u0026#39;nodes\u0026#39;: n, \u0026#39;edges\u0026#39;: m, \u0026#39;avg_degree\u0026#39;: avg_degree, \u0026#39;clustering\u0026#39;: clustering, \u0026#39;avg_path_length\u0026#39;: avg_path_length, \u0026#39;diameter\u0026#39;: diameter } # Example: Analyze a scale-free network G = nx.barabasi_albert_graph(1000, 3) properties = analyze_network_properties(G) print(f\u0026#34;Network properties:\u0026#34;) for key, value in properties.items(): print(f\u0026#34;{key}: {value:.4f}\u0026#34;) Key Takeaways Networks are universal: They appear in virtually every complex system Mathematical framework: Degree distribution, clustering, and path length are fundamental measures Scale-free property: Many real networks follow power-law degree distributions Small-world effect: Short path lengths with high clustering are common Robustness-vulnerability trade-off: Scale-free networks are robust to random failures but vulnerable to targeted attacks Materials applications: Network concepts directly apply to nanowire systems and disordered materials References Newman, M. E. J. (2010). Networks: An Introduction. Oxford University Press. Barabási, A. L., \u0026amp; Albert, R. (1999). Emergence of scaling in random networks. Science, 286(5439), 509-512. Watts, D. J., \u0026amp; Strogatz, S. H. (1998). Collective dynamics of \u0026lsquo;small-world\u0026rsquo; networks. Nature, 393(6684), 440-442. This is the first in a series of chapter-by-chapter study notes for Newman\u0026rsquo;s Networks textbook. Each chapter builds upon the previous ones, so understanding these fundamental concepts is crucial for the advanced topics to come.\n","permalink":"https://Linlin-resh.github.io/posts/reading-notes-newman-ch1/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eChapter 1 of Newman\u0026rsquo;s seminal work \u003cem\u003eNetworks: An Introduction\u003c/em\u003e serves as the foundation for understanding network science. This chapter establishes the mathematical framework, introduces key concepts, and provides compelling real-world examples that demonstrate the ubiquity and importance of networks in our world.\u003c/p\u003e\n\u003ch2 id=\"11-why-networks-matter\"\u003e1.1 Why Networks Matter\u003c/h2\u003e\n\u003ch3 id=\"the-ubiquity-of-networks\"\u003eThe Ubiquity of Networks\u003c/h3\u003e\n\u003cp\u003eNetworks are \u003cstrong\u003eeverywhere\u003c/strong\u003e in both natural and artificial systems:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eBiological systems\u003c/strong\u003e: Neural networks, protein interaction networks, metabolic pathways\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSocial systems\u003c/strong\u003e: Friendship networks, collaboration networks, communication networks\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTechnological systems\u003c/strong\u003e: Internet, power grids, transportation networks\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInformation systems\u003c/strong\u003e: World Wide Web, citation networks, knowledge graphs\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"why-study-networks\"\u003eWhy Study Networks?\u003c/h3\u003e\n\u003cp\u003eUnderstanding network structure helps us:\u003c/p\u003e","title":"Reading Notes: Newman's Networks Chapter 1 - Introduction"},{"content":"Introduction Chapter 10 of Newman\u0026rsquo;s Networks: An Introduction explores network processes - the dynamic phenomena that occur on networks. This chapter covers percolation, epidemic spreading, synchronization, and other processes that reveal how network structure affects system behavior.\n10.1 Percolation Theory Bond Percolation Model Definition Bond percolation on a network:\nInitial state: All edges present Process: Remove edges with probability $1-p$ Question: What is the probability of a giant connected component? Mathematical Analysis Percolation probability: $$P(p) = 1 - \\sum_{k=0}^{\\infty} P(k) (1-p)^k$$\nWhere $P(k)$ is the degree distribution.\nCritical threshold: $$p_c = \\frac{1}{\\kappa - 1}$$\nWhere $\\kappa = \\frac{\\langle k^2 \\rangle}{\\langle k \\rangle}$ is the degree ratio.\nPhase Transition Below threshold ($p \u0026lt; p_c$):\nOnly small, isolated components exist Largest component size: $O(\\ln n)$ At threshold ($p = p_c$):\nGiant component emerges Largest component size: $O(n^{2/3})$ Above threshold ($p \u0026gt; p_c$):\nGiant component dominates Largest component size: $O(n)$ Site Percolation Model Definition Site percolation on a network:\nInitial state: All nodes present Process: Remove nodes with probability $1-p$ Question: What is the probability of a giant connected component? Mathematical Analysis Percolation probability: $$P(p) = 1 - \\sum_{k=0}^{\\infty} P(k) (1-p)^k$$\nCritical threshold: $$p_c = \\frac{1}{\\kappa - 1}$$\nSame as bond percolation for degree-uncorrelated networks.\nPercolation on Scale-Free Networks Critical Threshold For power-law degree distribution $P(k) \\sim k^{-\\gamma}$:\n$\\gamma \u0026gt; 3$: $p_c \u0026gt; 0$ (finite threshold) $2 \u0026lt; \\gamma \\leq 3$: $p_c = 0$ (no threshold) $\\gamma \\leq 2$: $p_c = 0$ (no threshold) Mathematical Derivation Second moment: $$\\langle k^2 \\rangle = \\int_{k_{\\min}}^{\\infty} k^2 P(k) , dk$$\nFor power-law distribution: $$\\langle k^2 \\rangle \\sim \\int_{k_{\\min}}^{\\infty} k^{2-\\gamma} , dk$$\nConvergence condition: $\\gamma \u0026gt; 3$\nCritical threshold: $$p_c = \\frac{1}{\\kappa - 1} = \\frac{\\langle k \\rangle}{\\langle k^2 \\rangle - \\langle k \\rangle}$$\nApplications to Materials Science Nanowire Networks Network formation:\nNodes: Nanowire junctions Edges: Nanowire segments Percolation: Electrical connectivity Critical density: $$\\rho_c = \\frac{1}{\\pi r^2 (\\kappa - 1)}$$\nWhere $r$ is the interaction radius.\nConductivity: $$\\sigma \\sim (\\rho - \\rho_c)^t$$\nWhere $t \\approx 2.0$ is the conductivity exponent.\nDefect Networks Defect percolation:\nNodes: Defect sites Edges: Defect interactions Percolation: Defect clustering Critical concentration: $$c_c = \\frac{1}{\\kappa - 1}$$\n10.2 Epidemic Spreading SIR Model Model Definition SIR model on a network:\nS: Susceptible (haven\u0026rsquo;t been infected) I: Infected (can spread the disease) R: Recovered (immune, cannot spread) Dynamics Transition rates:\nS → I: $\\beta$ (infection rate) I → R: $\\gamma$ (recovery rate) Differential equations: $$\\frac{dS_i}{dt} = -\\beta S_i \\sum_{j} A_{ij} I_j$$ $$\\frac{dI_i}{dt} = \\beta S_i \\sum_{j} A_{ij} I_j - \\gamma I_i$$ $$\\frac{dR_i}{dt} = \\gamma I_i$$\nBasic Reproduction Number Definition: $$R_0 = \\frac{\\beta}{\\gamma} \\frac{\\langle k^2 \\rangle}{\\langle k \\rangle}$$\nEpidemic threshold: $$R_0 \u0026gt; 1$$\nCritical infection rate: $$\\beta_c = \\frac{\\gamma \\langle k \\rangle}{\\langle k^2 \\rangle}$$\nSIS Model Model Definition SIS model on a network:\nS: Susceptible I: Infected No recovered state: Infected nodes can become susceptible again Dynamics Transition rates:\nS → I: $\\beta$ (infection rate) I → S: $\\gamma$ (recovery rate) Differential equations: $$\\frac{dS_i}{dt} = -\\beta S_i \\sum_{j} A_{ij} I_j + \\gamma I_i$$ $$\\frac{dI_i}{dt} = \\beta S_i \\sum_{j} A_{ij} I_j - \\gamma I_i$$\nSteady State Infection prevalence: $$\\rho = \\frac{\\beta \\langle k^2 \\rangle}{\\gamma \\langle k \\rangle + \\beta \\langle k^2 \\rangle}$$\nCritical threshold: $$\\beta_c = \\frac{\\gamma \\langle k \\rangle}{\\langle k^2 \\rangle}$$\nComplex Contagion Threshold Models Activation condition: $$\\sum_{j \\in \\text{active neighbors}} w_{ji} \\geq \\theta_i$$\nWhere:\n$w_{ji}$: Influence weight from $j$ to $i$ $\\theta_i$: Threshold for node $i$ Cascade Condition Global cascade condition: $$\\frac{\\langle k^2 \\rangle}{\\langle k \\rangle} \u0026gt; \\frac{1}{\\theta}$$\nWhere $\\theta$ is the average threshold.\nMathematical Analysis Cascade size: $$S = \\sum_{k=0}^{\\infty} P(k) \\sum_{m=0}^{k} \\binom{k}{m} p^m (1-p)^{k-m} \\Theta(m - \\theta k)$$\nWhere $\\Theta(x)$ is the Heaviside step function.\n10.3 Synchronization Kuramoto Model Model Definition Kuramoto model on a network:\n$$\\frac{d\\theta_i}{dt} = \\omega_i + \\frac{K}{N} \\sum_{j=1}^N A_{ij} \\sin(\\theta_j - \\theta_i)$$\nWhere:\n$\\theta_i$: Phase of oscillator $i$ $\\omega_i$: Natural frequency $K$: Coupling strength $A_{ij}$: Adjacency matrix Order Parameters Global order parameter: $$r = \\left| \\frac{1}{N} \\sum_{j=1}^N e^{i\\theta_j} \\right|$$\nLocal order parameter: $$r_i = \\left| \\frac{1}{k_i} \\sum_{j \\in \\mathcal{N}_i} e^{i\\theta_j} \\right|$$\nSynchronization Threshold Critical coupling: $$K_c = \\frac{2}{\\pi g(0)} \\frac{\\langle k^2 \\rangle}{\\langle k \\rangle}$$\nWhere $g(0)$ is the frequency distribution at zero.\nMaster Stability Function Linear Stability Analysis Perturbation equations: $$\\frac{d\\delta \\theta_i}{dt} = \\delta \\omega_i + \\frac{K}{N} \\sum_{j=1}^N A_{ij} \\cos(\\theta_j - \\theta_i) (\\delta \\theta_j - \\delta \\theta_i)$$\nEigenvalue problem: $$\\lambda \\delta \\theta = L \\delta \\theta$$\nWhere $L$ is the Laplacian matrix.\nStability Condition Synchronization is stable if: $$\\lambda_2 \u0026gt; \\frac{\\sigma^2}{K}$$\nWhere $\\lambda_2$ is the second smallest eigenvalue of the Laplacian.\n10.4 Random Walks Simple Random Walk Model Definition Simple random walk on a network:\nStart: Random initial node Move: At each step, move to random neighbor Transition matrix: $P_{ij} = \\frac{A_{ij}}{k_i}$ Stationary Distribution Stationary distribution: $$\\pi_i = \\frac{k_i}{2m}$$\nVerification: $$\\sum_i \\pi_i P_{ij} = \\sum_i \\frac{k_i}{2m} \\frac{A_{ij}}{k_i} = \\frac{1}{2m} \\sum_i A_{ij} = \\frac{k_j}{2m} = \\pi_j$$\nMixing Time Mixing time: $$\\tau_{\\text{mix}} = \\min{t : \\max_i ||P^t(i, \\cdot) - \\pi||_1 \\leq \\epsilon}$$\nSpectral bound: $$\\tau_{\\text{mix}} \\leq \\frac{1}{1 - \\lambda_2} \\log\\left(\\frac{1}{\\epsilon \\pi_{\\min}}\\right)$$\nWhere $\\lambda_2$ is the second largest eigenvalue of $P$.\nPageRank Model Definition PageRank as a random walk:\n$$\\mathbf{PR} = (1-d) \\frac{\\mathbf{1}}{n} + d P^T \\mathbf{PR}$$\nWhere $P$ is the transition matrix.\nMathematical Properties Convergence: Guaranteed for $d \u0026lt; 1$\nStationary distribution: Unique solution\nInterpretation: Probability of being at each node in the long run\n10.5 Applications to Materials Science Information Spreading in Materials Defect Propagation Defect spreading can be modeled as:\nS: Healthy sites I: Defected sites R: Repaired sites Mathematical model: $$\\frac{dS_i}{dt} = -\\beta S_i \\sum_{j} A_{ij} I_j + \\gamma R_i$$ $$\\frac{dI_i}{dt} = \\beta S_i \\sum_{j} A_{ij} I_j - \\delta I_i$$ $$\\frac{dR_i}{dt} = \\delta I_i - \\gamma R_i$$\nCritical Defect Rate Critical defect rate: $$\\beta_c = \\frac{\\delta \\langle k \\rangle}{\\langle k^2 \\rangle}$$\nFor scale-free networks: $\\beta_c \\to 0$ as $n \\to \\infty$\nPhase Transitions in Materials Order-Disorder Transitions Order parameter: $$\\phi = \\frac{1}{N} \\sum_{i=1}^N \\cos(\\theta_i - \\theta_0)$$\nWhere $\\theta_0$ is the preferred orientation.\nCritical temperature: $$T_c = \\frac{K \\langle k^2 \\rangle}{2 \\langle k \\rangle}$$\nPercolation in Disordered Systems Defect percolation: $$P(\\text{percolation}) = 1 - \\sum_{k=0}^{\\infty} P(k) (1-p)^k$$\nCritical concentration: $$c_c = \\frac{1}{\\kappa - 1}$$\nNetwork-Based Materials Design Structure-Property Relationships Property prediction: $$P = f(\\langle k \\rangle, C, L, \\ldots)$$\nNetwork optimization: $$\\min_{\\text{network}} \\sum_i w_i |P_i - P_i^{\\text{target}}|^2$$\nConstraints:\nConnectivity: Network must be connected Degree bounds: $k_{\\min} \\leq k_i \\leq k_{\\max}$ Clustering bounds: $C_{\\min} \\leq C \\leq C_{\\max}$ Code Example: Network Processes import networkx as nx import numpy as np import matplotlib.pyplot as plt from collections import Counter from scipy.integrate import odeint def simulate_percolation(G, p_values): \u0026#34;\u0026#34;\u0026#34;Simulate percolation on network\u0026#34;\u0026#34;\u0026#34; results = [] for p in p_values: # Remove edges with probability 1-p edges_to_remove = [] for edge in G.edges(): if np.random.random() \u0026gt; p: edges_to_remove.append(edge) # Create subgraph G_sub = G.copy() G_sub.remove_edges_from(edges_to_remove) # Analyze components components = list(nx.connected_components(G_sub)) component_sizes = [len(comp) for comp in components] giant_component_size = max(component_sizes) if component_sizes else 0 giant_component_fraction = giant_component_size / G.number_of_nodes() results.append({ \u0026#39;p\u0026#39;: p, \u0026#39;giant_component_fraction\u0026#39;: giant_component_fraction, \u0026#39;num_components\u0026#39;: len(components), \u0026#39;avg_component_size\u0026#39;: np.mean(component_sizes) }) return results def simulate_sir_epidemic(G, beta, gamma, initial_infected=1): \u0026#34;\u0026#34;\u0026#34;Simulate SIR epidemic on network\u0026#34;\u0026#34;\u0026#34; n = G.number_of_nodes() # Initial conditions S = np.ones(n) # Susceptible I = np.zeros(n) # Infected R = np.zeros(n) # Recovered # Initial infected nodes infected_nodes = np.random.choice(n, initial_infected, replace=False) S[infected_nodes] = 0 I[infected_nodes] = 1 # Time points t = np.linspace(0, 100, 1000) # SIR dynamics def sir_equations(y, t): S, I, R = y dS_dt = np.zeros(n) dI_dt = np.zeros(n) dR_dt = np.zeros(n) for i in range(n): # Infection rate infection_rate = 0 for j in G.neighbors(i): infection_rate += I[j] infection_rate *= beta * S[i] # Recovery rate recovery_rate = gamma * I[i] dS_dt[i] = -infection_rate dI_dt[i] = infection_rate - recovery_rate dR_dt[i] = recovery_rate return np.concatenate([dS_dt, dI_dt, dR_dt]) # Solve ODEs y0 = np.concatenate([S, I, R]) sol = odeint(sir_equations, y0, t) # Extract results S_t = sol[:, :n] I_t = sol[:, n:2*n] R_t = sol[:, 2*n:] return t, S_t, I_t, R_t def simulate_kuramoto(G, K, omega, initial_phases=None): \u0026#34;\u0026#34;\u0026#34;Simulate Kuramoto model on network\u0026#34;\u0026#34;\u0026#34; n = G.number_of_nodes() # Initial phases if initial_phases is None: theta = np.random.uniform(0, 2*np.pi, n) else: theta = initial_phases # Time points t = np.linspace(0, 100, 1000) # Kuramoto dynamics def kuramoto_equations(theta, t): dtheta_dt = np.zeros(n) for i in range(n): coupling = 0 for j in G.neighbors(i): coupling += np.sin(theta[j] - theta[i]) coupling *= K / n dtheta_dt[i] = omega[i] + coupling return dtheta_dt # Solve ODEs sol = odeint(kuramoto_equations, theta, t) return t, sol def analyze_network_processes(G, process_type=\u0026#34;percolation\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Analyze network processes\u0026#34;\u0026#34;\u0026#34; if process_type == \u0026#34;percolation\u0026#34;: # Percolation analysis p_values = np.linspace(0.1, 1.0, 20) results = simulate_percolation(G, p_values) # Find critical threshold critical_p = None for i, result in enumerate(results): if result[\u0026#39;giant_component_fraction\u0026#39;] \u0026gt; 0.5: critical_p = result[\u0026#39;p\u0026#39;] break return { \u0026#39;process_type\u0026#39;: process_type, \u0026#39;critical_threshold\u0026#39;: critical_p, \u0026#39;results\u0026#39;: results } elif process_type == \u0026#34;sir\u0026#34;: # SIR epidemic analysis beta = 0.1 gamma = 0.05 t, S_t, I_t, R_t = simulate_sir_epidemic(G, beta, gamma) # Calculate basic reproduction number degrees = [d for n, d in G.degree()] avg_degree = np.mean(degrees) degree_variance = np.var(degrees) R0 = (beta / gamma) * (avg_degree + degree_variance / avg_degree) return { \u0026#39;process_type\u0026#39;: process_type, \u0026#39;R0\u0026#39;: R0, \u0026#39;time\u0026#39;: t, \u0026#39;S\u0026#39;: S_t, \u0026#39;I\u0026#39;: I_t, \u0026#39;R\u0026#39;: R_t } elif process_type == \u0026#34;kuramoto\u0026#34;: # Kuramoto synchronization analysis K = 1.0 omega = np.random.normal(0, 1, G.number_of_nodes()) t, theta_t = simulate_kuramoto(G, K, omega) # Calculate order parameter r_t = np.abs(np.mean(np.exp(1j * theta_t), axis=1)) return { \u0026#39;process_type\u0026#39;: process_type, \u0026#39;time\u0026#39;: t, \u0026#39;phases\u0026#39;: theta_t, \u0026#39;order_parameter\u0026#39;: r_t } def plot_network_processes(G, process_type=\u0026#34;percolation\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Plot network process results\u0026#34;\u0026#34;\u0026#34; results = analyze_network_processes(G, process_type) if process_type == \u0026#34;percolation\u0026#34;: p_values = [r[\u0026#39;p\u0026#39;] for r in results[\u0026#39;results\u0026#39;]] giant_fractions = [r[\u0026#39;giant_component_fraction\u0026#39;] for r in results[\u0026#39;results\u0026#39;]] plt.figure(figsize=(10, 6)) plt.plot(p_values, giant_fractions, \u0026#39;bo-\u0026#39;, markersize=8) plt.axvline(x=results[\u0026#39;critical_threshold\u0026#39;], color=\u0026#39;r\u0026#39;, linestyle=\u0026#39;--\u0026#39;, label=f\u0026#39;Critical threshold: {results[\u0026#34;critical_threshold\u0026#34;]:.3f}\u0026#39;) plt.xlabel(\u0026#39;Probability p\u0026#39;) plt.ylabel(\u0026#39;Giant Component Fraction\u0026#39;) plt.title(\u0026#39;Percolation Phase Transition\u0026#39;) plt.legend() plt.grid(True, alpha=0.3) plt.show() elif process_type == \u0026#34;sir\u0026#34;: t = results[\u0026#39;time\u0026#39;] S_t = results[\u0026#39;S\u0026#39;] I_t = results[\u0026#39;I\u0026#39;] R_t = results[\u0026#39;R\u0026#39;] plt.figure(figsize=(12, 8)) # Plot SIR curves plt.subplot(2, 2, 1) plt.plot(t, np.mean(S_t, axis=1), \u0026#39;b-\u0026#39;, label=\u0026#39;Susceptible\u0026#39;) plt.plot(t, np.mean(I_t, axis=1), \u0026#39;r-\u0026#39;, label=\u0026#39;Infected\u0026#39;) plt.plot(t, np.mean(R_t, axis=1), \u0026#39;g-\u0026#39;, label=\u0026#39;Recovered\u0026#39;) plt.xlabel(\u0026#39;Time\u0026#39;) plt.ylabel(\u0026#39;Fraction\u0026#39;) plt.title(\u0026#39;SIR Epidemic Dynamics\u0026#39;) plt.legend() plt.grid(True, alpha=0.3) # Plot R0 plt.subplot(2, 2, 2) plt.bar([\u0026#39;R0\u0026#39;], [results[\u0026#39;R0\u0026#39;]], color=\u0026#39;orange\u0026#39;) plt.ylabel(\u0026#39;Basic Reproduction Number\u0026#39;) plt.title(\u0026#39;Epidemic Threshold\u0026#39;) plt.grid(True, alpha=0.3) # Plot individual node dynamics plt.subplot(2, 2, 3) for i in range(min(5, S_t.shape[1])): plt.plot(t, S_t[:, i], \u0026#39;b-\u0026#39;, alpha=0.3) plt.plot(t, I_t[:, i], \u0026#39;r-\u0026#39;, alpha=0.3) plt.plot(t, R_t[:, i], \u0026#39;g-\u0026#39;, alpha=0.3) plt.xlabel(\u0026#39;Time\u0026#39;) plt.ylabel(\u0026#39;Fraction\u0026#39;) plt.title(\u0026#39;Individual Node Dynamics\u0026#39;) plt.grid(True, alpha=0.3) # Plot network visualization plt.subplot(2, 2, 4) pos = nx.spring_layout(G, k=1, iterations=50) nx.draw(G, pos, node_size=50, node_color=\u0026#39;lightblue\u0026#39;, edge_color=\u0026#39;gray\u0026#39;, alpha=0.6) plt.title(\u0026#39;Network Structure\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() elif process_type == \u0026#34;kuramoto\u0026#34;: t = results[\u0026#39;time\u0026#39;] theta_t = results[\u0026#39;phases\u0026#39;] r_t = results[\u0026#39;order_parameter\u0026#39;] plt.figure(figsize=(12, 8)) # Plot order parameter plt.subplot(2, 2, 1) plt.plot(t, r_t, \u0026#39;b-\u0026#39;, linewidth=2) plt.xlabel(\u0026#39;Time\u0026#39;) plt.ylabel(\u0026#39;Order Parameter r\u0026#39;) plt.title(\u0026#39;Synchronization Order Parameter\u0026#39;) plt.grid(True, alpha=0.3) # Plot phase evolution plt.subplot(2, 2, 2) for i in range(min(10, theta_t.shape[1])): plt.plot(t, theta_t[:, i], alpha=0.7) plt.xlabel(\u0026#39;Time\u0026#39;) plt.ylabel(\u0026#39;Phase θ\u0026#39;) plt.title(\u0026#39;Phase Evolution\u0026#39;) plt.grid(True, alpha=0.3) # Plot phase distribution plt.subplot(2, 2, 3) final_phases = theta_t[-1, :] plt.hist(final_phases, bins=20, alpha=0.7, edgecolor=\u0026#39;black\u0026#39;) plt.xlabel(\u0026#39;Final Phase\u0026#39;) plt.ylabel(\u0026#39;Count\u0026#39;) plt.title(\u0026#39;Final Phase Distribution\u0026#39;) plt.grid(True, alpha=0.3) # Plot network visualization plt.subplot(2, 2, 4) pos = nx.spring_layout(G, k=1, iterations=50) nx.draw(G, pos, node_size=50, node_color=\u0026#39;lightblue\u0026#39;, edge_color=\u0026#39;gray\u0026#39;, alpha=0.6) plt.title(\u0026#39;Network Structure\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() # Example: Analyze network processes G = nx.barabasi_albert_graph(100, 3) # Percolation analysis print(\u0026#34;Percolation Analysis:\u0026#34;) percolation_results = analyze_network_processes(G, \u0026#34;percolation\u0026#34;) print(f\u0026#34;Critical threshold: {percolation_results[\u0026#39;critical_threshold\u0026#39;]:.3f}\u0026#34;) # SIR epidemic analysis print(\u0026#34;\\nSIR Epidemic Analysis:\u0026#34;) sir_results = analyze_network_processes(G, \u0026#34;sir\u0026#34;) print(f\u0026#34;Basic reproduction number: {sir_results[\u0026#39;R0\u0026#39;]:.3f}\u0026#34;) # Kuramoto synchronization analysis print(\u0026#34;\\nKuramoto Synchronization Analysis:\u0026#34;) kuramoto_results = analyze_network_processes(G, \u0026#34;kuramoto\u0026#34;) print(f\u0026#34;Final order parameter: {kuramoto_results[\u0026#39;order_parameter\u0026#39;][-1]:.3f}\u0026#34;) # Plot results plot_network_processes(G, \u0026#34;percolation\u0026#34;) plot_network_processes(G, \u0026#34;sir\u0026#34;) plot_network_processes(G, \u0026#34;kuramoto\u0026#34;) Key Takeaways Percolation theory: Provides framework for understanding connectivity transitions Epidemic spreading: Network structure affects disease transmission dynamics Synchronization: Coupling strength and network topology determine synchronization Random walks: Reveal network structure and mixing properties Phase transitions: Critical thresholds determine system behavior Applications: Network processes help understand materials science phenomena Mathematical analysis: Rigorous theory enables prediction of dynamic behavior References Newman, M. E. J. (2010). Networks: An Introduction. Oxford University Press. Stauffer, D., \u0026amp; Aharony, A. (1994). Introduction to Percolation Theory. Taylor \u0026amp; Francis. Pastor-Satorras, R., \u0026amp; Vespignani, A. (2001). Epidemic spreading in scale-free networks. Physical Review Letters, 86(14), 3200. Kuramoto, Y. (1984). Chemical Oscillations, Waves, and Turbulence. Springer. Network processes provide insights into how dynamic phenomena unfold on complex networks, with important applications in understanding materials behavior and system dynamics.\n","permalink":"https://Linlin-resh.github.io/posts/reading-notes-newman-ch10/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eChapter 10 of Newman\u0026rsquo;s \u003cem\u003eNetworks: An Introduction\u003c/em\u003e explores \u003cstrong\u003enetwork processes\u003c/strong\u003e - the dynamic phenomena that occur on networks. This chapter covers percolation, epidemic spreading, synchronization, and other processes that reveal how network structure affects system behavior.\u003c/p\u003e\n\u003ch2 id=\"101-percolation-theory\"\u003e10.1 Percolation Theory\u003c/h2\u003e\n\u003ch3 id=\"bond-percolation\"\u003eBond Percolation\u003c/h3\u003e\n\u003ch4 id=\"model-definition\"\u003eModel Definition\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eBond percolation\u003c/strong\u003e on a network:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eInitial state\u003c/strong\u003e: All edges present\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eProcess\u003c/strong\u003e: Remove edges with probability $1-p$\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eQuestion\u003c/strong\u003e: What is the probability of a giant connected component?\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"mathematical-analysis\"\u003eMathematical Analysis\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003ePercolation probability\u003c/strong\u003e:\n$$P(p) = 1 - \\sum_{k=0}^{\\infty} P(k) (1-p)^k$$\u003c/p\u003e","title":"Reading Notes: Newman's Networks Chapter 10 - Network Processes"},{"content":"Introduction Chapter 11 of Newman\u0026rsquo;s Networks: An Introduction focuses on community structure - the identification of densely connected groups of nodes within networks. This chapter covers various methods for detecting communities, their mathematical foundations, and applications to understanding network organization.\n11.1 What is Community Structure? Definition Community structure refers to the presence of groups of nodes that are:\nDensely connected within groups Sparsely connected between groups Functionally related or similar in some way Mathematical Framework Community detection aims to partition the network into communities such that:\n$$\\max \\sum_{c} \\frac{e_c}{m} - \\left(\\frac{k_c}{2m}\\right)^2$$\nWhere:\n$e_c$: Number of edges within community $c$ $k_c$: Total degree of nodes in community $c$ $m$: Total number of edges 11.2 Modularity Definition Modularity measures the quality of a community partition:\n$$Q = \\frac{1}{2m} \\sum_{ij} \\left[ A_{ij} - \\frac{k_i k_j}{2m} \\right] \\delta(c_i, c_j)$$\nWhere:\n$A_{ij}$: Adjacency matrix $k_i, k_j$: Degrees of nodes $i, j$ $c_i, c_j$: Community assignments $\\delta(c_i, c_j)$: Kronecker delta Properties Range: $Q \\in [-1, 1]$\nInterpretation:\n$Q \u0026gt; 0$: More edges within communities than expected by chance $Q = 0$: Random network structure $Q \u0026lt; 0$: Fewer edges within communities than expected Maximum modularity: $$Q_{\\max} = 1 - \\frac{1}{2m} \\sum_{c} \\frac{k_c^2}{2m}$$\nResolution Limit Problem: Modularity may not detect small communities\nResolution limit: $$Q_{\\text{max}} = 1 - \\frac{1}{2m} \\sum_{c} \\frac{k_c^2}{2m}$$\nSmall communities: May not be detected if $k_c \u0026lt; \\sqrt{2m}$\n11.3 Modularity Optimization Greedy Algorithm Algorithm:\nStart with each node in its own community Merge communities that increase modularity Repeat until no improvement possible Modularity change: $$\\Delta Q = \\frac{1}{2m} \\left[ 2e_{ij} - \\frac{k_i k_j}{m} \\right]$$\nWhere $e_{ij}$ is the number of edges between communities $i$ and $j$.\nLouvain Algorithm Two-phase algorithm:\nPhase 1: Local optimization\nFor each node, move to community that maximizes modularity Repeat until no improvement Phase 2: Community aggregation\nMerge communities into single nodes Repeat Phase 1 Time complexity: $O(m \\log n)$\nSimulated Annealing Energy function: $E = -Q$\nTemperature schedule: $T(t) = T_0 e^{-\\alpha t}$\nAcceptance probability: $P = e^{-\\Delta E/T}$\nAdvantages: Can escape local optima\n11.4 Spectral Clustering Laplacian Matrix Unnormalized Laplacian: $$L = D - A$$\nNormalized Laplacian: $$L_{\\text{norm}} = D^{-1/2} L D^{-1/2} = I - D^{-1/2} A D^{-1/2}$$\nRandom walk Laplacian: $$L_{\\text{rw}} = D^{-1} L = I - D^{-1} A$$\nEigenvalue Analysis Eigenvalue decomposition: $$L = U \\Lambda U^T$$\nProperties:\nSmallest eigenvalue: $\\lambda_1 = 0$ (always) Multiplicity of 0: Number of connected components Second smallest eigenvalue: $\\lambda_2 \u0026gt; 0$ if graph is connected Spectral Clustering Algorithm Algorithm:\nCompute Laplacian matrix $L$ Find $k$ smallest eigenvalues and eigenvectors Use eigenvectors to cluster nodes Mathematical foundation: $$\\min_{\\mathbf{x}} \\mathbf{x}^T L \\mathbf{x} \\quad \\text{subject to } \\mathbf{x}^T \\mathbf{1} = 0, \\mathbf{x}^T \\mathbf{x} = n$$\nSolution: Fiedler vector (second eigenvector)\nRatio Cut Ratio cut: $$\\text{RatioCut}(S, T) = \\frac{\\text{cut}(S, T)}{|S|} + \\frac{\\text{cut}(S, T)}{|T|}$$\nSpectral relaxation: $$\\min_{\\mathbf{x}} \\mathbf{x}^T L \\mathbf{x} \\quad \\text{subject to } \\mathbf{x}^T \\mathbf{1} = 0, \\mathbf{x}^T \\mathbf{1} = n$$\nSolution: Fiedler vector\n11.5 Girvan-Newman Algorithm Betweenness Centrality Edge betweenness: $$C_B(e) = \\sum_{s \\neq t} \\frac{\\sigma_{st}(e)}{\\sigma_{st}}$$\nWhere:\n$\\sigma_{st}$: Number of shortest paths from $s$ to $t$ $\\sigma_{st}(e)$: Number of shortest paths from $s$ to $t$ passing through edge $e$ Algorithm Girvan-Newman algorithm:\nCalculate betweenness centrality for all edges Remove edge with highest betweenness Recalculate betweenness for remaining edges Repeat until no edges remain Hierarchical clustering: Dendrogram shows community structure\nTime Complexity Betweenness calculation: $O(nm)$ per edge Total complexity: $O(nm^2)$\nOptimization: Use approximation algorithms\n11.6 Other Community Detection Methods Label Propagation Algorithm:\nInitialize each node with unique label Each node adopts label of majority of neighbors Repeat until convergence Convergence: Guaranteed for connected graphs\nTime complexity: $O(m)$\nInfomap Information-theoretic approach: $$\\min \\sum_{c} p_c \\log p_c + \\sum_{c} \\frac{e_c}{m} \\log \\frac{e_c}{m}$$\nRandom walk perspective: Minimize description length of random walk\nStochastic Block Model Model: $$P(A_{ij} = 1) = \\theta_{c_i c_j}$$\nWhere $\\theta_{c_i c_j}$ is the probability of edge between communities $c_i$ and $c_j$.\nLikelihood: $$L = \\prod_{i\u0026lt;j} \\theta_{c_i c_j}^{A_{ij}} (1 - \\theta_{c_i c_j})^{1 - A_{ij}}$$\n11.7 Applications to Materials Science Defect Clustering Network representation:\nNodes: Defect sites Edges: Defect interactions Communities: Clustered defect regions Mathematical framework: $$Q = \\frac{1}{2m} \\sum_{ij} \\left[ A_{ij} - \\frac{k_i k_j}{2m} \\right] \\delta(c_i, c_j)$$\nApplications:\nDefect percolation: Community structure affects percolation threshold Material properties: Clustered defects influence mechanical properties Phase Separation Network-based phase separation:\nNodes: Atomic sites Edges: Chemical bonds Communities: Different phases Order parameter: $$\\phi = \\frac{1}{N} \\sum_{i=1}^N \\delta(c_i, c_{\\text{phase}})$$\nPhase transition: Community structure changes at critical temperature\nNanowire Networks Community structure in nanowire networks:\nNodes: Nanowire junctions Edges: Nanowire segments Communities: Dense nanowire regions Electrical properties: Community structure affects conductivity\n11.8 Evaluation Metrics Adjusted Rand Index Definition: $$ARI = \\frac{\\sum_{ij} \\binom{n_{ij}}{2} - \\left[\\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2}\\right] / \\binom{n}{2}}{\\frac{1}{2} \\left[\\sum_i \\binom{a_i}{2} + \\sum_j \\binom{b_j}{2}\\right] - \\left[\\sum_i \\binom{a_i}{2} \\sum_j \\binom{b_j}{2}\\right] / \\binom{n}{2}}$$\nWhere:\n$n_{ij}$: Number of nodes in community $i$ of partition $A$ and community $j$ of partition $B$ $a_i$: Number of nodes in community $i$ of partition $A$ $b_j$: Number of nodes in community $j$ of partition $B$ Normalized Mutual Information Definition: $$NMI = \\frac{2I(A, B)}{H(A) + H(B)}$$\nWhere:\n$I(A, B)$: Mutual information between partitions $H(A), H(B)$: Entropy of partitions Modularity Quality measure: Higher modularity indicates better community structure\nLimitations: Resolution limit, may not detect small communities\nCode Example: Community Detection import networkx as nx import numpy as np import matplotlib.pyplot as plt from sklearn.cluster import SpectralClustering from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score from collections import defaultdict def detect_communities_modularity(G): \u0026#34;\u0026#34;\u0026#34;Detect communities using modularity optimization\u0026#34;\u0026#34;\u0026#34; # Greedy modularity optimization communities = nx.community.greedy_modularity_communities(G) modularity = nx.community.modularity(G, communities) return { \u0026#39;communities\u0026#39;: communities, \u0026#39;modularity\u0026#39;: modularity, \u0026#39;num_communities\u0026#39;: len(communities) } def detect_communities_spectral(G, n_communities=3): \u0026#34;\u0026#34;\u0026#34;Detect communities using spectral clustering\u0026#34;\u0026#34;\u0026#34; # Get adjacency matrix A = nx.adjacency_matrix(G).toarray() # Spectral clustering clustering = SpectralClustering(n_clusters=n_communities, affinity=\u0026#39;precomputed\u0026#39;, random_state=42) labels = clustering.fit_predict(A) # Convert to community format communities = defaultdict(set) for i, label in enumerate(labels): communities[label].add(i) communities = list(communities.values()) modularity = nx.community.modularity(G, communities) return { \u0026#39;communities\u0026#39;: communities, \u0026#39;modularity\u0026#39;: modularity, \u0026#39;num_communities\u0026#39;: len(communities) } def detect_communities_girvan_newman(G): \u0026#34;\u0026#34;\u0026#34;Detect communities using Girvan-Newman algorithm\u0026#34;\u0026#34;\u0026#34; # Girvan-Newman algorithm communities = nx.community.girvan_newman(G) # Get the best partition (highest modularity) best_partition = None best_modularity = -1 for partition in communities: modularity = nx.community.modularity(G, partition) if modularity \u0026gt; best_modularity: best_modularity = modularity best_partition = partition return { \u0026#39;communities\u0026#39;: best_partition, \u0026#39;modularity\u0026#39;: best_modularity, \u0026#39;num_communities\u0026#39;: len(best_partition) } def evaluate_communities(G, communities, ground_truth=None): \u0026#34;\u0026#34;\u0026#34;Evaluate community detection results\u0026#34;\u0026#34;\u0026#34; # Convert communities to labels labels = np.zeros(G.number_of_nodes()) for i, community in enumerate(communities): for node in community: labels[node] = i # Calculate metrics results = { \u0026#39;modularity\u0026#39;: nx.community.modularity(G, communities), \u0026#39;num_communities\u0026#39;: len(communities), \u0026#39;avg_community_size\u0026#39;: np.mean([len(c) for c in communities]), \u0026#39;community_size_std\u0026#39;: np.std([len(c) for c in communities]) } # Compare with ground truth if available if ground_truth is not None: ground_truth_labels = np.zeros(G.number_of_nodes()) for i, community in enumerate(ground_truth): for node in community: ground_truth_labels[node] = i results[\u0026#39;ari\u0026#39;] = adjusted_rand_score(ground_truth_labels, labels) results[\u0026#39;nmi\u0026#39;] = normalized_mutual_info_score(ground_truth_labels, labels) return results def plot_communities(G, communities, title=\u0026#34;Community Detection\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Plot network with community structure\u0026#34;\u0026#34;\u0026#34; # Create color map for communities colors = plt.cm.Set3(np.linspace(0, 1, len(communities))) node_colors = {} for i, community in enumerate(communities): color = colors[i] for node in community: node_colors[node] = color # Plot network pos = nx.spring_layout(G, k=1, iterations=50) node_color_list = [node_colors.get(node, \u0026#39;lightblue\u0026#39;) for node in G.nodes()] plt.figure(figsize=(12, 8)) # Main plot plt.subplot(2, 2, 1) nx.draw(G, pos, node_color=node_color_list, edge_color=\u0026#39;gray\u0026#39;, alpha=0.6, node_size=50) plt.title(f\u0026#39;{title} (Modularity: {nx.community.modularity(G, communities):.3f})\u0026#39;) plt.axis(\u0026#39;off\u0026#39;) # Community size distribution plt.subplot(2, 2, 2) community_sizes = [len(c) for c in communities] plt.hist(community_sizes, bins=20, alpha=0.7, edgecolor=\u0026#39;black\u0026#39;) plt.xlabel(\u0026#39;Community Size\u0026#39;) plt.ylabel(\u0026#39;Count\u0026#39;) plt.title(\u0026#39;Community Size Distribution\u0026#39;) plt.grid(True, alpha=0.3) # Modularity vs number of communities plt.subplot(2, 2, 3) n_communities_range = range(2, min(20, G.number_of_nodes())) modularities = [] for n_comm in n_communities_range: # Use spectral clustering for different numbers of communities A = nx.adjacency_matrix(G).toarray() clustering = SpectralClustering(n_clusters=n_comm, affinity=\u0026#39;precomputed\u0026#39;, random_state=42) labels = clustering.fit_predict(A) # Convert to community format comm_dict = defaultdict(set) for i, label in enumerate(labels): comm_dict[label].add(i) comm_list = list(comm_dict.values()) modularity = nx.community.modularity(G, comm_list) modularities.append(modularity) plt.plot(n_communities_range, modularities, \u0026#39;bo-\u0026#39;, markersize=6) plt.xlabel(\u0026#39;Number of Communities\u0026#39;) plt.ylabel(\u0026#39;Modularity\u0026#39;) plt.title(\u0026#39;Modularity vs Number of Communities\u0026#39;) plt.grid(True, alpha=0.3) # Network statistics plt.subplot(2, 2, 4) stats = { \u0026#39;Nodes\u0026#39;: G.number_of_nodes(), \u0026#39;Edges\u0026#39;: G.number_of_edges(), \u0026#39;Communities\u0026#39;: len(communities), \u0026#39;Modularity\u0026#39;: nx.community.modularity(G, communities) } y_pos = np.arange(len(stats)) plt.barh(y_pos, list(stats.values()), alpha=0.7) plt.yticks(y_pos, list(stats.keys())) plt.xlabel(\u0026#39;Value\u0026#39;) plt.title(\u0026#39;Network Statistics\u0026#39;) plt.grid(True, alpha=0.3) plt.tight_layout() plt.show() def compare_community_methods(G): \u0026#34;\u0026#34;\u0026#34;Compare different community detection methods\u0026#34;\u0026#34;\u0026#34; methods = { \u0026#39;Modularity\u0026#39;: detect_communities_modularity, \u0026#39;Spectral\u0026#39;: detect_communities_spectral, \u0026#39;Girvan-Newman\u0026#39;: detect_communities_girvan_newman } results = {} for method_name, method_func in methods.items(): try: if method_name == \u0026#39;Spectral\u0026#39;: # Try different numbers of communities best_result = None best_modularity = -1 for n_comm in range(2, min(10, G.number_of_nodes())): result = method_func(G, n_comm) if result[\u0026#39;modularity\u0026#39;] \u0026gt; best_modularity: best_modularity = result[\u0026#39;modularity\u0026#39;] best_result = result results[method_name] = best_result else: results[method_name] = method_func(G) except Exception as e: print(f\u0026#34;Error with {method_name}: {e}\u0026#34;) results[method_name] = None return results # Example: Community detection on a network G = nx.karate_club_graph() # Zachary\u0026#39;s karate club # Compare methods comparison_results = compare_community_methods(G) print(\u0026#34;Community Detection Comparison:\u0026#34;) for method, result in comparison_results.items(): if result is not None: print(f\u0026#34;\\n{method}:\u0026#34;) print(f\u0026#34; Number of communities: {result[\u0026#39;num_communities\u0026#39;]}\u0026#34;) print(f\u0026#34; Modularity: {result[\u0026#39;modularity\u0026#39;]:.3f}\u0026#34;) print(f\u0026#34; Average community size: {np.mean([len(c) for c in result[\u0026#39;communities\u0026#39;]]):.1f}\u0026#34;) # Plot best result best_method = max(comparison_results.keys(), key=lambda x: comparison_results[x][\u0026#39;modularity\u0026#39;] if comparison_results[x] else -1) best_result = comparison_results[best_method] if best_result is not None: plot_communities(G, best_result[\u0026#39;communities\u0026#39;], f\u0026#34;Best Method: {best_method}\u0026#34;) Key Takeaways Community structure: Identifies densely connected groups within networks Modularity: Measures quality of community partitions Detection methods: Various algorithms with different strengths Spectral clustering: Uses eigenvalues of Laplacian matrix Girvan-Newman: Hierarchical method based on edge betweenness Applications: Important for understanding network organization Evaluation: Multiple metrics for assessing community quality References Newman, M. E. J. (2010). Networks: An Introduction. Oxford University Press. Fortunato, S. (2010). Community detection in graphs. Physics Reports, 486(3-5), 75-174. Girvan, M., \u0026amp; Newman, M. E. J. (2002). Community structure in social and biological networks. Proceedings of the National Academy of Sciences, 99(12), 7821-7826. Blondel, V. D., et al. (2008). Fast unfolding of communities in large networks. Journal of Statistical Mechanics: Theory and Experiment, 2008(10), P10008. Community detection provides insights into the modular organization of networks, with important applications in understanding materials structure and defect clustering.\n","permalink":"https://Linlin-resh.github.io/posts/reading-notes-newman-ch11/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eChapter 11 of Newman\u0026rsquo;s \u003cem\u003eNetworks: An Introduction\u003c/em\u003e focuses on \u003cstrong\u003ecommunity structure\u003c/strong\u003e - the identification of densely connected groups of nodes within networks. This chapter covers various methods for detecting communities, their mathematical foundations, and applications to understanding network organization.\u003c/p\u003e\n\u003ch2 id=\"111-what-is-community-structure\"\u003e11.1 What is Community Structure?\u003c/h2\u003e\n\u003ch3 id=\"definition\"\u003eDefinition\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eCommunity structure\u003c/strong\u003e refers to the presence of groups of nodes that are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDensely connected\u003c/strong\u003e within groups\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSparsely connected\u003c/strong\u003e between groups\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFunctionally related\u003c/strong\u003e or similar in some way\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"mathematical-framework\"\u003eMathematical Framework\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eCommunity detection\u003c/strong\u003e aims to partition the network into communities such that:\u003c/p\u003e","title":"Reading Notes: Newman's Networks Chapter 11 - Community Structure"},{"content":"Introduction Chapter 12 of Newman\u0026rsquo;s Networks: An Introduction explores network inference - the process of reconstructing or inferring network structure from partial or noisy data. This chapter covers statistical methods, machine learning approaches, and practical techniques for network reconstruction.\n12.1 Statistical Inference Maximum Likelihood Estimation Likelihood Function Likelihood function: $$L(\\theta) = \\prod_{i,j} P(A_{ij}|\\theta)$$\nWhere:\n$A_{ij}$: Observed adjacency matrix $\\theta$: Model parameters $P(A_{ij}|\\theta)$: Probability of edge $(i,j)$ given parameters Log-Likelihood Log-likelihood: $$\\ell(\\theta) = \\sum_{i,j} \\log P(A_{ij}|\\theta)$$\nMLE solution: $$\\hat{\\theta} = \\arg\\max_{\\theta} \\ell(\\theta)$$\nGradient Descent Gradient: $$\\frac{\\partial \\ell}{\\partial \\theta} = \\sum_{i,j} \\frac{1}{P(A_{ij}|\\theta)} \\frac{\\partial P(A_{ij}|\\theta)}{\\partial \\theta}$$\nUpdate rule: $$\\theta^{(t+1)} = \\theta^{(t)} + \\alpha \\frac{\\partial \\ell}{\\partial \\theta}$$\nBayesian Inference Prior Distribution Prior distribution: $$P(\\theta) = \\text{prior knowledge about parameters}$$\nCommon priors:\nUniform: $P(\\theta) = \\text{constant}$ Gaussian: $P(\\theta) = \\mathcal{N}(\\mu, \\sigma^2)$ Beta: $P(\\theta) = \\text{Beta}(\\alpha, \\beta)$ Posterior Distribution Posterior distribution: $$P(\\theta|A) = \\frac{P(A|\\theta)P(\\theta)}{P(A)}$$\nWhere $P(A)$ is the marginal likelihood: $$P(A) = \\int P(A|\\theta)P(\\theta) , d\\theta$$\nMarkov Chain Monte Carlo Metropolis-Hastings algorithm:\nPropose new parameter $\\theta\u0026rsquo;$ from proposal distribution $q(\\theta\u0026rsquo;|\\theta)$ Accept with probability: $$P_{\\text{accept}} = \\min\\left(1, \\frac{P(\\theta\u0026rsquo;|A)q(\\theta|\\theta\u0026rsquo;)}{P(\\theta|A)q(\\theta\u0026rsquo;|\\theta)}\\right)$$ Repeat until convergence Model Selection Information Criteria Akaike Information Criterion (AIC): $$AIC = -2\\ell(\\hat{\\theta}) + 2k$$\nBayesian Information Criterion (BIC): $$BIC = -2\\ell(\\hat{\\theta}) + k \\log n$$\nWhere $k$ is the number of parameters.\nCross-Validation k-fold cross-validation:\nSplit data into $k$ folds Train on $k-1$ folds, test on remaining fold Repeat for all folds Average performance 12.2 Network Reconstruction Link Prediction Similarity Measures Common neighbors: $$S_{ij} = |\\mathcal{N}_i \\cap \\mathcal{N}_j|$$\nJaccard coefficient: $$S_{ij} = \\frac{|\\mathcal{N}_i \\cap \\mathcal{N}_j|}{|\\mathcal{N}_i \\cup \\mathcal{N}_j|}$$\nAdamic-Adar: $$S_{ij} = \\sum_{k \\in \\mathcal{N}_i \\cap \\mathcal{N}_j} \\frac{1}{\\log k_k}$$\nPreferential attachment: $$S_{ij} = k_i \\cdot k_j$$\nResource allocation: $$S_{ij} = \\sum_{k \\in \\mathcal{N}_i \\cap \\mathcal{N}_j} \\frac{1}{k_k}$$\nPrediction Accuracy Area Under Curve (AUC): $$AUC = \\frac{1}{n(n-1)} \\sum_{i \\neq j} \\mathbb{I}(S_{ij} \u0026gt; S_{\\text{random}})$$\nPrecision: $$P = \\frac{TP}{TP + FP}$$\nRecall: $$R = \\frac{TP}{TP + FN}$$\nF1-score: $$F1 = \\frac{2PR}{P + R}$$\nMatrix Completion Nuclear Norm Minimization Problem: $$\\min_{X} ||X - A||F^2 + \\lambda ||X||*$$\nWhere:\n$||X||_*$: Nuclear norm (sum of singular values) $\\lambda$: Regularization parameter Alternating Least Squares Algorithm:\nInitialize $U$ and $V$ randomly Fix $V$, solve for $U$: $U = \\arg\\min_U ||UV^T - A||_F^2$ Fix $U$, solve for $V$: $V = \\arg\\min_V ||UV^T - A||_F^2$ Repeat until convergence Solution: $X = UV^T$\nGraph Neural Networks Graph Convolutional Networks Layer update: $$H^{(l+1)} = \\sigma(\\tilde{A} H^{(l)} W^{(l)})$$\nWhere:\n$\\tilde{A} = D^{-1/2} A D^{-1/2}$: Normalized adjacency matrix $H^{(l)}$: Node features at layer $l$ $W^{(l)}$: Weight matrix at layer $l$ Link Prediction with GNNs Node embeddings: $$z_i = \\text{GNN}(A, X)_i$$\nLink prediction: $$P(A_{ij} = 1) = \\sigma(z_i^T z_j)$$\n12.3 Missing Data Imputation Missing Data Types Missing Completely at Random (MCAR) Definition: Missingness is independent of observed and unobserved data\nMathematical condition: $$P(M|X, Y) = P(M)$$\nWhere $M$ is the missingness indicator.\nMissing at Random (MAR) Definition: Missingness depends only on observed data\nMathematical condition: $$P(M|X, Y) = P(M|X)$$\nMissing Not at Random (MNAR) Definition: Missingness depends on unobserved data\nMathematical condition: $$P(M|X, Y) \\neq P(M|X)$$\nImputation Methods Mean Imputation Method: Replace missing values with mean of observed values\nFormula: $$\\hat{x}{ij} = \\frac{1}{n_j} \\sum{i: x_{ij} \\text{ observed}} x_{ij}$$\nAdvantages: Simple, fast Disadvantages: Reduces variance, may bias estimates\nRegression Imputation Method: Use regression to predict missing values\nFormula: $$\\hat{x}{ij} = \\beta_0 + \\sum{k \\neq j} \\beta_k x_{ik}$$\nAdvantages: Preserves relationships Disadvantages: Assumes linear relationships\nMultiple Imputation Method: Generate multiple imputed datasets\nAlgorithm:\nGenerate $m$ imputed datasets Analyze each dataset separately Combine results using Rubin\u0026rsquo;s rules Combined estimate: $$\\bar{\\theta} = \\frac{1}{m} \\sum_{i=1}^m \\hat{\\theta}_i$$\nVariance: $$\\text{Var}(\\bar{\\theta}) = \\frac{1}{m} \\sum_{i=1}^m \\text{Var}(\\hat{\\theta}i) + \\frac{1}{m-1} \\sum{i=1}^m (\\hat{\\theta}_i - \\bar{\\theta})^2$$\n12.4 Network Tomography Network Discovery Traceroute Method: Send packets with increasing TTL values\nInformation obtained:\nPath from source to destination Round-trip time Packet loss rate Limitations:\nMay not discover all paths Load balancing can cause inconsistencies Ping Method: Send ICMP echo requests\nInformation obtained:\nRound-trip time Packet loss rate Network reachability Limitations:\nOnly end-to-end information May be blocked by firewalls Topology Inference Graph Construction From traceroute data:\nCollect paths from multiple sources to multiple destinations Construct graph by connecting consecutive nodes in paths Merge duplicate edges From ping data:\nTest connectivity between all pairs of nodes Add edge if nodes can communicate May miss intermediate nodes Validation Consistency checks:\nTriangle inequality: $d_{ij} \\leq d_{ik} + d_{kj}$ Symmetry: $d_{ij} = d_{ji}$ Transitivity: If $A_{ik} = 1$ and $A_{kj} = 1$, then $A_{ij} = 1$ Statistical validation:\nCompare inferred topology with known topology Use cross-validation techniques 12.5 Applications to Materials Science Defect Network Inference Problem Given: Partial observations of defect interactions Goal: Infer complete defect network structure\nMathematical formulation: $$\\min_{A} ||A - A_{\\text{observed}}||_F^2 + \\lambda R(A)$$\nWhere $R(A)$ is a regularization term.\nMethods Sparse reconstruction: $$R(A) = ||A||_1$$\nLow-rank reconstruction: $$R(A) = ||A||_*$$\nCommunity structure: $$R(A) = \\sum_{c} ||A_c||_F^2$$\nNanowire Network Reconstruction Problem Given: Partial electrical measurements Goal: Infer complete nanowire network\nConstraints:\nElectrical: Ohm\u0026rsquo;s law must be satisfied Topological: Network must be connected Physical: Edge weights must be positive Optimization Objective function: $$\\min_{A, R} \\sum_{i,j} (V_i - V_j - R_{ij} I_{ij})^2 + \\lambda ||A||_1$$\nConstraints:\n$A_{ij} \\geq 0$ (positive resistance) $A_{ij} = 0$ if no nanowire between $i$ and $j$ $\\sum_j A_{ij} \u0026gt; 0$ (each node must have at least one connection) Phase Transition Inference Problem Given: Partial observations of phase transitions Goal: Infer complete phase diagram\nNetwork representation:\nNodes: Different phases Edges: Phase transitions Weights: Transition probabilities Methods Bayesian inference: $$P(\\text{phase}|T, P) = \\frac{P(T, P|\\text{phase})P(\\text{phase})}{P(T, P)}$$\nMachine learning: $$f: (T, P) \\rightarrow \\text{phase}$$\nCode Example: Network Inference import networkx as nx import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score, precision_recall_curve from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from scipy.optimize import minimize from scipy.sparse import csr_matrix from collections import defaultdict def generate_partial_network(G, missing_fraction=0.3): \u0026#34;\u0026#34;\u0026#34;Generate partial network by removing edges\u0026#34;\u0026#34;\u0026#34; edges = list(G.edges()) n_remove = int(len(edges) * missing_fraction) edges_to_remove = np.random.choice(len(edges), n_remove, replace=False) partial_G = G.copy() partial_G.remove_edges_from([edges[i] for i in edges_to_remove]) return partial_G, edges_to_remove def calculate_similarity_measures(G): \u0026#34;\u0026#34;\u0026#34;Calculate various similarity measures for link prediction\u0026#34;\u0026#34;\u0026#34; n = G.number_of_nodes() similarities = {} # Common neighbors cn = np.zeros((n, n)) for i in range(n): for j in range(i+1, n): cn[i, j] = len(set(G.neighbors(i)) \u0026amp; set(G.neighbors(j))) cn[j, i] = cn[i, j] similarities[\u0026#39;common_neighbors\u0026#39;] = cn # Jaccard coefficient jaccard = np.zeros((n, n)) for i in range(n): for j in range(i+1, n): neighbors_i = set(G.neighbors(i)) neighbors_j = set(G.neighbors(j)) union = neighbors_i | neighbors_j if len(union) \u0026gt; 0: jaccard[i, j] = len(neighbors_i \u0026amp; neighbors_j) / len(union) jaccard[j, i] = jaccard[i, j] similarities[\u0026#39;jaccard\u0026#39;] = jaccard # Adamic-Adar aa = np.zeros((n, n)) for i in range(n): for j in range(i+1, n): common_neighbors = set(G.neighbors(i)) \u0026amp; set(G.neighbors(j)) aa[i, j] = sum(1/np.log(G.degree(k)) for k in common_neighbors if G.degree(k) \u0026gt; 1) aa[j, i] = aa[i, j] similarities[\u0026#39;adamic_adar\u0026#39;] = aa # Preferential attachment pa = np.zeros((n, n)) degrees = [G.degree(i) for i in range(n)] for i in range(n): for j in range(i+1, n): pa[i, j] = degrees[i] * degrees[j] pa[j, i] = pa[i, j] similarities[\u0026#39;preferential_attachment\u0026#39;] = pa return similarities def predict_links(G, similarities, method=\u0026#39;common_neighbors\u0026#39;): \u0026#34;\u0026#34;\u0026#34;Predict links using similarity measures\u0026#34;\u0026#34;\u0026#34; n = G.number_of_nodes() A = nx.adjacency_matrix(G).toarray() # Get similarity scores scores = similarities[method] # Create training and test sets edges = [(i, j) for i in range(n) for j in range(i+1, n)] labels = [A[i, j] for i, j in edges] features = [scores[i, j] for i, j in edges] X_train, X_test, y_train, y_test = train_test_split( features, labels, test_size=0.3, random_state=42 ) # Train classifier clf = LogisticRegression(random_state=42) clf.fit(np.array(X_train).reshape(-1, 1), y_train) # Predict probabilities y_pred_proba = clf.predict_proba(np.array(X_test).reshape(-1, 1))[:, 1] y_pred = clf.predict(np.array(X_test).reshape(-1, 1)) # Calculate metrics auc = roc_auc_score(y_test, y_pred_proba) precision, recall, _ = precision_recall_curve(y_test, y_pred_proba) return { \u0026#39;auc\u0026#39;: auc, \u0026#39;precision\u0026#39;: precision, \u0026#39;recall\u0026#39;: recall, \u0026#39;y_test\u0026#39;: y_test, \u0026#39;y_pred_proba\u0026#39;: y_pred_proba } def matrix_completion(A_observed, rank=5, lambda_reg=0.1): \u0026#34;\u0026#34;\u0026#34;Perform matrix completion using nuclear norm minimization\u0026#34;\u0026#34;\u0026#34; def objective(X_flat): X = X_flat.reshape(A_observed.shape) # Nuclear norm (sum of singular values) U, s, Vt = np.linalg.svd(X, full_matrices=False) nuclear_norm = np.sum(s) # Frobenius norm of observed entries mask = ~np.isnan(A_observed) frobenius_norm = np.sum((X[mask] - A_observed[mask])**2) return frobenius_norm + lambda_reg * nuclear_norm # Initialize with random matrix X_init = np.random.randn(*A_observed.shape) X_init = X_init.flatten() # Optimize result = minimize(objective, X_init, method=\u0026#39;L-BFGS-B\u0026#39;) X_completed = result.x.reshape(A_observed.shape) return X_completed def impute_missing_data(A, method=\u0026#39;mean\u0026#39;): \u0026#34;\u0026#34;\u0026#34;Impute missing data in adjacency matrix\u0026#34;\u0026#34;\u0026#34; A_imputed = A.copy() if method == \u0026#39;mean\u0026#39;: # Mean imputation mean_val = np.nanmean(A) A_imputed[np.isnan(A)] = mean_val elif method == \u0026#39;regression\u0026#39;: # Regression imputation for j in range(A.shape[1]): missing_mask = np.isnan(A[:, j]) if np.any(missing_mask) and not np.all(missing_mask): # Use other columns to predict missing values X = A[:, ~missing_mask] y = A[~missing_mask, j] if X.shape[1] \u0026gt; 0: # Simple linear regression beta = np.linalg.lstsq(X, y, rcond=None)[0] A_imputed[missing_mask, j] = X[missing_mask] @ beta elif method == \u0026#39;matrix_completion\u0026#39;: # Matrix completion A_imputed = matrix_completion(A) return A_imputed def evaluate_reconstruction(G_original, G_reconstructed): \u0026#34;\u0026#34;\u0026#34;Evaluate network reconstruction quality\u0026#34;\u0026#34;\u0026#34; A_original = nx.adjacency_matrix(G_original).toarray() A_reconstructed = nx.adjacency_matrix(G_reconstructed).toarray() # Edge accuracy edge_accuracy = np.mean(A_original == A_reconstructed) # Precision and recall tp = np.sum((A_original == 1) \u0026amp; (A_reconstructed == 1)) fp = np.sum((A_original == 0) \u0026amp; (A_reconstructed == 1)) fn = np.sum((A_original == 1) \u0026amp; (A_reconstructed == 0)) precision = tp / (tp + fp) if (tp + fp) \u0026gt; 0 else 0 recall = tp / (tp + fn) if (tp + fn) \u0026gt; 0 else 0 f1 = 2 * precision * recall / (precision + recall) if (precision + recall) \u0026gt; 0 else 0 # Structural similarity degree_correlation = np.corrcoef( [G_original.degree(i) for i in G_original.nodes()], [G_reconstructed.degree(i) for i in G_reconstructed.nodes()] )[0, 1] return { \u0026#39;edge_accuracy\u0026#39;: edge_accuracy, \u0026#39;precision\u0026#39;: precision, \u0026#39;recall\u0026#39;: recall, \u0026#39;f1\u0026#39;: f1, \u0026#39;degree_correlation\u0026#39;: degree_correlation } def plot_inference_results(G_original, G_partial, G_reconstructed, link_prediction_results, title=\u0026#34;Network Inference\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Plot network inference results\u0026#34;\u0026#34;\u0026#34; fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12)) # Original network pos = nx.spring_layout(G_original, k=1, iterations=50) nx.draw(G_original, pos, ax=ax1, node_color=\u0026#39;lightblue\u0026#39;, edge_color=\u0026#39;gray\u0026#39;, alpha=0.6, node_size=50) ax1.set_title(\u0026#39;Original Network\u0026#39;) ax1.axis(\u0026#39;off\u0026#39;) # Partial network nx.draw(G_partial, pos, ax=ax2, node_color=\u0026#39;lightcoral\u0026#39;, edge_color=\u0026#39;gray\u0026#39;, alpha=0.6, node_size=50) ax2.set_title(\u0026#39;Partial Network (Missing Edges)\u0026#39;) ax2.axis(\u0026#39;off\u0026#39;) # Reconstructed network nx.draw(G_reconstructed, pos, ax=ax3, node_color=\u0026#39;lightgreen\u0026#39;, edge_color=\u0026#39;gray\u0026#39;, alpha=0.6, node_size=50) ax3.set_title(\u0026#39;Reconstructed Network\u0026#39;) ax3.axis(\u0026#39;off\u0026#39;) # Link prediction ROC curve if \u0026#39;y_test\u0026#39; in link_prediction_results and \u0026#39;y_pred_proba\u0026#39; in link_prediction_results: from sklearn.metrics import roc_curve fpr, tpr, _ = roc_curve(link_prediction_results[\u0026#39;y_test\u0026#39;], link_prediction_results[\u0026#39;y_pred_proba\u0026#39;]) ax4.plot(fpr, tpr, \u0026#39;b-\u0026#39;, linewidth=2, label=f\u0026#39;AUC = {link_prediction_results[\u0026#34;auc\u0026#34;]:.3f}\u0026#39;) ax4.plot([0, 1], [0, 1], \u0026#39;r--\u0026#39;, alpha=0.5) ax4.set_xlabel(\u0026#39;False Positive Rate\u0026#39;) ax4.set_ylabel(\u0026#39;True Positive Rate\u0026#39;) ax4.set_title(\u0026#39;Link Prediction ROC Curve\u0026#39;) ax4.legend() ax4.grid(True, alpha=0.3) plt.suptitle(title) plt.tight_layout() plt.show() # Example: Network inference G = nx.barabasi_albert_graph(50, 3) # Generate partial network G_partial, removed_edges = generate_partial_network(G, missing_fraction=0.3) # Calculate similarity measures similarities = calculate_similarity_measures(G_partial) # Predict links link_prediction_results = predict_links(G_partial, similarities, method=\u0026#39;common_neighbors\u0026#39;) # Reconstruct network A_partial = nx.adjacency_matrix(G_partial).toarray() A_reconstructed = impute_missing_data(A_partial, method=\u0026#39;matrix_completion\u0026#39;) # Create reconstructed network G_reconstructed = nx.from_numpy_array(A_reconstructed) G_reconstructed = nx.Graph(G_reconstructed) # Remove self-loops and parallel edges # Evaluate reconstruction evaluation = evaluate_reconstruction(G, G_reconstructed) print(\u0026#34;Network Inference Results:\u0026#34;) print(f\u0026#34;Edge accuracy: {evaluation[\u0026#39;edge_accuracy\u0026#39;]:.3f}\u0026#34;) print(f\u0026#34;Precision: {evaluation[\u0026#39;precision\u0026#39;]:.3f}\u0026#34;) print(f\u0026#34;Recall: {evaluation[\u0026#39;recall\u0026#39;]:.3f}\u0026#34;) print(f\u0026#34;F1-score: {evaluation[\u0026#39;f1\u0026#39;]:.3f}\u0026#34;) print(f\u0026#34;Degree correlation: {evaluation[\u0026#39;degree_correlation\u0026#39;]:.3f}\u0026#34;) print(f\u0026#34;Link prediction AUC: {link_prediction_results[\u0026#39;auc\u0026#39;]:.3f}\u0026#34;) # Plot results plot_inference_results(G, G_partial, G_reconstructed, link_prediction_results) Key Takeaways Statistical inference: Maximum likelihood and Bayesian methods for parameter estimation Link prediction: Various similarity measures for predicting missing edges Matrix completion: Nuclear norm minimization for network reconstruction Missing data: Different types and imputation methods Network tomography: Inferring topology from partial observations Applications: Important for materials science and defect networks Evaluation: Multiple metrics for assessing reconstruction quality References Newman, M. E. J. (2010). Networks: An Introduction. Oxford University Press. Liben-Nowell, D., \u0026amp; Kleinberg, J. (2007). The link-prediction problem for social networks. Journal of the American Society for Information Science and Technology, 58(7), 1019-1031. Candès, E. J., \u0026amp; Recht, B. (2009). Exact matrix completion via convex optimization. Foundations of Computational Mathematics, 9(6), 717-772. Kipf, T. N., \u0026amp; Welling, M. (2016). Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907. Network inference provides powerful tools for reconstructing network structure from partial data, with important applications in materials science and complex systems.\n","permalink":"https://Linlin-resh.github.io/posts/reading-notes-newman-ch12/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eChapter 12 of Newman\u0026rsquo;s \u003cem\u003eNetworks: An Introduction\u003c/em\u003e explores \u003cstrong\u003enetwork inference\u003c/strong\u003e - the process of reconstructing or inferring network structure from partial or noisy data. This chapter covers statistical methods, machine learning approaches, and practical techniques for network reconstruction.\u003c/p\u003e\n\u003ch2 id=\"121-statistical-inference\"\u003e12.1 Statistical Inference\u003c/h2\u003e\n\u003ch3 id=\"maximum-likelihood-estimation\"\u003eMaximum Likelihood Estimation\u003c/h3\u003e\n\u003ch4 id=\"likelihood-function\"\u003eLikelihood Function\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eLikelihood function\u003c/strong\u003e:\n$$L(\\theta) = \\prod_{i,j} P(A_{ij}|\\theta)$$\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$A_{ij}$: Observed adjacency matrix\u003c/li\u003e\n\u003cli\u003e$\\theta$: Model parameters\u003c/li\u003e\n\u003cli\u003e$P(A_{ij}|\\theta)$: Probability of edge $(i,j)$ given parameters\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"log-likelihood\"\u003eLog-Likelihood\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eLog-likelihood\u003c/strong\u003e:\n$$\\ell(\\theta) = \\sum_{i,j} \\log P(A_{ij}|\\theta)$$\u003c/p\u003e","title":"Reading Notes: Newman's Networks Chapter 12 - Network Inference"},{"content":"Introduction Chapter 13 of Newman\u0026rsquo;s Networks: An Introduction explores network dynamics - the temporal evolution of network structure and the dynamic processes that occur on networks. This chapter covers temporal networks, network evolution models, and the interplay between network structure and dynamics.\n13.1 Temporal Networks Definition Temporal network is a sequence of networks over time:\n$$G(t) = (V, E(t)) \\quad \\text{for } t \\in [0, T]$$\nWhere:\n$V$: Set of nodes (usually constant) $E(t)$: Set of edges at time $t$ (varies with time) $T$: Total time period Representation Time-Varying Adjacency Matrix Adjacency matrix: $$A_{ij}(t) = \\begin{cases} 1 \u0026amp; \\text{if edge } (i,j) \\text{ exists at time } t \\ 0 \u0026amp; \\text{otherwise} \\end{cases}$$\nProperties:\nSymmetric: $A_{ij}(t) = A_{ji}(t)$ for undirected networks Time-dependent: $A(t)$ changes over time Binary: $A_{ij}(t) \\in {0, 1}$ for unweighted networks Aggregated Network Aggregated adjacency matrix: $$A_{ij} = \\int_0^T A_{ij}(t) , dt$$\nDiscrete version: $$A_{ij} = \\sum_{t=1}^T A_{ij}(t)$$\nProperties:\nWeighted: $A_{ij} \\in [0, T]$ represents total connection time Static: Single network representing entire time period Temporal Measures Temporal Degree Temporal degree of node $i$: $$k_i(t) = \\sum_{j} A_{ij}(t)$$\nAverage temporal degree: $$\\langle k_i \\rangle = \\frac{1}{T} \\int_0^T k_i(t) , dt$$\nTemporal Clustering Temporal clustering coefficient: $$C_i(t) = \\frac{2e_i(t)}{k_i(t)(k_i(t)-1)}$$\nWhere $e_i(t)$ is the number of edges between neighbors of node $i$ at time $t$.\nAverage temporal clustering: $$\\langle C_i \\rangle = \\frac{1}{T} \\int_0^T C_i(t) , dt$$\nTemporal Path Length Temporal path from $i$ to $j$:\nSequence of edges $(i, v_1, t_1), (v_1, v_2, t_2), \\ldots, (v_k, j, t_k)$ Where $t_1 \u0026lt; t_2 \u0026lt; \\ldots \u0026lt; t_k$ (causality constraint) Temporal distance: $$d_{ij}^T = \\min{\\text{length of temporal path from } i \\text{ to } j}$$\n13.2 Dynamic Processes on Temporal Networks SIR Model on Temporal Networks Model Definition Temporal SIR model: $$\\frac{dS_i}{dt} = -\\beta S_i \\sum_{j} A_{ij}(t) I_j$$ $$\\frac{dI_i}{dt} = \\beta S_i \\sum_{j} A_{ij}(t) I_j - \\gamma I_i$$ $$\\frac{dR_i}{dt} = \\gamma I_i$$\nWhere:\n$A_{ij}(t)$: Temporal adjacency matrix $\\beta$: Infection rate $\\gamma$: Recovery rate Basic Reproduction Number Temporal basic reproduction number: $$R_0^T = \\frac{\\beta}{\\gamma} \\frac{\\langle k^2 \\rangle_T}{\\langle k \\rangle_T}$$\nWhere:\n$\\langle k \\rangle_T = \\frac{1}{T} \\int_0^T \\langle k(t) \\rangle , dt$ $\\langle k^2 \\rangle_T = \\frac{1}{T} \\int_0^T \\langle k^2(t) \\rangle , dt$ Critical Threshold Epidemic threshold: $$R_0^T \u0026gt; 1$$\nCritical infection rate: $$\\beta_c^T = \\frac{\\gamma \\langle k \\rangle_T}{\\langle k^2 \\rangle_T}$$\nSynchronization on Temporal Networks Kuramoto Model Temporal Kuramoto model: $$\\frac{d\\theta_i}{dt} = \\omega_i + \\frac{K}{N} \\sum_{j} A_{ij}(t) \\sin(\\theta_j - \\theta_i)$$\nWhere:\n$\\theta_i(t)$: Phase of oscillator $i$ at time $t$ $\\omega_i$: Natural frequency $K$: Coupling strength $A_{ij}(t)$: Temporal adjacency matrix Order Parameter Temporal order parameter: $$r(t) = \\left| \\frac{1}{N} \\sum_{j=1}^N e^{i\\theta_j(t)} \\right|$$\nAverage order parameter: $$\\langle r \\rangle = \\frac{1}{T} \\int_0^T r(t) , dt$$\nSynchronization Threshold Critical coupling: $$K_c^T = \\frac{2}{\\pi g(0)} \\frac{\\langle k^2 \\rangle_T}{\\langle k \\rangle_T}$$\nWhere $g(0)$ is the frequency distribution at zero.\n13.3 Network Evolution Models Growing Networks Preferential Attachment Temporal preferential attachment: $$\\Pi(k_i, t) = \\frac{k_i(t) + a}{\\sum_j (k_j(t) + a)}$$\nWhere:\n$k_i(t)$: Degree of node $i$ at time $t$ $a$: Initial attractiveness parameter Degree evolution: $$\\frac{dk_i}{dt} = m \\Pi(k_i, t)$$\nWhere $m$ is the number of edges added per time step.\nSolution Degree distribution: $$P(k) \\sim k^{-\\gamma}$$\nWhere $\\gamma = 3 + \\frac{a}{m}$.\nDegree evolution: $$k_i(t) = m \\left(\\frac{t}{t_i}\\right)^{\\beta}$$\nWhere $\\beta = \\frac{1}{2}$ for $a = 0$.\nAging and Preferential Attachment Model Definition Aging model: $$\\Pi(k_i, t, t_i) = \\frac{k_i(t) e^{-\\alpha(t-t_i)}}{\\sum_j k_j(t) e^{-\\alpha(t-t_j)}}$$\nWhere:\n$t_i$: Time when node $i$ was added $\\alpha$: Aging parameter Degree Distribution For $\\alpha \u0026gt; 0$: $$P(k) \\sim k^{-\\gamma} e^{-\\beta k}$$\nWhere:\n$\\gamma = 3 + \\frac{a}{m}$ $\\beta = \\frac{\\alpha}{m}$ For $\\alpha = 0$: Standard preferential attachment\nFitness Models Model Definition Fitness model: $$\\Pi(k_i, \\eta_i) = \\frac{\\eta_i k_i(t)}{\\sum_j \\eta_j k_j(t)}$$\nWhere $\\eta_i$ is the fitness of node $i$.\nDegree Distribution Fitness distribution: $$P(k) \\sim k^{-\\gamma} \\int \\eta^{\\gamma-1} \\rho(\\eta) , d\\eta$$\nWhere $\\rho(\\eta)$ is the fitness distribution.\nPower-law exponent: $$\\gamma = 1 + \\frac{1}{\\langle \\eta \\rangle}$$\n13.4 Network Rewiring Random Rewiring Model Definition Random rewiring:\nRemove edge with probability $p$ Add new edge randomly Repeat for all edges Rewiring probability: $$P(\\text{rewire}) = p$$\nDegree Distribution For small $p$: Slight deviation from original distribution For large $p$: Approaches random network distribution\nPreferential Rewiring Model Definition Preferential rewiring:\nRemove edge with probability $p$ Add edge preferentially (high-degree nodes) Repeat for all edges Rewiring probability: $$P(\\text{rewire to node } i) = \\frac{k_i}{\\sum_j k_j}$$\nDegree Distribution Maintains scale-free structure for appropriate $p$\nSocial Rewiring Model Definition Social rewiring:\nTriadic closure: Prefer to connect to friends of friends Homophily: Prefer to connect to similar nodes Geographic proximity: Prefer to connect to nearby nodes Rewiring probability: $$P(\\text{rewire to node } j) = \\frac{w_{ij}}{\\sum_k w_{ik}}$$\nWhere $w_{ij}$ is the weight of connection to node $j$.\n13.5 Applications to Materials Science Defect Network Evolution Model Defect network evolution:\nNodes: Defect sites Edges: Defect interactions Growth: New defects appear over time Rewiring: Defects can move and reconnect Mathematical model: $$\\frac{dk_i}{dt} = m \\Pi(k_i, t) - \\gamma k_i$$\nWhere:\n$m$: Rate of new defect formation $\\gamma$: Rate of defect annihilation $\\Pi(k_i, t)$: Preferential attachment probability Phase Transitions Defect percolation: $$P(\\text{percolation}) = 1 - \\sum_{k=0}^{\\infty} P(k) (1-p)^k$$\nCritical defect concentration: $$c_c = \\frac{1}{\\kappa - 1}$$\nWhere $\\kappa = \\frac{\\langle k^2 \\rangle}{\\langle k \\rangle}$.\nNanowire Network Growth Model Nanowire network growth:\nNodes: Nanowire junctions Edges: Nanowire segments Growth: New nanowires are added Rewiring: Nanowires can break and reconnect Mathematical model: $$\\frac{dk_i}{dt} = m \\Pi(k_i, t) - \\lambda k_i$$\nWhere:\n$m$: Rate of new nanowire formation $\\lambda$: Rate of nanowire breaking $\\Pi(k_i, t)$: Preferential attachment probability Electrical Properties Conductivity evolution: $$\\sigma(t) = \\sigma_0 \\left(\\frac{k(t)}{k_0}\\right)^{\\alpha}$$\nWhere:\n$\\sigma_0$: Initial conductivity $k_0$: Initial average degree $\\alpha$: Conductivity exponent Phase Transition Dynamics Model Phase transition dynamics:\nNodes: Atomic sites Edges: Chemical bonds Dynamics: Bonds form and break over time Mathematical model: $$\\frac{dA_{ij}}{dt} = \\beta A_{ij} (1 - A_{ij}) - \\gamma A_{ij}$$\nWhere:\n$\\beta$: Bond formation rate $\\gamma$: Bond breaking rate $A_{ij}$: Bond strength between sites $i$ and $j$ Order Parameter Order parameter evolution: $$\\frac{d\\phi}{dt} = -\\frac{\\partial F}{\\partial \\phi}$$\nWhere $F$ is the free energy: $$F = \\frac{1}{2} \\sum_{i,j} A_{ij} (1 - A_{ij}) + \\frac{1}{2} \\sum_{i,j} A_{ij} A_{ji}$$\nCode Example: Network Dynamics import networkx as nx import numpy as np import matplotlib.pyplot as plt from scipy.integrate import odeint from collections import defaultdict def generate_temporal_network(n, T, p=0.1): \u0026#34;\u0026#34;\u0026#34;Generate temporal network with random rewiring\u0026#34;\u0026#34;\u0026#34; # Start with random network G = nx.erdos_renyi_graph(n, 0.1) # Generate temporal sequence temporal_networks = [] current_G = G.copy() for t in range(T): # Random rewiring edges_to_remove = [] for edge in current_G.edges(): if np.random.random() \u0026lt; p: edges_to_remove.append(edge) # Remove edges current_G.remove_edges_from(edges_to_remove) # Add new edges for _ in range(len(edges_to_remove)): i, j = np.random.choice(n, 2, replace=False) current_G.add_edge(i, j) temporal_networks.append(current_G.copy()) return temporal_networks def simulate_temporal_sir(G_temporal, beta=0.1, gamma=0.05, initial_infected=1): \u0026#34;\u0026#34;\u0026#34;Simulate SIR model on temporal network\u0026#34;\u0026#34;\u0026#34; n = G_temporal[0].number_of_nodes() T = len(G_temporal) # Initial conditions S = np.ones(n) I = np.zeros(n) R = np.zeros(n) # Initial infected nodes infected_nodes = np.random.choice(n, initial_infected, replace=False) S[infected_nodes] = 0 I[infected_nodes] = 1 # Store results S_history = [S.copy()] I_history = [I.copy()] R_history = [R.copy()] # Simulate dynamics for t in range(T): G = G_temporal[t] # SIR dynamics dS_dt = np.zeros(n) dI_dt = np.zeros(n) dR_dt = np.zeros(n) for i in range(n): # Infection rate infection_rate = 0 for j in G.neighbors(i): infection_rate += I[j] infection_rate *= beta * S[i] # Recovery rate recovery_rate = gamma * I[i] dS_dt[i] = -infection_rate dI_dt[i] = infection_rate - recovery_rate dR_dt[i] = recovery_rate # Update states S += dS_dt I += dI_dt R += dR_dt # Ensure non-negative values S = np.maximum(S, 0) I = np.maximum(I, 0) R = np.maximum(R, 0) # Store results S_history.append(S.copy()) I_history.append(I.copy()) R_history.append(R.copy()) return S_history, I_history, R_history def simulate_temporal_kuramoto(G_temporal, K=1.0, omega=None): \u0026#34;\u0026#34;\u0026#34;Simulate Kuramoto model on temporal network\u0026#34;\u0026#34;\u0026#34; n = G_temporal[0].number_of_nodes() T = len(G_temporal) # Natural frequencies if omega is None: omega = np.random.normal(0, 1, n) # Initial phases theta = np.random.uniform(0, 2*np.pi, n) # Store results theta_history = [theta.copy()] r_history = [] # Simulate dynamics for t in range(T): G = G_temporal[t] # Kuramoto dynamics dtheta_dt = np.zeros(n) for i in range(n): coupling = 0 for j in G.neighbors(i): coupling += np.sin(theta[j] - theta[i]) coupling *= K / n dtheta_dt[i] = omega[i] + coupling # Update phases theta += dtheta_dt # Calculate order parameter r = np.abs(np.mean(np.exp(1j * theta))) r_history.append(r) # Store results theta_history.append(theta.copy()) return theta_history, r_history def simulate_network_growth(n, T, m=1, alpha=0.1): \u0026#34;\u0026#34;\u0026#34;Simulate network growth with preferential attachment and aging\u0026#34;\u0026#34;\u0026#34; # Start with small network G = nx.complete_graph(min(3, n)) node_times = list(range(min(3, n))) # Store degree evolution degree_history = [] for t in range(min(3, n), n): # Add new node G.add_node(t) node_times.append(t) # Add edges with preferential attachment and aging for _ in range(m): # Calculate attachment probabilities probs = [] for i in G.nodes(): if i != t: k_i = G.degree(i) age_factor = np.exp(-alpha * (t - node_times[i])) probs.append(k_i * age_factor) # Normalize probabilities probs = np.array(probs) probs = probs / np.sum(probs) # Choose node to connect to if len(probs) \u0026gt; 0: target = np.random.choice(list(G.nodes())[:-1], p=probs) G.add_edge(t, target) # Store degree distribution degrees = [G.degree(i) for i in G.nodes()] degree_history.append(degrees.copy()) return G, degree_history def analyze_temporal_network(G_temporal): \u0026#34;\u0026#34;\u0026#34;Analyze temporal network properties\u0026#34;\u0026#34;\u0026#34; T = len(G_temporal) n = G_temporal[0].number_of_nodes() # Calculate temporal measures temporal_degrees = [] temporal_clustering = [] temporal_path_lengths = [] for t in range(T): G = G_temporal[t] # Degree distribution degrees = [G.degree(i) for i in G.nodes()] temporal_degrees.append(degrees) # Clustering coefficient clustering = nx.average_clustering(G) temporal_clustering.append(clustering) # Path length if nx.is_connected(G): path_length = nx.average_shortest_path_length(G) else: # Analyze largest component largest_cc = max(nx.connected_components(G), key=len) subgraph = G.subgraph(largest_cc) path_length = nx.average_shortest_path_length(subgraph) temporal_path_lengths.append(path_length) # Calculate aggregated measures avg_degree = np.mean([np.mean(degrees) for degrees in temporal_degrees]) avg_clustering = np.mean(temporal_clustering) avg_path_length = np.mean(temporal_path_lengths) return { \u0026#39;temporal_degrees\u0026#39;: temporal_degrees, \u0026#39;temporal_clustering\u0026#39;: temporal_clustering, \u0026#39;temporal_path_lengths\u0026#39;: temporal_path_lengths, \u0026#39;avg_degree\u0026#39;: avg_degree, \u0026#39;avg_clustering\u0026#39;: avg_clustering, \u0026#39;avg_path_length\u0026#39;: avg_path_length } def plot_temporal_analysis(G_temporal, S_history, I_history, R_history, theta_history, r_history, degree_history): \u0026#34;\u0026#34;\u0026#34;Plot temporal network analysis results\u0026#34;\u0026#34;\u0026#34; fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12)) # SIR dynamics t_sir = range(len(S_history)) S_avg = [np.mean(S) for S in S_history] I_avg = [np.mean(I) for I in I_history] R_avg = [np.mean(R) for R in R_history] ax1.plot(t_sir, S_avg, \u0026#39;b-\u0026#39;, label=\u0026#39;Susceptible\u0026#39;, linewidth=2) ax1.plot(t_sir, I_avg, \u0026#39;r-\u0026#39;, label=\u0026#39;Infected\u0026#39;, linewidth=2) ax1.plot(t_sir, R_avg, \u0026#39;g-\u0026#39;, label=\u0026#39;Recovered\u0026#39;, linewidth=2) ax1.set_xlabel(\u0026#39;Time\u0026#39;) ax1.set_ylabel(\u0026#39;Fraction\u0026#39;) ax1.set_title(\u0026#39;Temporal SIR Dynamics\u0026#39;) ax1.legend() ax1.grid(True, alpha=0.3) # Synchronization dynamics t_sync = range(len(r_history)) ax2.plot(t_sync, r_history, \u0026#39;b-\u0026#39;, linewidth=2) ax2.set_xlabel(\u0026#39;Time\u0026#39;) ax2.set_ylabel(\u0026#39;Order Parameter r\u0026#39;) ax2.set_title(\u0026#39;Temporal Synchronization\u0026#39;) ax2.grid(True, alpha=0.3) # Degree evolution t_deg = range(len(degree_history)) avg_degrees = [np.mean(degrees) for degrees in degree_history] ax3.plot(t_deg, avg_degrees, \u0026#39;b-\u0026#39;, linewidth=2) ax3.set_xlabel(\u0026#39;Time\u0026#39;) ax3.set_ylabel(\u0026#39;Average Degree\u0026#39;) ax3.set_title(\u0026#39;Network Growth\u0026#39;) ax3.grid(True, alpha=0.3) # Network visualization at different times times_to_plot = [0, len(G_temporal)//2, len(G_temporal)-1] for i, t in enumerate(times_to_plot): G = G_temporal[t] pos = nx.spring_layout(G, k=1, iterations=50) # Plot in subplot ax = plt.subplot(2, 2, 4) nx.draw(G, pos, ax=ax, node_size=30, node_color=\u0026#39;lightblue\u0026#39;, edge_color=\u0026#39;gray\u0026#39;, alpha=0.6) ax.set_title(f\u0026#39;Network at t={t}\u0026#39;) ax.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() # Example: Temporal network analysis n, T = 50, 100 G_temporal = generate_temporal_network(n, T, p=0.1) # Simulate SIR dynamics S_history, I_history, R_history = simulate_temporal_sir(G_temporal, beta=0.1, gamma=0.05) # Simulate synchronization theta_history, r_history = simulate_temporal_kuramoto(G_temporal, K=1.0) # Simulate network growth G_grown, degree_history = simulate_network_growth(100, 50, m=2, alpha=0.1) # Analyze temporal network temporal_analysis = analyze_temporal_network(G_temporal) print(\u0026#34;Temporal Network Analysis:\u0026#34;) print(f\u0026#34;Average degree: {temporal_analysis[\u0026#39;avg_degree\u0026#39;]:.3f}\u0026#34;) print(f\u0026#34;Average clustering: {temporal_analysis[\u0026#39;avg_clustering\u0026#39;]:.3f}\u0026#34;) print(f\u0026#34;Average path length: {temporal_analysis[\u0026#39;avg_path_length\u0026#39;]:.3f}\u0026#34;) # Plot results plot_temporal_analysis(G_temporal, S_history, I_history, R_history, theta_history, r_history, degree_history) Key Takeaways Temporal networks: Networks that change over time Dynamic processes: SIR, synchronization, and other processes on temporal networks Network evolution: Growth, rewiring, and aging models Mathematical analysis: Differential equations and probability theory Applications: Important for materials science and complex systems Simulation: Computational methods for studying network dynamics Emergent properties: How network structure affects dynamic behavior References Newman, M. E. J. (2010). Networks: An Introduction. Oxford University Press. Holme, P., \u0026amp; Saramäki, J. (2012). Temporal networks. Physics Reports, 519(3), 97-125. Barabási, A. L., \u0026amp; Albert, R. (1999). Emergence of scaling in random networks. Science, 286(5439), 509-512. Kuramoto, Y. (1984). Chemical Oscillations, Waves, and Turbulence. Springer. Network dynamics provides insights into how networks evolve over time and how dynamic processes unfold on them, with important applications in understanding materials behavior and system evolution.\n","permalink":"https://Linlin-resh.github.io/posts/reading-notes-newman-ch13/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eChapter 13 of Newman\u0026rsquo;s \u003cem\u003eNetworks: An Introduction\u003c/em\u003e explores \u003cstrong\u003enetwork dynamics\u003c/strong\u003e - the temporal evolution of network structure and the dynamic processes that occur on networks. This chapter covers temporal networks, network evolution models, and the interplay between network structure and dynamics.\u003c/p\u003e\n\u003ch2 id=\"131-temporal-networks\"\u003e13.1 Temporal Networks\u003c/h2\u003e\n\u003ch3 id=\"definition\"\u003eDefinition\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eTemporal network\u003c/strong\u003e is a sequence of networks over time:\u003c/p\u003e\n\u003cp\u003e$$G(t) = (V, E(t)) \\quad \\text{for } t \\in [0, T]$$\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$V$: Set of nodes (usually constant)\u003c/li\u003e\n\u003cli\u003e$E(t)$: Set of edges at time $t$ (varies with time)\u003c/li\u003e\n\u003cli\u003e$T$: Total time period\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"representation\"\u003eRepresentation\u003c/h3\u003e\n\u003ch4 id=\"time-varying-adjacency-matrix\"\u003eTime-Varying Adjacency Matrix\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eAdjacency matrix\u003c/strong\u003e:\n$$A_{ij}(t) = \\begin{cases}\n1 \u0026amp; \\text{if edge } (i,j) \\text{ exists at time } t \\\n0 \u0026amp; \\text{otherwise}\n\\end{cases}$$\u003c/p\u003e","title":"Reading Notes: Newman's Networks Chapter 13 - Network Dynamics"},{"content":"Introduction Chapter 14 of Newman\u0026rsquo;s Networks: An Introduction explores network control - the theory and methods for controlling complex networks to achieve desired behaviors. This chapter covers controllability theory, control strategies, and applications to understanding and manipulating network dynamics.\n14.1 Controllability Theory Linear Controllability System Model Linear time-invariant system: $$\\frac{dx}{dt} = Ax + Bu$$\nWhere:\n$x \\in \\mathbb{R}^n$: State vector $u \\in \\mathbb{R}^m$: Control input $A \\in \\mathbb{R}^{n \\times n}$: System matrix $B \\in \\mathbb{R}^{n \\times m}$: Input matrix Controllability Matrix Controllability matrix: $$\\mathcal{C} = [B, AB, A^2B, \\ldots, A^{n-1}B]$$\nControllability condition: System is controllable if and only if $$\\text{rank}(\\mathcal{C}) = n$$\nKalman\u0026rsquo;s Controllability Test Theorem: The system is controllable if and only if the controllability matrix has full rank.\nProof: Based on Cayley-Hamilton theorem and linear algebra.\nStructural Controllability Definition Structural controllability: System is structurally controllable if it is controllable for almost all parameter values.\nAdvantage: Depends only on network topology, not specific parameter values.\nMinimum Number of Drivers Theorem: Minimum number of driver nodes needed for structural controllability is: $$N_D = \\max_i \\mu_i$$\nWhere $\\mu_i$ is the geometric multiplicity of eigenvalue $\\lambda_i$ of $A$.\nDriver Node Selection Algorithm:\nFind maximum matching in bipartite graph Unmatched nodes are driver nodes Minimum number of drivers = $n - |M|$ Where $|M|$ is the size of maximum matching.\nEnergy Control Control Energy Control energy: $$E = \\int_0^T ||u(t)||^2 dt$$\nMinimum energy control: $$u^*(t) = B^T e^{A^T(T-t)} W^{-1}(T) x_f$$\nWhere $W(T)$ is the controllability Gramian: $$W(T) = \\int_0^T e^{At} BB^T e^{A^T t} dt$$\nControllability Gramian Properties:\nSymmetric: $W(T) = W^T(T)$ Positive definite: If system is controllable Energy bound: $E \\geq x_f^T W^{-1}(T) x_f$ Optimal Control Linear Quadratic Regulator (LQR) Cost function: $$J = \\int_0^T (x^T Q x + u^T R u) dt$$\nWhere:\n$Q \\geq 0$: State weight matrix $R \u0026gt; 0$: Control weight matrix Optimal control: $$u^*(t) = -R^{-1} B^T P(t) x(t)$$\nWhere $P(t)$ satisfies the Riccati equation: $$-\\frac{dP}{dt} = A^T P + PA - PBR^{-1}B^T P + Q$$\nSteady-State Solution Algebraic Riccati equation: $$A^T P + PA - PBR^{-1}B^T P + Q = 0$$\nOptimal control: $$u^*(t) = -R^{-1} B^T P x(t)$$\n14.2 Network Controllability Controllability of Complex Networks Driver Node Analysis Driver nodes: Nodes that need external control input\nControllable nodes: Nodes that can be controlled by driver nodes\nMathematical formulation: $$\\min_{B} ||B||_0 \\quad \\text{subject to } \\text{rank}(\\mathcal{C}) = n$$\nWhere $||B||_0$ is the number of non-zero columns in $B$.\nControllability Gramian Network controllability Gramian: $$W = \\int_0^{\\infty} e^{At} BB^T e^{A^T t} dt$$\nProperties:\nTrace: $\\text{tr}(W)$ measures controllability Determinant: $\\det(W)$ measures controllability Eigenvalues: $\\lambda_i(W)$ measure controllability in different directions Control Centrality Definition Control centrality of node $i$: $$C_i = \\text{tr}(W_i)$$\nWhere $W_i$ is the controllability Gramian when node $i$ is the only driver.\nProperties Range: $C_i \\geq 0$\nInterpretation: Higher $C_i$ means node $i$ is more important for control\nCalculation: $C_i = \\sum_{j=1}^n \\frac{1}{\\lambda_j^2}$ where $\\lambda_j$ are eigenvalues of $A$.\nControllability and Network Structure Degree and Controllability High-degree nodes: Often important for control\nLow-degree nodes: May be easier to control\nMathematical relationship: $$C_i \\sim k_i^{\\alpha}$$\nWhere $\\alpha$ depends on network structure.\nClustering and Controllability High clustering: May reduce controllability\nLow clustering: May increase controllability\nMathematical relationship: $$C_i \\sim C_i^{-\\beta}$$\nWhere $\\beta$ depends on network structure.\n14.3 Control Strategies Centralized Control Single Controller Control law: $$u(t) = -K x(t)$$\nWhere $K$ is the feedback gain matrix.\nDesign: $K = R^{-1} B^T P$\nAdvantages: Optimal performance Disadvantages: Requires global information\nMultiple Controllers Control law: $$u_i(t) = -K_i x_i(t) + \\sum_{j \\in \\mathcal{N}i} K{ij} x_j(t)$$\nWhere:\n$K_i$: Local feedback gain $K_{ij}$: Coupling gain $\\mathcal{N}_i$: Neighbors of node $i$ Decentralized Control Local Control Control law: $$u_i(t) = -K_i x_i(t)$$\nDesign: Each controller uses only local information\nAdvantages: Scalable, robust Disadvantages: May not achieve global optimum\nDistributed Control Control law: $$u_i(t) = -K_i x_i(t) + \\sum_{j \\in \\mathcal{N}i} K{ij} (x_j(t) - x_i(t))$$\nDesign: Controllers communicate with neighbors\nAdvantages: Better performance than local control Disadvantages: Requires communication\nAdaptive Control Model Reference Adaptive Control Reference model: $$\\frac{dx_m}{dt} = A_m x_m + B_m r$$\nControl law: $$u(t) = K_x(t) x(t) + K_r(t) r(t)$$\nAdaptation law: $$\\frac{dK_x}{dt} = -\\Gamma_x x(t) e^T(t) P B$$ $$\\frac{dK_r}{dt} = -\\Gamma_r r(t) e^T(t) P B$$\nWhere $e(t) = x(t) - x_m(t)$ is the tracking error.\nSelf-Tuning Control Parameter estimation: $$\\hat{\\theta}(t) = \\hat{\\theta}(t-1) + \\Gamma \\phi(t) e(t)$$\nControl law: $$u(t) = \\hat{\\theta}^T(t) \\phi(t)$$\nWhere $\\phi(t)$ is the regressor vector.\n14.4 Applications to Materials Science Defect Control Problem Given: Defect network with dynamics $$\\frac{dx_i}{dt} = f_i(x_i) + \\sum_{j} A_{ij} g_{ij}(x_i, x_j) + u_i$$\nGoal: Control defect density to desired level\nControl objective: $$\\min_{u} \\int_0^T ||x(t) - x_d||^2 dt$$\nSolution Optimal control: $$u_i^*(t) = -R^{-1} B^T P (x_i(t) - x_{d,i})$$\nFeedback gain: $$K = R^{-1} B^T P$$\nStability: Guaranteed if $A$ is Hurwitz\nPhase Control Problem Given: Phase transition dynamics $$\\frac{d\\phi}{dt} = f(\\phi) + u$$\nGoal: Control phase to desired value\nControl objective: $$\\min_{u} \\int_0^T (\\phi(t) - \\phi_d)^2 dt$$\nSolution Optimal control: $$u^*(t) = -R^{-1} B^T P (\\phi(t) - \\phi_d)$$\nPhase transition: Controlled by external field\nNanowire Network Control Problem Given: Nanowire network with electrical dynamics $$\\frac{dV_i}{dt} = \\frac{1}{C_i} \\sum_{j} \\frac{V_j - V_i}{R_{ij}} + u_i$$\nGoal: Control voltage distribution\nControl objective: $$\\min_{u} \\int_0^T ||V(t) - V_d||^2 dt$$\nSolution Optimal control: $$u_i^*(t) = -R^{-1} C_i (V_i(t) - V_{d,i})$$\nElectrical properties: Controlled by external current\n14.5 Robust Control Robust Controllability Definition Robust controllability: System remains controllable under parameter uncertainty\nMathematical condition: $$\\text{rank}(\\mathcal{C}(\\theta)) = n \\quad \\forall \\theta \\in \\Theta$$\nWhere $\\Theta$ is the uncertainty set.\nRobust Control Design Control law: $$u(t) = -K(\\theta) x(t)$$\nDesign: $K(\\theta)$ minimizes worst-case performance\nPerformance: $$J = \\max_{\\theta \\in \\Theta} \\int_0^T (x^T Q x + u^T R u) dt$$\nH∞ Control Problem Given: System with disturbances $$\\frac{dx}{dt} = Ax + Bu + B_w w$$ $$z = Cx + Du$$\nGoal: Minimize effect of disturbances\nPerformance: $$||T_{zw}||_{\\infty} \u0026lt; \\gamma$$\nWhere $T_{zw}$ is the transfer function from $w$ to $z$.\nSolution Control law: $$u(t) = -K x(t)$$\nDesign: $K$ minimizes $||T_{zw}||_{\\infty}$\nRiccati equation: $$A^T P + PA - P(BR^{-1}B^T - \\gamma^{-2}B_w B_w^T) P + C^T C = 0$$\nCode Example: Network Control import networkx as nx import numpy as np import matplotlib.pyplot as plt from scipy.linalg import solve_continuous_are, eig from scipy.optimize import minimize from sklearn.linear_model import LinearRegression def check_controllability(A, B): \u0026#34;\u0026#34;\u0026#34;Check if system is controllable\u0026#34;\u0026#34;\u0026#34; n = A.shape[0] # Build controllability matrix C = B A_power = np.eye(n) for i in range(1, n): A_power = A_power @ A C = np.hstack([C, A_power @ B]) # Check rank rank = np.linalg.matrix_rank(C) is_controllable = rank == n return is_controllable, rank, C def find_driver_nodes(A): \u0026#34;\u0026#34;\u0026#34;Find minimum number of driver nodes for structural controllability\u0026#34;\u0026#34;\u0026#34; n = A.shape[0] # Create bipartite graph G_bipartite = nx.Graph() # Add nodes for i in range(n): G_bipartite.add_node(f\u0026#39;x_{i}\u0026#39;, bipartite=0) G_bipartite.add_node(f\u0026#39;y_{i}\u0026#39;, bipartite=1) # Add edges for i in range(n): for j in range(n): if A[i, j] != 0: G_bipartite.add_edge(f\u0026#39;x_{i}\u0026#39;, f\u0026#39;y_{j}\u0026#39;) # Find maximum matching matching = nx.bipartite.maximum_matching(G_bipartite) # Find unmatched nodes matched_x = set() for x, y in matching.items(): if x.startswith(\u0026#39;x_\u0026#39;): matched_x.add(x) # Driver nodes are unmatched x nodes driver_nodes = [] for i in range(n): if f\u0026#39;x_{i}\u0026#39; not in matched_x: driver_nodes.append(i) return driver_nodes, len(driver_nodes) def design_lqr_controller(A, B, Q, R): \u0026#34;\u0026#34;\u0026#34;Design LQR controller\u0026#34;\u0026#34;\u0026#34; # Solve algebraic Riccati equation P = solve_continuous_are(A, B, Q, R) # Calculate feedback gain K = np.linalg.inv(R) @ B.T @ P # Calculate closed-loop matrix A_cl = A - B @ K # Check stability eigenvals = eig(A_cl)[0] is_stable = np.all(np.real(eigenvals) \u0026lt; 0) return K, P, A_cl, is_stable def simulate_controlled_system(A, B, K, x0, T, dt=0.01): \u0026#34;\u0026#34;\u0026#34;Simulate controlled system\u0026#34;\u0026#34;\u0026#34; n = A.shape[0] steps = int(T / dt) # Initialize x = x0.copy() x_history = [x.copy()] u_history = [] # Simulate for t in range(steps): # Calculate control input u = -K @ x u_history.append(u.copy()) # Update state x = x + dt * (A @ x + B @ u) x_history.append(x.copy()) return np.array(x_history), np.array(u_history) def calculate_control_energy(u_history, dt): \u0026#34;\u0026#34;\u0026#34;Calculate control energy\u0026#34;\u0026#34;\u0026#34; energy = np.sum(np.sum(u_history**2, axis=1)) * dt return energy def design_robust_controller(A, B, Q, R, uncertainty_set): \u0026#34;\u0026#34;\u0026#34;Design robust controller for uncertain system\u0026#34;\u0026#34;\u0026#34; def objective(K_flat): K = K_flat.reshape(B.shape[1], A.shape[0]) # Check stability for all parameter values max_eigenval = -np.inf for A_uncertain in uncertainty_set: A_cl = A_uncertain - B @ K eigenvals = eig(A_cl)[0] max_eigenval = max(max_eigenval, np.max(np.real(eigenvals))) # Return negative of maximum real part (we want it to be negative) return -max_eigenval # Initial guess K0 = np.random.randn(B.shape[1], A.shape[0]) # Optimize result = minimize(objective, K0.flatten(), method=\u0026#39;BFGS\u0026#39;) K_robust = result.x.reshape(B.shape[1], A.shape[0]) return K_robust def analyze_control_centrality(A, B): \u0026#34;\u0026#34;\u0026#34;Analyze control centrality of nodes\u0026#34;\u0026#34;\u0026#34; n = A.shape[0] control_centrality = np.zeros(n) for i in range(n): # Create B matrix with only node i as driver B_i = np.zeros((n, 1)) B_i[i, 0] = 1 # Check controllability is_controllable, rank, C = check_controllability(A, B_i) if is_controllable: # Calculate controllability Gramian try: P = solve_continuous_are(A, B_i, np.eye(n), np.eye(1)) control_centrality[i] = np.trace(P) except: control_centrality[i] = 0 else: control_centrality[i] = 0 return control_centrality def plot_control_analysis(A, B, K, x_history, u_history, control_centrality): \u0026#34;\u0026#34;\u0026#34;Plot control analysis results\u0026#34;\u0026#34;\u0026#34; fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12)) # State evolution t = np.linspace(0, len(x_history)-1, len(x_history)) for i in range(min(5, x_history.shape[1])): ax1.plot(t, x_history[:, i], label=f\u0026#39;State {i}\u0026#39;, linewidth=2) ax1.set_xlabel(\u0026#39;Time\u0026#39;) ax1.set_ylabel(\u0026#39;State Value\u0026#39;) ax1.set_title(\u0026#39;State Evolution\u0026#39;) ax1.legend() ax1.grid(True, alpha=0.3) # Control input t_u = np.linspace(0, len(u_history)-1, len(u_history)) for i in range(min(5, u_history.shape[1])): ax2.plot(t_u, u_history[:, i], label=f\u0026#39;Control {i}\u0026#39;, linewidth=2) ax2.set_xlabel(\u0026#39;Time\u0026#39;) ax2.set_ylabel(\u0026#39;Control Input\u0026#39;) ax2.set_title(\u0026#39;Control Input Evolution\u0026#39;) ax2.legend() ax2.grid(True, alpha=0.3) # Control centrality nodes = range(len(control_centrality)) bars = ax3.bar(nodes, control_centrality, alpha=0.7, color=\u0026#39;skyblue\u0026#39;) ax3.set_xlabel(\u0026#39;Node Index\u0026#39;) ax3.set_ylabel(\u0026#39;Control Centrality\u0026#39;) ax3.set_title(\u0026#39;Control Centrality of Nodes\u0026#39;) ax3.grid(True, alpha=0.3) # Network visualization with control centrality G = nx.from_numpy_array(A) pos = nx.spring_layout(G, k=1, iterations=50) # Color nodes by control centrality node_colors = control_centrality node_sizes = 100 + 50 * control_centrality / np.max(control_centrality) nx.draw(G, pos, ax=ax4, node_color=node_colors, node_size=node_sizes, edge_color=\u0026#39;gray\u0026#39;, alpha=0.6, cmap=\u0026#39;viridis\u0026#39;) ax4.set_title(\u0026#39;Network with Control Centrality\u0026#39;) ax4.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() # Example: Network control analysis n = 20 G = nx.barabasi_albert_graph(n, 3) A = nx.adjacency_matrix(G).toarray() # Add self-loops for stability A = A - np.diag(np.sum(A, axis=1)) # Create control input matrix (all nodes can be controlled) B = np.eye(n) # Check controllability is_controllable, rank, C = check_controllability(A, B) print(f\u0026#34;System is controllable: {is_controllable}\u0026#34;) print(f\u0026#34;Controllability rank: {rank}/{n}\u0026#34;) # Find driver nodes driver_nodes, n_drivers = find_driver_nodes(A) print(f\u0026#34;Driver nodes: {driver_nodes}\u0026#34;) print(f\u0026#34;Number of drivers: {n_drivers}\u0026#34;) # Design LQR controller Q = np.eye(n) R = 0.1 * np.eye(n) K, P, A_cl, is_stable = design_lqr_controller(A, B, Q, R) print(f\u0026#34;Controller is stable: {is_stable}\u0026#34;) # Simulate controlled system x0 = np.random.randn(n) T = 10 x_history, u_history = simulate_controlled_system(A, B, K, x0, T) # Calculate control energy energy = calculate_control_energy(u_history, 0.01) print(f\u0026#34;Control energy: {energy:.3f}\u0026#34;) # Analyze control centrality control_centrality = analyze_control_centrality(A, B) print(f\u0026#34;Control centrality range: {np.min(control_centrality):.3f} - {np.max(control_centrality):.3f}\u0026#34;) # Plot results plot_control_analysis(A, B, K, x_history, u_history, control_centrality) Key Takeaways Controllability theory: Mathematical foundation for network control Driver nodes: Minimum set of nodes needed for control Control strategies: Centralized, decentralized, and adaptive control Energy optimization: LQR and other optimal control methods Robust control: Control under uncertainty Applications: Important for materials science and complex systems Control centrality: Measures importance of nodes for control References Newman, M. E. J. (2010). Networks: An Introduction. Oxford University Press. Liu, Y. Y., Slotine, J. J., \u0026amp; Barabási, A. L. (2011). Controllability of complex networks. Nature, 473(7346), 167-173. Kalman, R. E. (1960). On the general theory of control systems. IRE Transactions on Automatic Control, 5(4), 110-110. Lewis, F. L., Vrabie, D., \u0026amp; Syrmos, V. L. (2012). Optimal Control. John Wiley \u0026amp; Sons. Network control provides powerful tools for understanding and manipulating complex systems, with important applications in materials science and engineering.\n","permalink":"https://Linlin-resh.github.io/posts/reading-notes-newman-ch14/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eChapter 14 of Newman\u0026rsquo;s \u003cem\u003eNetworks: An Introduction\u003c/em\u003e explores \u003cstrong\u003enetwork control\u003c/strong\u003e - the theory and methods for controlling complex networks to achieve desired behaviors. This chapter covers controllability theory, control strategies, and applications to understanding and manipulating network dynamics.\u003c/p\u003e\n\u003ch2 id=\"141-controllability-theory\"\u003e14.1 Controllability Theory\u003c/h2\u003e\n\u003ch3 id=\"linear-controllability\"\u003eLinear Controllability\u003c/h3\u003e\n\u003ch4 id=\"system-model\"\u003eSystem Model\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eLinear time-invariant system\u003c/strong\u003e:\n$$\\frac{dx}{dt} = Ax + Bu$$\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$x \\in \\mathbb{R}^n$: State vector\u003c/li\u003e\n\u003cli\u003e$u \\in \\mathbb{R}^m$: Control input\u003c/li\u003e\n\u003cli\u003e$A \\in \\mathbb{R}^{n \\times n}$: System matrix\u003c/li\u003e\n\u003cli\u003e$B \\in \\mathbb{R}^{n \\times m}$: Input matrix\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"controllability-matrix\"\u003eControllability Matrix\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eControllability matrix\u003c/strong\u003e:\n$$\\mathcal{C} = [B, AB, A^2B, \\ldots, A^{n-1}B]$$\u003c/p\u003e","title":"Reading Notes: Newman's Networks Chapter 14 - Network Control"},{"content":"Introduction Chapter 15 of Newman\u0026rsquo;s Networks: An Introduction explores multilayer networks - networks that consist of multiple layers, each representing different types of relationships or interactions. This chapter covers the mathematical framework, analysis methods, and applications of multilayer networks.\n15.1 Multilayer Network Structure Definition Multilayer network is a collection of networks with:\nNodes: Can exist in multiple layers Intra-layer edges: Connections within each layer Inter-layer edges: Connections between layers Layers: Different types of relationships or time periods Mathematical Representation Supra-adjacency Matrix Supra-adjacency matrix: $$A = \\bigoplus_{\\alpha} A^{\\alpha} + \\bigoplus_{\\alpha \\neq \\beta} C^{\\alpha \\beta}$$\nWhere:\n$A^{\\alpha}$: Adjacency matrix of layer $\\alpha$ $C^{\\alpha \\beta}$: Inter-layer coupling matrix $\\bigoplus$: Direct sum operator Tensor Representation Multilayer adjacency tensor: $$A_{ij}^{\\alpha \\beta} = \\begin{cases} 1 \u0026amp; \\text{if } (i, \\alpha) \\text{ connected to } (j, \\beta) \\ 0 \u0026amp; \\text{otherwise} \\end{cases}$$\nProperties:\nDimensions: $n \\times n \\times L \\times L$ Symmetric: $A_{ij}^{\\alpha \\beta} = A_{ji}^{\\beta \\alpha}$ for undirected networks Sparse: Most entries are zero Types of Multilayer Networks Multiplex Networks Definition: Each layer represents a different type of relationship\nExample: Social network with layers for friendship, work, family\nMathematical representation: $$A_{ij}^{\\alpha \\beta} = \\begin{cases} A_{ij}^{\\alpha} \u0026amp; \\text{if } \\alpha = \\beta \\ 0 \u0026amp; \\text{if } \\alpha \\neq \\beta \\end{cases}$$\nTemporal Networks Definition: Each layer represents a different time period\nExample: Network evolution over time\nMathematical representation: $$A_{ij}^{\\alpha \\beta} = \\begin{cases} A_{ij}^{\\alpha} \u0026amp; \\text{if } \\alpha = \\beta \\ C_{ij}^{\\alpha \\beta} \u0026amp; \\text{if } |\\alpha - \\beta| = 1 \\ 0 \u0026amp; \\text{otherwise} \\end{cases}$$\nMultidimensional Networks Definition: Each layer represents a different dimension\nExample: Network with different interaction types\nMathematical representation: $$A_{ij}^{\\alpha \\beta} = \\begin{cases} A_{ij}^{\\alpha} \u0026amp; \\text{if } \\alpha = \\beta \\ C_{ij}^{\\alpha \\beta} \u0026amp; \\text{if } \\alpha \\neq \\beta \\end{cases}$$\n15.2 Multilayer Network Measures Degree Measures Multilayer Degree Multilayer degree of node $i$: $$k_i = \\sum_{\\alpha} k_i^{\\alpha}$$\nWhere $k_i^{\\alpha}$ is the degree of node $i$ in layer $\\alpha$.\nOverlapping Degree Overlapping degree of node $i$: $$o_i = \\sum_{\\alpha} \\mathbb{I}(k_i^{\\alpha} \u0026gt; 0)$$\nWhere $\\mathbb{I}(\\cdot)$ is the indicator function.\nParticipation Coefficient Participation coefficient: $$P_i = 1 - \\sum_{\\alpha} \\left(\\frac{k_i^{\\alpha}}{k_i}\\right)^2$$\nProperties:\n$0 \\leq P_i \\leq 1$ $P_i = 0$: Node active in only one layer $P_i = 1$: Node equally active in all layers Clustering Measures Multilayer Clustering Multilayer clustering coefficient: $$C_i = \\frac{\\sum_{\\alpha} C_i^{\\alpha}}{L}$$\nWhere $C_i^{\\alpha}$ is the clustering coefficient of node $i$ in layer $\\alpha$.\nCross-layer Clustering Cross-layer clustering: $$C_i^{\\alpha \\beta} = \\frac{2e_i^{\\alpha \\beta}}{k_i^{\\alpha} k_i^{\\beta}}$$\nWhere $e_i^{\\alpha \\beta}$ is the number of edges between neighbors of node $i$ in layers $\\alpha$ and $\\beta$.\nCentrality Measures Multilayer PageRank Multilayer PageRank: $$PR_i^{\\alpha} = (1-d) \\frac{1}{nL} + d \\sum_{j, \\beta} \\frac{A_{ij}^{\\alpha \\beta} PR_j^{\\beta}}{k_j^{\\beta}}$$\nWhere:\n$d$: Damping factor $n$: Number of nodes $L$: Number of layers Multilayer Betweenness Multilayer betweenness centrality: $$C_B^{\\alpha}(i) = \\sum_{s \\neq i \\neq t} \\frac{\\sigma_{st}^{\\alpha}(i)}{\\sigma_{st}^{\\alpha}}$$\nWhere $\\sigma_{st}^{\\alpha}(i)$ is the number of shortest paths from $s$ to $t$ in layer $\\alpha$ passing through $i$.\n15.3 Multilayer Network Analysis Community Detection Multilayer Modularity Multilayer modularity: $$Q = \\frac{1}{2\\mu} \\sum_{ij \\alpha \\beta} \\left[ A_{ij}^{\\alpha \\beta} - \\frac{k_i^{\\alpha} k_j^{\\beta}}{2m^{\\alpha}} \\right] \\delta(c_i^{\\alpha}, c_j^{\\beta})$$\nWhere:\n$\\mu = \\sum_{i \\alpha} k_i^{\\alpha}$: Total number of edges $m^{\\alpha} = \\sum_i k_i^{\\alpha}$: Number of edges in layer $\\alpha$ $c_i^{\\alpha}$: Community assignment of node $i$ in layer $\\alpha$ Multilayer Spectral Clustering Multilayer Laplacian: $$L^{\\alpha \\beta} = D^{\\alpha \\beta} - A^{\\alpha \\beta}$$\nWhere $D^{\\alpha \\beta}$ is the degree matrix.\nEigenvalue decomposition: $$L = U \\Lambda U^T$$\nClustering: Use first $k$ eigenvectors to cluster nodes\nRandom Walks Multilayer Random Walk Transition probability: $$P_{ij}^{\\alpha \\beta} = \\frac{A_{ij}^{\\alpha \\beta}}{k_i^{\\alpha}}$$\nStationary distribution: $$\\pi_i^{\\alpha} = \\frac{k_i^{\\alpha}}{\\sum_{j \\beta} k_j^{\\beta}}$$\nMultilayer PageRank PageRank equation: $$\\pi_i^{\\alpha} = (1-d) \\frac{1}{nL} + d \\sum_{j \\beta} P_{ji}^{\\beta \\alpha} \\pi_j^{\\beta}$$\nSolution: Iterative method or eigenvalue problem\nSynchronization Multilayer Kuramoto Model Phase dynamics: $$\\frac{d\\theta_i^{\\alpha}}{dt} = \\omega_i^{\\alpha} + \\frac{K}{nL} \\sum_{j \\beta} A_{ij}^{\\alpha \\beta} \\sin(\\theta_j^{\\beta} - \\theta_i^{\\alpha})$$\nWhere:\n$\\theta_i^{\\alpha}$: Phase of node $i$ in layer $\\alpha$ $\\omega_i^{\\alpha}$: Natural frequency $K$: Coupling strength Order Parameter Multilayer order parameter: $$r = \\left| \\frac{1}{nL} \\sum_{i \\alpha} e^{i\\theta_i^{\\alpha}} \\right|$$\nLayer-specific order parameter: $$r^{\\alpha} = \\left| \\frac{1}{n} \\sum_{i} e^{i\\theta_i^{\\alpha}} \\right|$$\n15.4 Applications to Materials Science Multilayer Materials Composite Materials Network representation:\nLayer 1: Matrix material Layer 2: Filler material Layer 3: Interface region Inter-layer connections: Phase boundaries\nMathematical model: $$A_{ij}^{\\alpha \\beta} = \\begin{cases} A_{ij}^{\\alpha} \u0026amp; \\text{if } \\alpha = \\beta \\ C_{ij}^{\\alpha \\beta} \u0026amp; \\text{if } \\alpha \\neq \\beta \\end{cases}$$\nNanocomposite Materials Network structure:\nLayer 1: Carbon nanotubes Layer 2: Polymer matrix Layer 3: Interface interactions Properties:\nElectrical conductivity: Depends on inter-layer connections Mechanical strength: Depends on intra-layer connections Thermal conductivity: Depends on both types of connections Phase Transitions Multilayer Phase Transitions Network representation:\nLayer 1: Crystalline phase Layer 2: Amorphous phase Layer 3: Defect phase Phase transition dynamics: $$\\frac{d\\phi_i^{\\alpha}}{dt} = f_i^{\\alpha}(\\phi_i^{\\alpha}) + \\sum_{j \\beta} A_{ij}^{\\alpha \\beta} g_{ij}^{\\alpha \\beta}(\\phi_i^{\\alpha}, \\phi_j^{\\beta})$$\nWhere $\\phi_i^{\\alpha}$ is the order parameter of node $i$ in layer $\\alpha$.\nCritical Behavior Critical temperature: $$T_c = \\frac{K \\langle k^2 \\rangle}{\\langle k \\rangle}$$\nWhere $\\langle k \\rangle$ and $\\langle k^2 \\rangle$ are calculated across all layers.\nDefect Networks Multilayer Defect Networks Network representation:\nLayer 1: Point defects Layer 2: Line defects Layer 3: Surface defects Inter-layer connections: Defect interactions\nMathematical model: $$A_{ij}^{\\alpha \\beta} = \\begin{cases} A_{ij}^{\\alpha} \u0026amp; \\text{if } \\alpha = \\beta \\ C_{ij}^{\\alpha \\beta} \u0026amp; \\text{if } \\alpha \\neq \\beta \\end{cases}$$\nDefect Clustering Multilayer clustering: $$C_i = \\frac{\\sum_{\\alpha} C_i^{\\alpha}}{L}$$\nCross-layer clustering: $$C_i^{\\alpha \\beta} = \\frac{2e_i^{\\alpha \\beta}}{k_i^{\\alpha} k_i^{\\beta}}$$\n15.5 Multilayer Network Models Random Multilayer Networks Erdős-Rényi Multilayer Model: Each layer is an Erdős-Rényi random graph\nParameters:\n$n$: Number of nodes $L$: Number of layers $p^{\\alpha}$: Edge probability in layer $\\alpha$ $p^{\\alpha \\beta}$: Inter-layer edge probability Mathematical formulation: $$P(A_{ij}^{\\alpha \\beta} = 1) = \\begin{cases} p^{\\alpha} \u0026amp; \\text{if } \\alpha = \\beta \\ p^{\\alpha \\beta} \u0026amp; \\text{if } \\alpha \\neq \\beta \\end{cases}$$\nScale-Free Multilayer Model: Each layer follows preferential attachment\nAlgorithm:\nStart with small initial network in each layer Add nodes with preferential attachment Add inter-layer edges with probability $p^{\\alpha \\beta}$ Degree distribution: $$P(k) \\sim k^{-\\gamma}$$\nWhere $\\gamma$ depends on the number of layers and inter-layer connections.\nMultilayer Community Models Stochastic Block Model Model: Nodes belong to communities across layers\nParameters:\n$c_i^{\\alpha}$: Community of node $i$ in layer $\\alpha$ $\\theta_{rs}^{\\alpha \\beta}$: Probability of edge between communities $r$ and $s$ in layers $\\alpha$ and $\\beta$ Mathematical formulation: $$P(A_{ij}^{\\alpha \\beta} = 1) = \\theta_{c_i^{\\alpha} c_j^{\\beta}}^{\\alpha \\beta}$$\nMultilayer Modularity Modularity: $$Q = \\frac{1}{2\\mu} \\sum_{ij \\alpha \\beta} \\left[ A_{ij}^{\\alpha \\beta} - \\frac{k_i^{\\alpha} k_j^{\\beta}}{2m^{\\alpha}} \\right] \\delta(c_i^{\\alpha}, c_j^{\\beta})$$\nOptimization: Maximize modularity to find communities\nCode Example: Multilayer Network Analysis import networkx as nx import numpy as np import matplotlib.pyplot as plt from collections import defaultdict from sklearn.cluster import SpectralClustering from scipy.sparse import csr_matrix class MultilayerNetwork: \u0026#34;\u0026#34;\u0026#34;Multilayer network class\u0026#34;\u0026#34;\u0026#34; def __init__(self, n_nodes, n_layers): self.n_nodes = n_nodes self.n_layers = n_layers self.layers = {} self.inter_layer_edges = defaultdict(list) # Initialize layers for layer in range(n_layers): self.layers[layer] = nx.Graph() for i in range(n_nodes): self.layers[layer].add_node(i) def add_intra_layer_edge(self, layer, u, v, weight=1): \u0026#34;\u0026#34;\u0026#34;Add edge within a layer\u0026#34;\u0026#34;\u0026#34; self.layers[layer].add_edge(u, v, weight=weight) def add_inter_layer_edge(self, layer1, layer2, u, v, weight=1): \u0026#34;\u0026#34;\u0026#34;Add edge between layers\u0026#34;\u0026#34;\u0026#34; self.inter_layer_edges[(layer1, layer2)].append((u, v, weight)) def get_supra_adjacency_matrix(self): \u0026#34;\u0026#34;\u0026#34;Get supra-adjacency matrix\u0026#34;\u0026#34;\u0026#34; n = self.n_nodes L = self.n_layers A = np.zeros((n * L, n * L)) # Intra-layer edges for layer in range(L): G = self.layers[layer] for u, v in G.edges(): weight = G[u][v].get(\u0026#39;weight\u0026#39;, 1) A[u * L + layer, v * L + layer] = weight A[v * L + layer, u * L + layer] = weight # Inter-layer edges for (layer1, layer2), edges in self.inter_layer_edges.items(): for u, v, weight in edges: A[u * L + layer1, v * L + layer2] = weight A[v * L + layer2, u * L + layer1] = weight return A def get_multilayer_degree(self, node): \u0026#34;\u0026#34;\u0026#34;Get multilayer degree of a node\u0026#34;\u0026#34;\u0026#34; degree = 0 for layer in self.layers: degree += self.layers[layer].degree(node) return degree def get_overlapping_degree(self, node): \u0026#34;\u0026#34;\u0026#34;Get overlapping degree of a node\u0026#34;\u0026#34;\u0026#34; overlapping = 0 for layer in self.layers: if self.layers[layer].degree(node) \u0026gt; 0: overlapping += 1 return overlapping def get_participation_coefficient(self, node): \u0026#34;\u0026#34;\u0026#34;Get participation coefficient of a node\u0026#34;\u0026#34;\u0026#34; degrees = [self.layers[layer].degree(node) for layer in self.layers] total_degree = sum(degrees) if total_degree == 0: return 0 participation = 1 - sum((d / total_degree) ** 2 for d in degrees) return participation def get_multilayer_clustering(self, node): \u0026#34;\u0026#34;\u0026#34;Get multilayer clustering coefficient of a node\u0026#34;\u0026#34;\u0026#34; clusterings = [] for layer in self.layers: if self.layers[layer].degree(node) \u0026gt; 1: clustering = nx.clustering(self.layers[layer], node) clusterings.append(clustering) if not clusterings: return 0 return np.mean(clusterings) def get_multilayer_pagerank(self, damping=0.85, max_iter=100, tol=1e-6): \u0026#34;\u0026#34;\u0026#34;Get multilayer PageRank\u0026#34;\u0026#34;\u0026#34; n = self.n_nodes L = self.n_layers # Initialize PageRank values pr = np.ones(n * L) / (n * L) # Get supra-adjacency matrix A = self.get_supra_adjacency_matrix() # Calculate transition matrix P = np.zeros_like(A) for i in range(n * L): row_sum = np.sum(A[i, :]) if row_sum \u0026gt; 0: P[i, :] = A[i, :] / row_sum # Iterate PageRank for _ in range(max_iter): pr_new = (1 - damping) / (n * L) + damping * np.dot(P.T, pr) if np.linalg.norm(pr_new - pr) \u0026lt; tol: break pr = pr_new # Reshape to node-layer format pr_reshaped = pr.reshape(n, L) return pr_reshaped def detect_communities(self, n_communities=3): \u0026#34;\u0026#34;\u0026#34;Detect communities using spectral clustering\u0026#34;\u0026#34;\u0026#34; A = self.get_supra_adjacency_matrix() # Spectral clustering clustering = SpectralClustering(n_clusters=n_communities, affinity=\u0026#39;precomputed\u0026#39;, random_state=42) labels = clustering.fit_predict(A) # Reshape to node-layer format labels_reshaped = labels.reshape(self.n_nodes, self.n_layers) return labels_reshaped def calculate_multilayer_modularity(self, communities): \u0026#34;\u0026#34;\u0026#34;Calculate multilayer modularity\u0026#34;\u0026#34;\u0026#34; n = self.n_nodes L = self.n_layers # Calculate total number of edges mu = 0 for layer in self.layers: mu += self.layers[layer].number_of_edges() # Calculate modularity Q = 0 for i in range(n): for j in range(n): for alpha in range(L): for beta in range(L): # Get edge weight if alpha == beta: # Intra-layer edge if self.layers[alpha].has_edge(i, j): A_ij = 1 else: A_ij = 0 else: # Inter-layer edge A_ij = 0 for (layer1, layer2), edges in self.inter_layer_edges.items(): if layer1 == alpha and layer2 == beta: for u, v, weight in edges: if u == i and v == j: A_ij = weight break # Get degrees k_i_alpha = self.layers[alpha].degree(i) k_j_beta = self.layers[beta].degree(j) # Get community assignments c_i_alpha = communities[i, alpha] c_j_beta = communities[j, beta] # Calculate modularity contribution if c_i_alpha == c_j_beta: Q += A_ij - (k_i_alpha * k_j_beta) / (2 * mu) return Q / (2 * mu) def generate_multilayer_network(n_nodes, n_layers, p_intra=0.1, p_inter=0.05): \u0026#34;\u0026#34;\u0026#34;Generate random multilayer network\u0026#34;\u0026#34;\u0026#34; # Create multilayer network ml_net = MultilayerNetwork(n_nodes, n_layers) # Add intra-layer edges for layer in range(n_layers): for i in range(n_nodes): for j in range(i+1, n_nodes): if np.random.random() \u0026lt; p_intra: ml_net.add_intra_layer_edge(layer, i, j) # Add inter-layer edges for layer1 in range(n_layers): for layer2 in range(layer1+1, n_layers): for i in range(n_nodes): for j in range(n_nodes): if np.random.random() \u0026lt; p_inter: ml_net.add_inter_layer_edge(layer1, layer2, i, j) return ml_net def analyze_multilayer_network(ml_net): \u0026#34;\u0026#34;\u0026#34;Analyze multilayer network properties\u0026#34;\u0026#34;\u0026#34; n = ml_net.n_nodes L = ml_net.n_layers # Calculate measures for each node measures = {} for i in range(n): measures[i] = { \u0026#39;multilayer_degree\u0026#39;: ml_net.get_multilayer_degree(i), \u0026#39;overlapping_degree\u0026#39;: ml_net.get_overlapping_degree(i), \u0026#39;participation_coefficient\u0026#39;: ml_net.get_participation_coefficient(i), \u0026#39;multilayer_clustering\u0026#39;: ml_net.get_multilayer_clustering(i) } # Calculate layer-specific measures layer_measures = {} for layer in range(L): G = ml_net.layers[layer] layer_measures[layer] = { \u0026#39;nodes\u0026#39;: G.number_of_nodes(), \u0026#39;edges\u0026#39;: G.number_of_edges(), \u0026#39;density\u0026#39;: nx.density(G), \u0026#39;clustering\u0026#39;: nx.average_clustering(G), \u0026#39;avg_path_length\u0026#39;: nx.average_shortest_path_length(G) if nx.is_connected(G) else float(\u0026#39;inf\u0026#39;) } return measures, layer_measures def plot_multilayer_analysis(ml_net, measures, layer_measures, communities): \u0026#34;\u0026#34;\u0026#34;Plot multilayer network analysis\u0026#34;\u0026#34;\u0026#34; fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12)) # Multilayer degree distribution degrees = [measures[i][\u0026#39;multilayer_degree\u0026#39;] for i in range(ml_net.n_nodes)] ax1.hist(degrees, bins=20, alpha=0.7, edgecolor=\u0026#39;black\u0026#39;) ax1.set_xlabel(\u0026#39;Multilayer Degree\u0026#39;) ax1.set_ylabel(\u0026#39;Count\u0026#39;) ax1.set_title(\u0026#39;Multilayer Degree Distribution\u0026#39;) ax1.grid(True, alpha=0.3) # Participation coefficient participation = [measures[i][\u0026#39;participation_coefficient\u0026#39;] for i in range(ml_net.n_nodes)] ax2.hist(participation, bins=20, alpha=0.7, edgecolor=\u0026#39;black\u0026#39;) ax2.set_xlabel(\u0026#39;Participation Coefficient\u0026#39;) ax2.set_ylabel(\u0026#39;Count\u0026#39;) ax2.set_title(\u0026#39;Participation Coefficient Distribution\u0026#39;) ax2.grid(True, alpha=0.3) # Layer comparison layers = list(layer_measures.keys()) densities = [layer_measures[layer][\u0026#39;density\u0026#39;] for layer in layers] clusterings = [layer_measures[layer][\u0026#39;clustering\u0026#39;] for layer in layers] ax3.scatter(densities, clusterings, s=100, alpha=0.7) for i, layer in enumerate(layers): ax3.annotate(f\u0026#39;Layer {layer}\u0026#39;, (densities[i], clusterings[i])) ax3.set_xlabel(\u0026#39;Density\u0026#39;) ax3.set_ylabel(\u0026#39;Clustering Coefficient\u0026#39;) ax3.set_title(\u0026#39;Layer Properties\u0026#39;) ax3.grid(True, alpha=0.3) # Community structure n_communities = len(np.unique(communities)) community_sizes = [] for comm in range(n_communities): size = np.sum(communities == comm) community_sizes.append(size) ax4.bar(range(n_communities), community_sizes, alpha=0.7, color=\u0026#39;skyblue\u0026#39;) ax4.set_xlabel(\u0026#39;Community\u0026#39;) ax4.set_ylabel(\u0026#39;Size\u0026#39;) ax4.set_title(\u0026#39;Community Size Distribution\u0026#39;) ax4.grid(True, alpha=0.3) plt.tight_layout() plt.show() # Example: Multilayer network analysis n_nodes, n_layers = 50, 3 ml_net = generate_multilayer_network(n_nodes, n_layers, p_intra=0.1, p_inter=0.05) # Analyze network measures, layer_measures = analyze_multilayer_network(ml_net) # Detect communities communities = ml_net.detect_communities(n_communities=3) # Calculate modularity modularity = ml_net.calculate_multilayer_modularity(communities) # Get PageRank pagerank = ml_net.get_multilayer_pagerank() print(\u0026#34;Multilayer Network Analysis:\u0026#34;) print(f\u0026#34;Number of nodes: {n_nodes}\u0026#34;) print(f\u0026#34;Number of layers: {n_layers}\u0026#34;) print(f\u0026#34;Modularity: {modularity:.3f}\u0026#34;) print(\u0026#34;\\nLayer Properties:\u0026#34;) for layer, props in layer_measures.items(): print(f\u0026#34;Layer {layer}: {props[\u0026#39;edges\u0026#39;]} edges, density={props[\u0026#39;density\u0026#39;]:.3f}, clustering={props[\u0026#39;clustering\u0026#39;]:.3f}\u0026#34;) print(\u0026#34;\\nNode Properties (first 5 nodes):\u0026#34;) for i in range(min(5, n_nodes)): print(f\u0026#34;Node {i}: degree={measures[i][\u0026#39;multilayer_degree\u0026#39;]}, participation={measures[i][\u0026#39;participation_coefficient\u0026#39;]:.3f}\u0026#34;) # Plot analysis plot_multilayer_analysis(ml_net, measures, layer_measures, communities) Key Takeaways Multilayer networks: Networks with multiple layers of relationships Mathematical framework: Supra-adjacency matrix and tensor representations Network measures: Degree, clustering, and centrality measures for multilayer networks Community detection: Spectral clustering and modularity optimization Random walks: PageRank and other random walk methods Applications: Important for materials science and complex systems Analysis methods: Comprehensive tools for studying multilayer networks References Newman, M. E. J. (2010). Networks: An Introduction. Oxford University Press. Kivelä, M., et al. (2014). Multilayer networks. Journal of Complex Networks, 2(3), 203-271. Boccaletti, S., et al. (2014). The structure and dynamics of multilayer networks. Physics Reports, 544(1), 1-122. De Domenico, M., et al. (2013). Mathematical formulation of multilayer networks. Physical Review X, 3(4), 041022. Multilayer networks provide a powerful framework for understanding complex systems with multiple types of relationships, with important applications in materials science and engineering.\n","permalink":"https://Linlin-resh.github.io/posts/reading-notes-newman-ch15/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eChapter 15 of Newman\u0026rsquo;s \u003cem\u003eNetworks: An Introduction\u003c/em\u003e explores \u003cstrong\u003emultilayer networks\u003c/strong\u003e - networks that consist of multiple layers, each representing different types of relationships or interactions. This chapter covers the mathematical framework, analysis methods, and applications of multilayer networks.\u003c/p\u003e\n\u003ch2 id=\"151-multilayer-network-structure\"\u003e15.1 Multilayer Network Structure\u003c/h2\u003e\n\u003ch3 id=\"definition\"\u003eDefinition\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eMultilayer network\u003c/strong\u003e is a collection of networks with:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eNodes\u003c/strong\u003e: Can exist in multiple layers\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIntra-layer edges\u003c/strong\u003e: Connections within each layer\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInter-layer edges\u003c/strong\u003e: Connections between layers\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLayers\u003c/strong\u003e: Different types of relationships or time periods\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"mathematical-representation\"\u003eMathematical Representation\u003c/h3\u003e\n\u003ch4 id=\"supra-adjacency-matrix\"\u003eSupra-adjacency Matrix\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eSupra-adjacency matrix\u003c/strong\u003e:\n$$A = \\bigoplus_{\\alpha} A^{\\alpha} + \\bigoplus_{\\alpha \\neq \\beta} C^{\\alpha \\beta}$$\u003c/p\u003e","title":"Reading Notes: Newman's Networks Chapter 15 - Multilayer Networks"},{"content":"Introduction Chapter 16 of Newman\u0026rsquo;s Networks: An Introduction explores network resilience - the ability of networks to maintain their functionality under stress, attacks, or failures. This chapter covers robustness measures, failure analysis, and strategies for enhancing network resilience.\n16.1 Robustness Measures Connectivity Robustness Robustness Function Robustness function: $$R(p) = \\frac{1}{n} \\sum_{i=1}^n \\frac{S_i(p)}{n}$$\nWhere:\n$p$: Fraction of nodes removed $S_i(p)$: Size of largest component after removing $i$ nodes $n$: Total number of nodes Properties:\n$R(0) = 1$: No nodes removed $R(1) = 0$: All nodes removed $R(p)$: Monotonic decreasing function Critical Threshold Critical threshold $p_c$: $$R(p_c) = \\frac{1}{2}$$\nInterpretation: Fraction of nodes that can be removed before network loses half its functionality\nMathematical condition: $$p_c = \\frac{1}{\\kappa - 1}$$\nWhere $\\kappa = \\frac{\\langle k^2 \\rangle}{\\langle k \\rangle}$ is the degree ratio.\nEfficiency Robustness Network Efficiency Global efficiency: $$E = \\frac{1}{n(n-1)} \\sum_{i \\neq j} \\frac{1}{d_{ij}}$$\nWhere $d_{ij}$ is the shortest path length between nodes $i$ and $j$.\nLocal efficiency: $$E_{\\text{local}} = \\frac{1}{n} \\sum_{i} E_i$$\nWhere $E_i$ is the efficiency of the subgraph of neighbors of node $i$.\nEfficiency Robustness Efficiency robustness: $$R_E(p) = \\frac{E(p)}{E(0)}$$\nWhere $E(p)$ is the efficiency after removing fraction $p$ of nodes.\nProperties:\n$R_E(0) = 1$: No nodes removed $R_E(1) = 0$: All nodes removed $R_E(p)$: Monotonic decreasing function Spectral Robustness Algebraic Connectivity Algebraic connectivity $\\lambda_2$: $$\\lambda_2 = \\min_{x \\perp \\mathbf{1}} \\frac{x^T L x}{x^T x}$$\nWhere $L$ is the Laplacian matrix.\nProperties:\n$\\lambda_2 \u0026gt; 0$: Network is connected $\\lambda_2 = 0$: Network is disconnected Higher $\\lambda_2$: More robust network Spectral Robustness Spectral robustness: $$R_{\\lambda}(p) = \\frac{\\lambda_2(p)}{\\lambda_2(0)}$$\nWhere $\\lambda_2(p)$ is the algebraic connectivity after removing fraction $p$ of nodes.\n16.2 Attack Strategies Random Attacks Random Node Removal Process: Remove nodes randomly with probability $p$\nRobustness: $R_{\\text{random}}(p) = 1 - p$\nCritical threshold: $p_c = 0.5$\nMathematical analysis: $$R_{\\text{random}}(p) = \\frac{1}{n} \\sum_{i=1}^n \\frac{S_i(p)}{n}$$\nWhere $S_i(p)$ is the size of largest component after removing $i$ nodes.\nRandom Edge Removal Process: Remove edges randomly with probability $p$\nRobustness: $R_{\\text{random}}(p) = 1 - p$\nCritical threshold: $p_c = 0.5$\nMathematical analysis: $$R_{\\text{random}}(p) = \\frac{1}{m} \\sum_{i=1}^m \\frac{S_i(p)}{m}$$\nWhere $S_i(p)$ is the size of largest component after removing $i$ edges.\nTargeted Attacks Degree-Based Attacks Process: Remove nodes with highest degree\nRobustness: $R_{\\text{targeted}}(p) \u0026lt; R_{\\text{random}}(p)$\nCritical threshold: $p_c \u0026lt; 0.5$\nMathematical analysis: $$R_{\\text{targeted}}(p) = \\frac{1}{n} \\sum_{i=1}^n \\frac{S_i(p)}{n}$$\nWhere $S_i(p)$ is the size of largest component after removing $i$ highest-degree nodes.\nBetweenness-Based Attacks Process: Remove nodes with highest betweenness centrality\nRobustness: $R_{\\text{targeted}}(p) \u0026lt; R_{\\text{random}}(p)$\nCritical threshold: $p_c \u0026lt; 0.5$\nMathematical analysis: $$R_{\\text{targeted}}(p) = \\frac{1}{n} \\sum_{i=1}^n \\frac{S_i(p)}{n}$$\nWhere $S_i(p)$ is the size of largest component after removing $i$ highest-betweenness nodes.\nCloseness-Based Attacks Process: Remove nodes with highest closeness centrality\nRobustness: $R_{\\text{targeted}}(p) \u0026lt; R_{\\text{random}}(p)$\nCritical threshold: $p_c \u0026lt; 0.5$\nMathematical analysis: $$R_{\\text{targeted}}(p) = \\frac{1}{n} \\sum_{i=1}^n \\frac{S_i(p)}{n}$$\nWhere $S_i(p)$ is the size of largest component after removing $i$ highest-closeness nodes.\nAdaptive Attacks Adaptive Degree-Based Attacks Process:\nRemove node with highest degree Recalculate degrees Repeat Robustness: $R_{\\text{adaptive}}(p) \u0026lt; R_{\\text{targeted}}(p)$\nCritical threshold: $p_c \u0026lt; 0.5$\nMathematical analysis: $$R_{\\text{adaptive}}(p) = \\frac{1}{n} \\sum_{i=1}^n \\frac{S_i(p)}{n}$$\nWhere $S_i(p)$ is the size of largest component after removing $i$ nodes adaptively.\nAdaptive Betweenness-Based Attacks Process:\nRemove node with highest betweenness Recalculate betweenness Repeat Robustness: $R_{\\text{adaptive}}(p) \u0026lt; R_{\\text{targeted}}(p)$\nCritical threshold: $p_c \u0026lt; 0.5$\nMathematical analysis: $$R_{\\text{adaptive}}(p) = \\frac{1}{n} \\sum_{i=1}^n \\frac{S_i(p)}{n}$$\nWhere $S_i(p)$ is the size of largest component after removing $i$ nodes adaptively.\n16.3 Failure Cascades Cascade Models Load Redistribution Model Model: When a node fails, its load is redistributed to neighbors\nLoad evolution: $$L_i(t+1) = L_i(t) + \\sum_{j \\in \\text{failed}} \\frac{L_j(t)}{|\\text{neighbors}(j)|}$$\nWhere:\n$L_i(t)$: Load of node $i$ at time $t$ $\\text{failed}$: Set of failed nodes $|\\text{neighbors}(j)|$: Number of neighbors of node $j$ Threshold Model Model: Node fails if its load exceeds threshold\nFailure condition: $$L_i(t) \u0026gt; T_i$$\nWhere $T_i$ is the threshold of node $i$.\nCascade condition: $$\\frac{\\langle k^2 \\rangle}{\\langle k \\rangle} \u0026gt; 2$$\nWhere $\\langle k^2 \\rangle$ and $\\langle k \\rangle$ are the second and first moments of the degree distribution.\nCascade Analysis Cascade Size Cascade size $S$: $$S = \\frac{1}{n} \\sum_{i=1}^n S_i$$\nWhere $S_i$ is the size of cascade starting from node $i$.\nExpected cascade size: $$\\langle S \\rangle = \\frac{1}{n} \\sum_{i=1}^n \\langle S_i \\rangle$$\nCascade Probability Cascade probability $P$: $$P = \\frac{1}{n} \\sum_{i=1}^n P_i$$\nWhere $P_i$ is the probability of cascade starting from node $i$.\nExpected cascade probability: $$\\langle P \\rangle = \\frac{1}{n} \\sum_{i=1}^n \\langle P_i \\rangle$$\n16.4 Resilience Enhancement Redundancy Edge Redundancy Edge redundancy: $$R_E = \\frac{m - m_{\\min}}{m_{\\min}}$$\nWhere:\n$m$: Number of edges $m_{\\min}$: Minimum number of edges for connectivity Properties:\n$R_E \\geq 0$: Always non-negative $R_E = 0$: No redundancy Higher $R_E$: More redundant network Node Redundancy Node redundancy: $$R_N = \\frac{n - n_{\\min}}{n_{\\min}}$$\nWhere:\n$n$: Number of nodes $n_{\\min}$: Minimum number of nodes for functionality Properties:\n$R_N \\geq 0$: Always non-negative $R_N = 0$: No redundancy Higher $R_N$: More redundant network Diversity Degree Diversity Degree diversity: $$D_k = \\frac{\\sigma_k}{\\langle k \\rangle}$$\nWhere:\n$\\sigma_k$: Standard deviation of degrees $\\langle k \\rangle$: Average degree Properties:\n$D_k \\geq 0$: Always non-negative $D_k = 0$: No diversity Higher $D_k$: More diverse network Path Diversity Path diversity: $$D_p = \\frac{1}{n(n-1)} \\sum_{i \\neq j} \\frac{\\sigma_{ij}}{\\langle d_{ij} \\rangle}$$\nWhere:\n$\\sigma_{ij}$: Standard deviation of path lengths between $i$ and $j$ $\\langle d_{ij} \\rangle$: Average path length between $i$ and $j$ Properties:\n$D_p \\geq 0$: Always non-negative $D_p = 0$: No diversity Higher $D_p$: More diverse network Modularity Modular Structure Modular structure: Network divided into modules with dense internal connections and sparse external connections\nModularity: $$Q = \\frac{1}{2m} \\sum_{ij} \\left[ A_{ij} - \\frac{k_i k_j}{2m} \\right] \\delta(c_i, c_j)$$\nWhere:\n$A_{ij}$: Adjacency matrix $k_i, k_j$: Degrees of nodes $i, j$ $c_i, c_j$: Community assignments $\\delta(c_i, c_j)$: Kronecker delta Properties:\n$Q \\in [-1, 1]$: Bounded $Q \u0026gt; 0$: More modular than random $Q \u0026lt; 0$: Less modular than random Resilience Benefits Modular networks:\nLocalized failures: Failures contained within modules Faster recovery: Modules can recover independently Reduced cascades: Failures less likely to spread 16.5 Applications to Materials Science Defect Networks Defect Tolerance Defect network robustness:\nRandom defects: Random node removal Clustered defects: Targeted node removal Critical defect concentration: $c_c = \\frac{1}{\\kappa - 1}$ Defect Clustering Defect clustering: $$C = \\frac{\\langle k^2 \\rangle - \\langle k \\rangle}{n \\langle k \\rangle^2}$$\nResilience benefits:\nLocalized damage: Clustered defects cause localized damage Faster healing: Defects can heal locally Reduced cascades: Defect propagation limited Nanowire Networks Electrical Robustness Electrical robustness: $$R_E = \\frac{\\sigma(p)}{\\sigma(0)}$$\nWhere $\\sigma(p)$ is the conductivity after removing fraction $p$ of nanowires.\nCritical threshold: $$p_c = \\frac{1}{\\kappa - 1}$$\nWhere $\\kappa = \\frac{\\langle k^2 \\rangle}{\\langle k \\rangle}$.\nMechanical Robustness Mechanical robustness: $$R_M = \\frac{E(p)}{E(0)}$$\nWhere $E(p)$ is the Young\u0026rsquo;s modulus after removing fraction $p$ of nanowires.\nCritical threshold: $$p_c = \\frac{1}{\\kappa - 1}$$\nWhere $\\kappa = \\frac{\\langle k^2 \\rangle}{\\langle k \\rangle}$.\nPhase Transitions Phase Stability Phase stability: $$S = \\frac{1}{n} \\sum_{i=1}^n S_i$$\nWhere $S_i$ is the stability of phase at node $i$.\nCritical temperature: $$T_c = \\frac{K \\langle k^2 \\rangle}{2 \\langle k \\rangle}$$\nWhere $K$ is the coupling strength.\nPhase Transition Robustness Phase transition robustness: $$R_P = \\frac{T_c(p)}{T_c(0)}$$\nWhere $T_c(p)$ is the critical temperature after removing fraction $p$ of nodes.\nCritical threshold: $$p_c = \\frac{1}{\\kappa - 1}$$\nWhere $\\kappa = \\frac{\\langle k^2 \\rangle}{\\langle k \\rangle}$.\nCode Example: Network Resilience Analysis import networkx as nx import numpy as np import matplotlib.pyplot as plt from collections import defaultdict from scipy.sparse import csgraph from sklearn.metrics import pairwise_distances def calculate_robustness_measures(G): \u0026#34;\u0026#34;\u0026#34;Calculate various robustness measures\u0026#34;\u0026#34;\u0026#34; n = G.number_of_nodes() m = G.number_of_edges() # Connectivity robustness if nx.is_connected(G): connectivity_robustness = 1.0 else: # Calculate largest component size components = list(nx.connected_components(G)) largest_component = max(components, key=len) connectivity_robustness = len(largest_component) / n # Efficiency robustness if nx.is_connected(G): efficiency = nx.global_efficiency(G) else: # Calculate efficiency of largest component components = list(nx.connected_components(G)) largest_component = max(components, key=len) subgraph = G.subgraph(largest_component) efficiency = nx.global_efficiency(subgraph) # Spectral robustness L = nx.laplacian_matrix(G).toarray() eigenvals = np.linalg.eigvals(L) eigenvals = np.real(eigenvals) eigenvals = np.sort(eigenvals) algebraic_connectivity = eigenvals[1] if len(eigenvals) \u0026gt; 1 else 0 # Redundancy measures edge_redundancy = (m - (n - 1)) / (n - 1) if n \u0026gt; 1 else 0 node_redundancy = (n - 1) / 1 if n \u0026gt; 1 else 0 # Diversity measures degrees = [G.degree(i) for i in G.nodes()] degree_diversity = np.std(degrees) / np.mean(degrees) if np.mean(degrees) \u0026gt; 0 else 0 # Path diversity if nx.is_connected(G): path_lengths = dict(nx.all_pairs_shortest_path_length(G)) path_diversity = 0 count = 0 for i in G.nodes(): for j in G.nodes(): if i != j: paths = path_lengths[i][j] path_diversity += paths count += 1 path_diversity = path_diversity / count if count \u0026gt; 0 else 0 else: path_diversity = 0 return { \u0026#39;connectivity_robustness\u0026#39;: connectivity_robustness, \u0026#39;efficiency\u0026#39;: efficiency, \u0026#39;algebraic_connectivity\u0026#39;: algebraic_connectivity, \u0026#39;edge_redundancy\u0026#39;: edge_redundancy, \u0026#39;node_redundancy\u0026#39;: node_redundancy, \u0026#39;degree_diversity\u0026#39;: degree_diversity, \u0026#39;path_diversity\u0026#39;: path_diversity } def simulate_random_attack(G, p_values): \u0026#34;\u0026#34;\u0026#34;Simulate random attack on network\u0026#34;\u0026#34;\u0026#34; n = G.number_of_nodes() results = [] for p in p_values: # Remove random nodes n_remove = int(p * n) nodes_to_remove = np.random.choice(list(G.nodes()), n_remove, replace=False) # Create attacked network G_attacked = G.copy() G_attacked.remove_nodes_from(nodes_to_remove) # Calculate robustness measures measures = calculate_robustness_measures(G_attacked) measures[\u0026#39;p\u0026#39;] = p results.append(measures) return results def simulate_targeted_attack(G, p_values, attack_type=\u0026#39;degree\u0026#39;): \u0026#34;\u0026#34;\u0026#34;Simulate targeted attack on network\u0026#34;\u0026#34;\u0026#34; n = G.number_of_nodes() results = [] for p in p_values: # Remove targeted nodes n_remove = int(p * n) if attack_type == \u0026#39;degree\u0026#39;: # Remove highest degree nodes degrees = [(i, G.degree(i)) for i in G.nodes()] degrees.sort(key=lambda x: x[1], reverse=True) nodes_to_remove = [i for i, _ in degrees[:n_remove]] elif attack_type == \u0026#39;betweenness\u0026#39;: # Remove highest betweenness nodes betweenness = nx.betweenness_centrality(G) nodes_to_remove = sorted(betweenness.keys(), key=lambda x: betweenness[x], reverse=True)[:n_remove] elif attack_type == \u0026#39;closeness\u0026#39;: # Remove highest closeness nodes closeness = nx.closeness_centrality(G) nodes_to_remove = sorted(closeness.keys(), key=lambda x: closeness[x], reverse=True)[:n_remove] else: # Random attack nodes_to_remove = np.random.choice(list(G.nodes()), n_remove, replace=False) # Create attacked network G_attacked = G.copy() G_attacked.remove_nodes_from(nodes_to_remove) # Calculate robustness measures measures = calculate_robustness_measures(G_attacked) measures[\u0026#39;p\u0026#39;] = p measures[\u0026#39;attack_type\u0026#39;] = attack_type results.append(measures) return results def simulate_cascade_failure(G, initial_failures, threshold_factor=1.5): \u0026#34;\u0026#34;\u0026#34;Simulate cascade failure in network\u0026#34;\u0026#34;\u0026#34; n = G.number_of_nodes() # Initialize loads loads = {i: G.degree(i) for i in G.nodes()} thresholds = {i: loads[i] * threshold_factor for i in G.nodes()} # Initial failures failed_nodes = set(initial_failures) active_nodes = set(G.nodes()) - failed_nodes # Simulate cascade cascade_size = len(failed_nodes) cascade_steps = 0 while True: new_failures = set() # Check each active node for node in active_nodes: if loads[node] \u0026gt; thresholds[node]: new_failures.add(node) # If no new failures, cascade stops if not new_failures: break # Update failed nodes failed_nodes.update(new_failures) active_nodes -= new_failures # Redistribute loads for failed_node in new_failures: neighbors = list(G.neighbors(failed_node)) active_neighbors = [n for n in neighbors if n in active_nodes] if active_neighbors: load_per_neighbor = loads[failed_node] / len(active_neighbors) for neighbor in active_neighbors: loads[neighbor] += load_per_neighbor cascade_size = len(failed_nodes) cascade_steps += 1 # Prevent infinite loops if cascade_steps \u0026gt; n: break return { \u0026#39;cascade_size\u0026#39;: cascade_size, \u0026#39;cascade_steps\u0026#39;: cascade_steps, \u0026#39;final_failed_nodes\u0026#39;: failed_nodes } def analyze_network_resilience(G): \u0026#34;\u0026#34;\u0026#34;Comprehensive network resilience analysis\u0026#34;\u0026#34;\u0026#34; # Calculate initial robustness measures initial_measures = calculate_robustness_measures(G) # Simulate different types of attacks p_values = np.linspace(0, 0.5, 20) random_results = simulate_random_attack(G, p_values) degree_results = simulate_targeted_attack(G, p_values, \u0026#39;degree\u0026#39;) betweenness_results = simulate_targeted_attack(G, p_values, \u0026#39;betweenness\u0026#39;) closeness_results = simulate_targeted_attack(G, p_values, \u0026#39;closeness\u0026#39;) # Simulate cascade failures cascade_results = [] for i in range(10): # Test 10 different initial failures initial_failures = np.random.choice(list(G.nodes()), 1, replace=False) cascade_result = simulate_cascade_failure(G, initial_failures) cascade_results.append(cascade_result) return { \u0026#39;initial_measures\u0026#39;: initial_measures, \u0026#39;random_results\u0026#39;: random_results, \u0026#39;degree_results\u0026#39;: degree_results, \u0026#39;betweenness_results\u0026#39;: betweenness_results, \u0026#39;closeness_results\u0026#39;: closeness_results, \u0026#39;cascade_results\u0026#39;: cascade_results } def plot_resilience_analysis(G, analysis_results): \u0026#34;\u0026#34;\u0026#34;Plot network resilience analysis results\u0026#34;\u0026#34;\u0026#34; fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12)) # Attack robustness comparison p_values = [r[\u0026#39;p\u0026#39;] for r in analysis_results[\u0026#39;random_results\u0026#39;]] # Connectivity robustness random_connectivity = [r[\u0026#39;connectivity_robustness\u0026#39;] for r in analysis_results[\u0026#39;random_results\u0026#39;]] degree_connectivity = [r[\u0026#39;connectivity_robustness\u0026#39;] for r in analysis_results[\u0026#39;degree_results\u0026#39;]] betweenness_connectivity = [r[\u0026#39;connectivity_robustness\u0026#39;] for r in analysis_results[\u0026#39;betweenness_results\u0026#39;]] closeness_connectivity = [r[\u0026#39;connectivity_robustness\u0026#39;] for r in analysis_results[\u0026#39;closeness_results\u0026#39;]] ax1.plot(p_values, random_connectivity, \u0026#39;b-\u0026#39;, label=\u0026#39;Random\u0026#39;, linewidth=2) ax1.plot(p_values, degree_connectivity, \u0026#39;r-\u0026#39;, label=\u0026#39;Degree\u0026#39;, linewidth=2) ax1.plot(p_values, betweenness_connectivity, \u0026#39;g-\u0026#39;, label=\u0026#39;Betweenness\u0026#39;, linewidth=2) ax1.plot(p_values, closeness_connectivity, \u0026#39;m-\u0026#39;, label=\u0026#39;Closeness\u0026#39;, linewidth=2) ax1.set_xlabel(\u0026#39;Fraction of Nodes Removed\u0026#39;) ax1.set_ylabel(\u0026#39;Connectivity Robustness\u0026#39;) ax1.set_title(\u0026#39;Attack Robustness Comparison\u0026#39;) ax1.legend() ax1.grid(True, alpha=0.3) # Efficiency robustness random_efficiency = [r[\u0026#39;efficiency\u0026#39;] for r in analysis_results[\u0026#39;random_results\u0026#39;]] degree_efficiency = [r[\u0026#39;efficiency\u0026#39;] for r in analysis_results[\u0026#39;degree_results\u0026#39;]] betweenness_efficiency = [r[\u0026#39;efficiency\u0026#39;] for r in analysis_results[\u0026#39;betweenness_results\u0026#39;]] closeness_efficiency = [r[\u0026#39;efficiency\u0026#39;] for r in analysis_results[\u0026#39;closeness_results\u0026#39;]] ax2.plot(p_values, random_efficiency, \u0026#39;b-\u0026#39;, label=\u0026#39;Random\u0026#39;, linewidth=2) ax2.plot(p_values, degree_efficiency, \u0026#39;r-\u0026#39;, label=\u0026#39;Degree\u0026#39;, linewidth=2) ax2.plot(p_values, betweenness_efficiency, \u0026#39;g-\u0026#39;, label=\u0026#39;Betweenness\u0026#39;, linewidth=2) ax2.plot(p_values, closeness_efficiency, \u0026#39;m-\u0026#39;, label=\u0026#39;Closeness\u0026#39;, linewidth=2) ax2.set_xlabel(\u0026#39;Fraction of Nodes Removed\u0026#39;) ax2.set_ylabel(\u0026#39;Network Efficiency\u0026#39;) ax2.set_title(\u0026#39;Efficiency Robustness Comparison\u0026#39;) ax2.legend() ax2.grid(True, alpha=0.3) # Cascade failure analysis cascade_sizes = [r[\u0026#39;cascade_size\u0026#39;] for r in analysis_results[\u0026#39;cascade_results\u0026#39;]] cascade_steps = [r[\u0026#39;cascade_steps\u0026#39;] for r in analysis_results[\u0026#39;cascade_results\u0026#39;]] ax3.scatter(cascade_steps, cascade_sizes, alpha=0.7, s=100) ax3.set_xlabel(\u0026#39;Cascade Steps\u0026#39;) ax3.set_ylabel(\u0026#39;Cascade Size\u0026#39;) ax3.set_title(\u0026#39;Cascade Failure Analysis\u0026#39;) ax3.grid(True, alpha=0.3) # Network visualization with robustness measures pos = nx.spring_layout(G, k=1, iterations=50) # Color nodes by degree degrees = [G.degree(i) for i in G.nodes()] node_colors = degrees node_sizes = 100 + 50 * np.array(degrees) / np.max(degrees) nx.draw(G, pos, ax=ax4, node_color=node_colors, node_size=node_sizes, edge_color=\u0026#39;gray\u0026#39;, alpha=0.6, cmap=\u0026#39;viridis\u0026#39;) ax4.set_title(\u0026#39;Network with Degree-based Coloring\u0026#39;) ax4.axis(\u0026#39;off\u0026#39;) plt.tight_layout() plt.show() # Example: Network resilience analysis G = nx.barabasi_albert_graph(100, 3) # Analyze network resilience analysis_results = analyze_network_resilience(G) # Print initial measures print(\u0026#34;Initial Network Robustness Measures:\u0026#34;) for measure, value in analysis_results[\u0026#39;initial_measures\u0026#39;].items(): print(f\u0026#34;{measure}: {value:.3f}\u0026#34;) # Print cascade analysis cascade_sizes = [r[\u0026#39;cascade_size\u0026#39;] for r in analysis_results[\u0026#39;cascade_results\u0026#39;]] cascade_steps = [r[\u0026#39;cascade_steps\u0026#39;] for r in analysis_results[\u0026#39;cascade_results\u0026#39;]] print(f\u0026#34;\\nCascade Failure Analysis:\u0026#34;) print(f\u0026#34;Average cascade size: {np.mean(cascade_sizes):.1f}\u0026#34;) print(f\u0026#34;Average cascade steps: {np.mean(cascade_steps):.1f}\u0026#34;) # Plot results plot_resilience_analysis(G, analysis_results) Key Takeaways Robustness measures: Multiple ways to quantify network resilience Attack strategies: Random vs. targeted attacks have different effects Failure cascades: Understanding how failures propagate through networks Resilience enhancement: Strategies for improving network robustness Applications: Important for materials science and engineering Mathematical analysis: Rigorous theory for understanding resilience Practical implications: Design principles for robust networks References Newman, M. E. J. (2010). Networks: An Introduction. Oxford University Press. Albert, R., Jeong, H., \u0026amp; Barabási, A. L. (2000). Error and attack tolerance of complex networks. Nature, 406(6794), 378-382. Callaway, D. S., et al. (2000). Network robustness and fragility: percolation on random graphs. Physical Review Letters, 85(25), 5468. Holme, P., et al. (2002). Attack vulnerability of complex networks. Physical Review E, 65(5), 056109. Network resilience analysis provides crucial insights for understanding and improving the robustness of complex systems, with important applications in materials science and engineering.\n","permalink":"https://Linlin-resh.github.io/posts/reading-notes-newman-ch16/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eChapter 16 of Newman\u0026rsquo;s \u003cem\u003eNetworks: An Introduction\u003c/em\u003e explores \u003cstrong\u003enetwork resilience\u003c/strong\u003e - the ability of networks to maintain their functionality under stress, attacks, or failures. This chapter covers robustness measures, failure analysis, and strategies for enhancing network resilience.\u003c/p\u003e\n\u003ch2 id=\"161-robustness-measures\"\u003e16.1 Robustness Measures\u003c/h2\u003e\n\u003ch3 id=\"connectivity-robustness\"\u003eConnectivity Robustness\u003c/h3\u003e\n\u003ch4 id=\"robustness-function\"\u003eRobustness Function\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eRobustness function\u003c/strong\u003e:\n$$R(p) = \\frac{1}{n} \\sum_{i=1}^n \\frac{S_i(p)}{n}$$\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$p$: Fraction of nodes removed\u003c/li\u003e\n\u003cli\u003e$S_i(p)$: Size of largest component after removing $i$ nodes\u003c/li\u003e\n\u003cli\u003e$n$: Total number of nodes\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eProperties\u003c/strong\u003e:\u003c/p\u003e","title":"Reading Notes: Newman's Networks Chapter 16 - Network Resilience"},{"content":"Introduction Chapter 17 of Newman\u0026rsquo;s Networks: An Introduction explores network optimization - the process of designing and modifying networks to achieve desired properties or performance. This chapter covers optimization problems, algorithms, and applications to real-world network design.\n17.1 Network Design Problems Basic Optimization Problems Minimum Spanning Tree Problem: Find minimum weight spanning tree\nMathematical formulation: $$\\min_{T} \\sum_{(i,j) \\in T} w_{ij}$$\nConstraints:\n$T$ is a spanning tree $w_{ij}$ is the weight of edge $(i,j)$ Algorithms:\nKruskal\u0026rsquo;s algorithm: $O(m \\log m)$ Prim\u0026rsquo;s algorithm: $O(m + n \\log n)$ Shortest Path Problem Problem: Find shortest path between two nodes\nMathematical formulation: $$\\min_{P} \\sum_{(i,j) \\in P} w_{ij}$$\nConstraints:\n$P$ is a path from source to destination $w_{ij}$ is the weight of edge $(i,j)$ Algorithms:\nDijkstra\u0026rsquo;s algorithm: $O(m + n \\log n)$ Bellman-Ford algorithm: $O(mn)$ Floyd-Warshall algorithm: $O(n^3)$ Maximum Flow Problem Problem: Find maximum flow from source to sink\nMathematical formulation: $$\\max_{f} \\sum_{j} f_{sj}$$\nConstraints:\nFlow conservation: $\\sum_{j} f_{ij} - \\sum_{j} f_{ji} = 0$ for $i \\neq s,t$ Capacity constraints: $0 \\leq f_{ij} \\leq c_{ij}$ Non-negativity: $f_{ij} \\geq 0$ Algorithms:\nFord-Fulkerson algorithm: $O(m \\cdot f^*)$ Edmonds-Karp algorithm: $O(m^2 n)$ Push-relabel algorithm: $O(n^2 m)$ Network Design Problems Steiner Tree Problem Problem: Find minimum weight tree connecting all terminals\nMathematical formulation: $$\\min_{T} \\sum_{(i,j) \\in T} w_{ij}$$\nConstraints:\n$T$ contains all terminal nodes $T$ is a tree Complexity: NP-hard\nApproximation algorithms:\n2-approximation: $O(n^2)$ 1.55-approximation: $O(n^2 \\log n)$ Traveling Salesman Problem Problem: Find minimum weight Hamiltonian cycle\nMathematical formulation: $$\\min_{C} \\sum_{(i,j) \\in C} w_{ij}$$\nConstraints:\n$C$ is a Hamiltonian cycle $w_{ij}$ is the weight of edge $(i,j)$ Complexity: NP-hard\nApproximation algorithms:\n2-approximation: $O(n^2)$ 1.5-approximation: $O(n^3)$ Facility Location Problem Problem: Find optimal locations for facilities\nMathematical formulation: $$\\min_{x,y} \\sum_{i} c_i x_i + \\sum_{i,j} d_{ij} y_{ij}$$\nConstraints:\nDemand satisfaction: $\\sum_{i} y_{ij} = d_j$ for all $j$ Capacity constraints: $\\sum_{j} y_{ij} \\leq c_i x_i$ for all $i$ Binary variables: $x_i \\in {0,1}$ Complexity: NP-hard\nApproximation algorithms:\n3-approximation: $O(n^2)$ 1.61-approximation: $O(n^2 \\log n)$ 17.2 Optimization Algorithms Exact Algorithms Branch and Bound Algorithm:\nBranch: Split problem into subproblems Bound: Calculate lower bound for each subproblem Prune: Eliminate subproblems with bounds worse than current best Repeat: Until all subproblems are solved or pruned Complexity: Exponential in worst case\nAdvantages: Guarantees optimal solution Disadvantages: May be slow for large problems\nDynamic Programming Algorithm:\nSubproblems: Break problem into smaller subproblems Memoization: Store solutions to subproblems Recursion: Solve subproblems recursively Reconstruction: Build optimal solution from subproblem solutions Complexity: $O(n^2)$ to $O(n^3)$ for many problems\nAdvantages: Efficient for problems with optimal substructure Disadvantages: May require exponential space\nApproximation Algorithms Greedy Algorithms Algorithm:\nInitialize: Start with empty solution Iterate: At each step, make locally optimal choice Terminate: When solution is complete Examples:\nKruskal\u0026rsquo;s algorithm: For minimum spanning tree Prim\u0026rsquo;s algorithm: For minimum spanning tree Dijkstra\u0026rsquo;s algorithm: For shortest path Advantages: Simple, fast Disadvantages: May not find optimal solution\nLocal Search Algorithm:\nInitialize: Start with feasible solution Neighborhood: Define neighborhood of current solution Improve: Move to better solution in neighborhood Terminate: When no improvement possible Examples:\n2-opt: For traveling salesman problem 3-opt: For traveling salesman problem Simulated annealing: For general optimization Advantages: Can escape local optima Disadvantages: May not find global optimum\nMetaheuristic Algorithms Genetic Algorithm Algorithm:\nInitialize: Create population of solutions Evaluate: Calculate fitness of each solution Select: Select parents for reproduction Crossover: Create offspring from parents Mutate: Apply random changes to offspring Replace: Replace population with offspring Repeat: Until convergence Parameters:\nPopulation size: Number of solutions Crossover rate: Probability of crossover Mutation rate: Probability of mutation Selection pressure: Strength of selection Advantages: Can handle complex problems Disadvantages: May be slow to converge\nSimulated Annealing Algorithm:\nInitialize: Start with random solution Neighborhood: Define neighborhood of current solution Accept: Accept better solutions, probabilistically accept worse solutions Cool: Reduce temperature over time Terminate: When temperature is low enough Parameters:\nInitial temperature: $T_0$ Cooling rate: $\\alpha$ Acceptance probability: $P = e^{-\\Delta E/T}$ Advantages: Can escape local optima Disadvantages: May be slow to converge\nParticle Swarm Optimization Algorithm:\nInitialize: Create swarm of particles Evaluate: Calculate fitness of each particle Update: Update velocity and position of each particle Repeat: Until convergence Parameters:\nSwarm size: Number of particles Inertia weight: $w$ Acceleration coefficients: $c_1, c_2$ Advantages: Simple, effective Disadvantages: May get stuck in local optima\n17.3 Network Design Principles Connectivity Minimum Connectivity Principle: Ensure network remains connected\nMathematical formulation: $$\\min_{G} \\sum_{(i,j) \\in E} w_{ij}$$\nConstraints:\n$G$ is connected $w_{ij}$ is the weight of edge $(i,j)$ Algorithms:\nMinimum spanning tree: $O(m \\log m)$ Steiner tree: NP-hard Redundancy Principle: Provide multiple paths between nodes\nMathematical formulation: $$\\max_{G} \\sum_{i \\neq j} \\text{paths}(i,j)$$\nConstraints:\n$G$ is connected $\\text{paths}(i,j)$ is the number of paths between $i$ and $j$ Algorithms:\nMaximum flow: $O(m^2 n)$ Minimum cut: $O(m^2 n)$ Efficiency Shortest Paths Principle: Minimize average path length\nMathematical formulation: $$\\min_{G} \\frac{1}{n(n-1)} \\sum_{i \\neq j} d_{ij}$$\nConstraints:\n$G$ is connected $d_{ij}$ is the shortest path length between $i$ and $j$ Algorithms:\nAll-pairs shortest paths: $O(n^3)$ Single-source shortest paths: $O(m + n \\log n)$ Load Balancing Principle: Distribute load evenly across network\nMathematical formulation: $$\\min_{G} \\max_{i} \\sum_{j} f_{ij}$$\nConstraints:\nFlow conservation: $\\sum_{j} f_{ij} - \\sum_{j} f_{ji} = 0$ for all $i$ Capacity constraints: $0 \\leq f_{ij} \\leq c_{ij}$ Algorithms:\nMaximum flow: $O(m^2 n)$ Minimum cost flow: $O(m^2 n \\log n)$ Robustness Fault Tolerance Principle: Network remains functional under failures\nMathematical formulation: $$\\max_{G} \\min_{F} \\text{connectivity}(G \\setminus F)$$\nConstraints:\n$F$ is a set of failed edges/nodes $\\text{connectivity}(G \\setminus F)$ is the connectivity of remaining network Algorithms:\nMinimum cut: $O(m^2 n)$ Maximum flow: $O(m^2 n)$ Resilience Principle: Network recovers quickly from failures\nMathematical formulation: $$\\min_{G} \\max_{F} \\text{recovery_time}(G, F)$$\nConstraints:\n$F$ is a set of failed edges/nodes $\\text{recovery_time}(G, F)$ is the time to recover from failure Algorithms:\nDynamic programming: $O(n^2)$ Simulation: $O(n^3)$ 17.4 Applications to Materials Science Nanowire Network Design Problem Given: Set of nanowire junctions and constraints Goal: Design optimal nanowire network\nMathematical formulation: $$\\min_{G} \\sum_{(i,j) \\in E} w_{ij}$$\nConstraints:\nElectrical: Ohm\u0026rsquo;s law must be satisfied Topological: Network must be connected Physical: Edge weights must be positive Solution Algorithm:\nInitialize: Start with empty network Add edges: Add edges that improve objective Check constraints: Ensure all constraints are satisfied Optimize: Use local search to improve solution Complexity: NP-hard\nApproximation: 2-approximation algorithm\nDefect Network Optimization Problem Given: Defect network with dynamics Goal: Optimize defect distribution\nMathematical formulation: $$\\min_{G} \\sum_{i} \\sigma_i k_i$$\nConstraints:\nDefect concentration: $c \\leq c_{\\max}$ Clustering: $C \\geq C_{\\min}$ Solution Algorithm:\nInitialize: Start with random defect distribution Evaluate: Calculate objective function Optimize: Use genetic algorithm to improve solution Validate: Check constraints Complexity: NP-hard\nApproximation: Genetic algorithm\nPhase Transition Optimization Problem Given: Phase transition dynamics Goal: Optimize phase transition properties\nMathematical formulation: $$\\min_{G} \\sum_{i} |T_i - T_{\\text{target}}|$$\nConstraints:\nTemperature: $T_{\\min} \\leq T_i \\leq T_{\\max}$ Pressure: $P_{\\min} \\leq P_i \\leq P_{\\max}$ Solution Algorithm:\nInitialize: Start with random phase distribution Simulate: Simulate phase transition dynamics Optimize: Use simulated annealing to improve solution Validate: Check constraints Complexity: NP-hard\nApproximation: Simulated annealing\n17.5 Multi-Objective Optimization Pareto Optimality Definition Pareto optimal solution: Solution that cannot be improved in one objective without worsening another\nMathematical formulation: $$\\min_{x} f(x) = (f_1(x), f_2(x), \\ldots, f_k(x))$$\nConstraints:\n$g_i(x) \\leq 0$ for all $i$ $h_j(x) = 0$ for all $j$ Pareto Front Pareto front: Set of all Pareto optimal solutions\nProperties:\nNon-dominated: No solution dominates another Complete: Contains all Pareto optimal solutions Minimal: No redundant solutions Multi-Objective Algorithms Weighted Sum Method Algorithm:\nWeights: Assign weights to objectives Combine: Combine objectives into single objective Optimize: Solve single-objective problem Repeat: For different weight combinations Mathematical formulation: $$\\min_{x} \\sum_{i=1}^k w_i f_i(x)$$\nAdvantages: Simple, efficient Disadvantages: May miss some Pareto optimal solutions\nε-Constraint Method Algorithm:\nSelect: Select one objective as primary Constrain: Convert other objectives to constraints Optimize: Solve constrained optimization problem Repeat: For different constraint values Mathematical formulation: $$\\min_{x} f_1(x)$$\nConstraints:\n$f_i(x) \\leq \\epsilon_i$ for $i = 2, \\ldots, k$ $g_j(x) \\leq 0$ for all $j$ $h_l(x) = 0$ for all $l$ Advantages: Can find all Pareto optimal solutions Disadvantages: May be computationally expensive\nNSGA-II Algorithm:\nInitialize: Create population of solutions Evaluate: Calculate fitness of each solution Select: Select parents using non-dominated sorting Crossover: Create offspring from parents Mutate: Apply random changes to offspring Replace: Replace population with offspring Repeat: Until convergence Advantages: Can handle complex problems Disadvantages: May be slow to converge\nCode Example: Network Optimization import networkx as nx import numpy as np import matplotlib.pyplot as plt from scipy.optimize import minimize from sklearn.cluster import KMeans import random class NetworkOptimizer: \u0026#34;\u0026#34;\u0026#34;Network optimization class\u0026#34;\u0026#34;\u0026#34; def __init__(self, G): self.G = G.copy() self.n = G.number_of_nodes() self.m = G.number_of_edges() def optimize_connectivity(self, budget): \u0026#34;\u0026#34;\u0026#34;Optimize network connectivity with budget constraint\u0026#34;\u0026#34;\u0026#34; def objective(edges): # Create network with selected edges G_new = nx.Graph() G_new.add_nodes_from(self.G.nodes()) for i, (u, v) in enumerate(self.G.edges()): if edges[i] \u0026gt; 0.5: # Binary decision G_new.add_edge(u, v) # Calculate connectivity (inverse of average path length) if nx.is_connected(G_new): avg_path_length = nx.average_shortest_path_length(G_new) return -1 / avg_path_length # Maximize connectivity else: return -1000 # Penalty for disconnected network def constraint(edges): # Budget constraint cost = np.sum(edges) return budget - cost # Initial guess x0 = np.random.random(self.m) # Optimize result = minimize(objective, x0, method=\u0026#39;SLSQP\u0026#39;, constraints={\u0026#39;type\u0026#39;: \u0026#39;ineq\u0026#39;, \u0026#39;fun\u0026#39;: constraint}, bounds=[(0, 1)] * self.m) # Create optimized network G_optimized = nx.Graph() G_optimized.add_nodes_from(self.G.nodes()) for i, (u, v) in enumerate(self.G.edges()): if result.x[i] \u0026gt; 0.5: G_optimized.add_edge(u, v) return G_optimized, result.fun def optimize_efficiency(self, budget): \u0026#34;\u0026#34;\u0026#34;Optimize network efficiency with budget constraint\u0026#34;\u0026#34;\u0026#34; def objective(edges): # Create network with selected edges G_new = nx.Graph() G_new.add_nodes_from(self.G.nodes()) for i, (u, v) in enumerate(self.G.edges()): if edges[i] \u0026gt; 0.5: G_new.add_edge(u, v) # Calculate efficiency if nx.is_connected(G_new): efficiency = nx.global_efficiency(G_new) return -efficiency # Maximize efficiency else: return -1000 # Penalty for disconnected network def constraint(edges): # Budget constraint cost = np.sum(edges) return budget - cost # Initial guess x0 = np.random.random(self.m) # Optimize result = minimize(objective, x0, method=\u0026#39;SLSQP\u0026#39;, constraints={\u0026#39;type\u0026#39;: \u0026#39;ineq\u0026#39;, \u0026#39;fun\u0026#39;: constraint}, bounds=[(0, 1)] * self.m) # Create optimized network G_optimized = nx.Graph() G_optimized.add_nodes_from(self.G.nodes()) for i, (u, v) in enumerate(self.G.edges()): if result.x[i] \u0026gt; 0.5: G_optimized.add_edge(u, v) return G_optimized, result.fun def optimize_robustness(self, budget): \u0026#34;\u0026#34;\u0026#34;Optimize network robustness with budget constraint\u0026#34;\u0026#34;\u0026#34; def objective(edges): # Create network with selected edges G_new = nx.Graph() G_new.add_nodes_from(self.G.nodes()) for i, (u, v) in enumerate(self.G.edges()): if edges[i] \u0026gt; 0.5: G_new.add_edge(u, v) # Calculate robustness (algebraic connectivity) if nx.is_connected(G_new): L = nx.laplacian_matrix(G_new).toarray() eigenvals = np.linalg.eigvals(L) eigenvals = np.real(eigenvals) eigenvals = np.sort(eigenvals) algebraic_connectivity = eigenvals[1] if len(eigenvals) \u0026gt; 1 else 0 return -algebraic_connectivity # Maximize robustness else: return -1000 # Penalty for disconnected network def constraint(edges): # Budget constraint cost = np.sum(edges) return budget - cost # Initial guess x0 = np.random.random(self.m) # Optimize result = minimize(objective, x0, method=\u0026#39;SLSQP\u0026#39;, constraints={\u0026#39;type\u0026#39;: \u0026#39;ineq\u0026#39;, \u0026#39;fun\u0026#39;: constraint}, bounds=[(0, 1)] * self.m) # Create optimized network G_optimized = nx.Graph() G_optimized.add_nodes_from(self.G.nodes()) for i, (u, v) in enumerate(self.G.edges()): if result.x[i] \u0026gt; 0.5: G_optimized.add_edge(u, v) return G_optimized, result.fun def multi_objective_optimization(self, budget, weights): \u0026#34;\u0026#34;\u0026#34;Multi-objective optimization\u0026#34;\u0026#34;\u0026#34; def objective(edges): # Create network with selected edges G_new = nx.Graph() G_new.add_nodes_from(self.G.nodes()) for i, (u, v) in enumerate(self.G.edges()): if edges[i] \u0026gt; 0.5: G_new.add_edge(u, v) if not nx.is_connected(G_new): return 1000 # Penalty for disconnected network # Calculate multiple objectives avg_path_length = nx.average_shortest_path_length(G_new) efficiency = nx.global_efficiency(G_new) L = nx.laplacian_matrix(G_new).toarray() eigenvals = np.linalg.eigvals(L) eigenvals = np.real(eigenvals) eigenvals = np.sort(eigenvals) algebraic_connectivity = eigenvals[1] if len(eigenvals) \u0026gt; 1 else 0 # Weighted sum total_objective = (weights[0] * avg_path_length + weights[1] * (1 - efficiency) + weights[2] * (1 - algebraic_connectivity)) return total_objective def constraint(edges): # Budget constraint cost = np.sum(edges) return budget - cost # Initial guess x0 = np.random.random(self.m) # Optimize result = minimize(objective, x0, method=\u0026#39;SLSQP\u0026#39;, constraints={\u0026#39;type\u0026#39;: \u0026#39;ineq\u0026#39;, \u0026#39;fun\u0026#39;: constraint}, bounds=[(0, 1)] * self.m) # Create optimized network G_optimized = nx.Graph() G_optimized.add_nodes_from(self.G.nodes()) for i, (u, v) in enumerate(self.G.edges()): if result.x[i] \u0026gt; 0.5: G_optimized.add_edge(u, v) return G_optimized, result.fun def genetic_algorithm_optimization(G, budget, generations=100, population_size=50): \u0026#34;\u0026#34;\u0026#34;Genetic algorithm for network optimization\u0026#34;\u0026#34;\u0026#34; def fitness(individual): # Create network with selected edges G_new = nx.Graph() G_new.add_nodes_from(G.nodes()) for i, (u, v) in enumerate(G.edges()): if individual[i] \u0026gt; 0.5: G_new.add_edge(u, v) if not nx.is_connected(G_new): return 0 # Penalty for disconnected network # Calculate fitness (efficiency) efficiency = nx.global_efficiency(G_new) return efficiency def crossover(parent1, parent2): # Single-point crossover point = random.randint(1, len(parent1) - 1) child1 = parent1[:point] + parent2[point:] child2 = parent2[:point] + parent1[point:] return child1, child2 def mutate(individual, mutation_rate=0.1): # Random mutation for i in range(len(individual)): if random.random() \u0026lt; mutation_rate: individual[i] = 1 - individual[i] # Flip bit return individual # Initialize population population = [] for _ in range(population_size): individual = np.random.random(G.number_of_edges()) # Ensure budget constraint while np.sum(individual) \u0026gt; budget: individual[np.argmax(individual)] = 0 population.append(individual) # Evolution for generation in range(generations): # Evaluate fitness fitness_scores = [fitness(individual) for individual in population] # Selection (tournament selection) new_population = [] for _ in range(population_size): # Tournament selection tournament_size = 5 tournament = random.sample(range(population_size), tournament_size) winner = max(tournament, key=lambda i: fitness_scores[i]) new_population.append(population[winner].copy()) # Crossover for i in range(0, population_size, 2): if i + 1 \u0026lt; population_size: child1, child2 = crossover(new_population[i], new_population[i + 1]) new_population[i] = child1 new_population[i + 1] = child2 # Mutation for individual in new_population: mutate(individual) # Ensure budget constraint while np.sum(individual) \u0026gt; budget: individual[np.argmax(individual)] = 0 population = new_population # Find best individual fitness_scores = [fitness(individual) for individual in population] best_individual = population[np.argmax(fitness_scores)] # Create optimized network G_optimized = nx.Graph() G_optimized.add_nodes_from(G.nodes()) for i, (u, v) in enumerate(G.edges()): if best_individual[i] \u0026gt; 0.5: G_optimized.add_edge(u, v) return G_optimized, max(fitness_scores) def plot_optimization_results(G_original, G_optimized, title=\u0026#34;Network Optimization\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Plot network optimization results\u0026#34;\u0026#34;\u0026#34; fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12)) # Original network pos = nx.spring_layout(G_original, k=1, iterations=50) nx.draw(G_original, pos, ax=ax1, node_color=\u0026#39;lightblue\u0026#39;, edge_color=\u0026#39;gray\u0026#39;, alpha=0.6, node_size=50) ax1.set_title(\u0026#39;Original Network\u0026#39;) ax1.axis(\u0026#39;off\u0026#39;) # Optimized network nx.draw(G_optimized, pos, ax=ax2, node_color=\u0026#39;lightgreen\u0026#39;, edge_color=\u0026#39;gray\u0026#39;, alpha=0.6, node_size=50) ax2.set_title(\u0026#39;Optimized Network\u0026#39;) ax2.axis(\u0026#39;off\u0026#39;) # Network properties comparison properties = { \u0026#39;Nodes\u0026#39;: [G_original.number_of_nodes(), G_optimized.number_of_nodes()], \u0026#39;Edges\u0026#39;: [G_original.number_of_edges(), G_optimized.number_of_edges()], \u0026#39;Density\u0026#39;: [nx.density(G_original), nx.density(G_optimized)], \u0026#39;Clustering\u0026#39;: [nx.average_clustering(G_original), nx.average_clustering(G_optimized)], \u0026#39;Efficiency\u0026#39;: [nx.global_efficiency(G_original), nx.global_efficiency(G_optimized)] } x = np.arange(len(properties)) width = 0.35 for i, (prop, values) in enumerate(properties.items()): ax3.bar(x[i] - width/2, values[0], width, label=\u0026#39;Original\u0026#39;, alpha=0.7) ax3.bar(x[i] + width/2, values[1], width, label=\u0026#39;Optimized\u0026#39;, alpha=0.7) ax3.set_xlabel(\u0026#39;Properties\u0026#39;) ax3.set_ylabel(\u0026#39;Values\u0026#39;) ax3.set_title(\u0026#39;Network Properties Comparison\u0026#39;) ax3.set_xticks(x) ax3.set_xticklabels(properties.keys()) ax3.legend() ax3.grid(True, alpha=0.3) # Degree distribution comparison degrees_original = [G_original.degree(i) for i in G_original.nodes()] degrees_optimized = [G_optimized.degree(i) for i in G_optimized.nodes()] ax4.hist(degrees_original, bins=20, alpha=0.7, label=\u0026#39;Original\u0026#39;, edgecolor=\u0026#39;black\u0026#39;) ax4.hist(degrees_optimized, bins=20, alpha=0.7, label=\u0026#39;Optimized\u0026#39;, edgecolor=\u0026#39;black\u0026#39;) ax4.set_xlabel(\u0026#39;Degree\u0026#39;) ax4.set_ylabel(\u0026#39;Count\u0026#39;) ax4.set_title(\u0026#39;Degree Distribution Comparison\u0026#39;) ax4.legend() ax4.grid(True, alpha=0.3) plt.tight_layout() plt.show() # Example: Network optimization G = nx.barabasi_albert_graph(50, 3) # Initialize optimizer optimizer = NetworkOptimizer(G) # Single-objective optimization budget = 0.5 # 50% of edges G_connectivity, connectivity_score = optimizer.optimize_connectivity(budget) G_efficiency, efficiency_score = optimizer.optimize_efficiency(budget) G_robustness, robustness_score = optimizer.optimize_robustness(budget) # Multi-objective optimization weights = [0.4, 0.3, 0.3] # Weights for path length, efficiency, robustness G_multi, multi_score = optimizer.multi_objective_optimization(budget, weights) # Genetic algorithm optimization G_genetic, genetic_score = genetic_algorithm_optimization(G, budget, generations=50) print(\u0026#34;Network Optimization Results:\u0026#34;) print(f\u0026#34;Original network: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\u0026#34;) print(f\u0026#34;Connectivity optimization: {connectivity_score:.3f}\u0026#34;) print(f\u0026#34;Efficiency optimization: {efficiency_score:.3f}\u0026#34;) print(f\u0026#34;Robustness optimization: {robustness_score:.3f}\u0026#34;) print(f\u0026#34;Multi-objective optimization: {multi_score:.3f}\u0026#34;) print(f\u0026#34;Genetic algorithm optimization: {genetic_score:.3f}\u0026#34;) # Plot results plot_optimization_results(G, G_efficiency, \u0026#34;Efficiency Optimization\u0026#34;) Key Takeaways Optimization problems: Various types of network optimization problems Algorithms: Exact, approximation, and metaheuristic algorithms Design principles: Connectivity, efficiency, and robustness Multi-objective optimization: Pareto optimality and algorithms Applications: Important for materials science and engineering Mathematical formulation: Rigorous mathematical framework Practical implementation: Code examples and real-world applications References Newman, M. E. J. (2010). Networks: An Introduction. Oxford University Press. Ahuja, R. K., Magnanti, T. L., \u0026amp; Orlin, J. B. (1993). Network Flows: Theory, Algorithms, and Applications. Prentice Hall. Goldberg, D. E. (1989). Genetic Algorithms in Search, Optimization, and Machine Learning. Addison-Wesley. Kirkpatrick, S., Gelatt, C. D., \u0026amp; Vecchi, M. P. (1983). Optimization by simulated annealing. Science, 220(4598), 671-680. Network optimization provides powerful tools for designing and improving complex networks, with important applications in materials science and engineering.\n","permalink":"https://Linlin-resh.github.io/posts/reading-notes-newman-ch17/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eChapter 17 of Newman\u0026rsquo;s \u003cem\u003eNetworks: An Introduction\u003c/em\u003e explores \u003cstrong\u003enetwork optimization\u003c/strong\u003e - the process of designing and modifying networks to achieve desired properties or performance. This chapter covers optimization problems, algorithms, and applications to real-world network design.\u003c/p\u003e\n\u003ch2 id=\"171-network-design-problems\"\u003e17.1 Network Design Problems\u003c/h2\u003e\n\u003ch3 id=\"basic-optimization-problems\"\u003eBasic Optimization Problems\u003c/h3\u003e\n\u003ch4 id=\"minimum-spanning-tree\"\u003eMinimum Spanning Tree\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eProblem\u003c/strong\u003e: Find minimum weight spanning tree\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMathematical formulation\u003c/strong\u003e:\n$$\\min_{T} \\sum_{(i,j) \\in T} w_{ij}$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eConstraints\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$T$ is a spanning tree\u003c/li\u003e\n\u003cli\u003e$w_{ij}$ is the weight of edge $(i,j)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eAlgorithms\u003c/strong\u003e:\u003c/p\u003e","title":"Reading Notes: Newman's Networks Chapter 17 - Network Optimization"},{"content":"Introduction Chapter 18 of Newman\u0026rsquo;s Networks: An Introduction explores future directions in network science - the emerging trends, open problems, and research frontiers that will shape the field in the coming years. This chapter covers machine learning, quantum networks, and other cutting-edge developments.\n18.1 Machine Learning on Networks Graph Neural Networks Graph Convolutional Networks (GCNs) Layer update: $$H^{(l+1)} = \\sigma(\\tilde{A} H^{(l)} W^{(l)})$$\nWhere:\n$\\tilde{A} = D^{-1/2} A D^{-1/2}$: Normalized adjacency matrix $H^{(l)}$: Node features at layer $l$ $W^{(l)}$: Weight matrix at layer $l$ $\\sigma$: Activation function Applications:\nNode classification: Predict node labels Link prediction: Predict missing edges Graph classification: Classify entire graphs Graph Attention Networks (GATs) Attention mechanism: $$\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}i} \\exp(e{ik})}$$\nWhere:\n$e_{ij} = \\text{LeakyReLU}(a^T [W h_i || W h_j])$ $a$: Attention vector $W$: Weight matrix $h_i$: Node features Node update: $$h_i\u0026rsquo; = \\sigma\\left(\\sum_{j \\in \\mathcal{N}i} \\alpha{ij} W h_j\\right)$$\nAdvantages:\nAdaptive: Attention weights adapt to different nodes Interpretable: Attention weights show importance Flexible: Can handle different graph structures Graph Transformer Networks Self-attention mechanism: $$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\nWhere:\n$Q, K, V$: Query, key, value matrices $d_k$: Dimension of key vectors Multi-head attention: $$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W^O$$\nWhere:\n$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$ $W_i^Q, W_i^K, W_i^V$: Weight matrices for head $i$ Deep Learning Applications Network Embedding Node2Vec: $$\\max_f \\sum_{u \\in V} \\log P(N_S(u)|f(u))$$\nWhere:\n$f$: Embedding function $N_S(u)$: Neighborhood of node $u$ $S$: Sampling strategy Graph2Vec: $$\\max_f \\sum_{G \\in \\mathcal{G}} \\log P(\\text{context}(G)|f(G))$$\nWhere:\n$f$: Graph embedding function $\\text{context}(G)$: Context of graph $G$ Network Generation Variational Autoencoders (VAEs): $$\\mathcal{L} = \\mathbb{E}{q\\phi(z|G)}[\\log p_\\theta(G|z)] - D_{KL}(q_\\phi(z|G) || p(z))$$\nWhere:\n$q_\\phi(z|G)$: Encoder $p_\\theta(G|z)$: Decoder $D_{KL}$: Kullback-Leibler divergence Generative Adversarial Networks (GANs): $$\\min_G \\max_D V(D, G) = \\mathbb{E}{x \\sim p{data}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))]$$\nWhere:\n$G$: Generator $D$: Discriminator $p_{data}$: Data distribution $p_z$: Noise distribution 18.2 Quantum Networks Quantum Graph Theory Quantum States Quantum state: $$|\\psi\\rangle = \\sum_{i} \\alpha_i |i\\rangle$$\nWhere:\n$\\alpha_i$: Complex amplitudes $|i\\rangle$: Basis states $\\sum_i |\\alpha_i|^2 = 1$: Normalization Density matrix: $$\\rho = \\sum_i p_i |\\psi_i\\rangle\\langle\\psi_i|$$\nWhere $p_i$ are probabilities.\nQuantum Entanglement Entangled state: $$|\\psi\\rangle = \\frac{1}{\\sqrt{2}}(|00\\rangle + |11\\rangle)$$\nProperties:\nNon-local: Cannot be written as product state Correlated: Measurements are correlated Useful: For quantum communication and computation Quantum Network Models Quantum Random Walks Quantum walk: $$|\\psi(t)\\rangle = U^t |\\psi(0)\\rangle$$\nWhere:\n$U$: Unitary evolution operator $|\\psi(0)\\rangle$: Initial state $|\\psi(t)\\rangle$: State at time $t$ Properties:\nUnitary: Preserves probability Reversible: Can go backwards in time Fast: Can be faster than classical walks Quantum Percolation Quantum percolation: $$P(\\text{percolation}) = 1 - \\sum_{k=0}^{\\infty} P(k) (1-p)^k$$\nWhere:\n$P(k)$: Quantum degree distribution $p$: Quantum edge probability Critical threshold: $$p_c = \\frac{1}{\\kappa - 1}$$\nWhere $\\kappa = \\frac{\\langle k^2 \\rangle}{\\langle k \\rangle}$.\nQuantum Applications Quantum Communication Quantum teleportation:\nEntangle: Create entangled pair Measure: Measure qubit and entangled pair Transmit: Send measurement results Reconstruct: Reconstruct original qubit Quantum key distribution:\nGenerate: Generate random key Encode: Encode key in quantum states Transmit: Send quantum states Decode: Decode key from quantum states Quantum Computing Quantum algorithms:\nShor\u0026rsquo;s algorithm: Factor integers Grover\u0026rsquo;s algorithm: Search databases Quantum simulation: Simulate quantum systems Quantum error correction:\nStabilizer codes: Detect and correct errors Surface codes: Topological error correction Concatenated codes: Hierarchical error correction 18.3 Temporal and Dynamic Networks Temporal Network Analysis Time-Varying Networks Temporal network: $$G(t) = (V, E(t)) \\quad \\text{for } t \\in [0, T]$$\nProperties:\nNodes: Usually constant Edges: Vary with time Dynamics: Can be continuous or discrete Temporal Measures Temporal degree: $$k_i(t) = \\sum_{j} A_{ij}(t)$$\nTemporal clustering: $$C_i(t) = \\frac{2e_i(t)}{k_i(t)(k_i(t)-1)}$$\nTemporal path length: $$d_{ij}^T = \\min{\\text{length of temporal path from } i \\text{ to } j}$$\nDynamic Network Models Preferential Attachment with Aging Model: $$\\Pi(k_i, t, t_i) = \\frac{k_i(t) e^{-\\alpha(t-t_i)}}{\\sum_j k_j(t) e^{-\\alpha(t-t_j)}}$$\nWhere:\n$k_i(t)$: Degree of node $i$ at time $t$ $t_i$: Time when node $i$ was added $\\alpha$: Aging parameter Degree distribution: $$P(k) \\sim k^{-\\gamma} e^{-\\beta k}$$\nWhere $\\gamma$ and $\\beta$ depend on $\\alpha$.\nFitness Models Model: $$\\Pi(k_i, \\eta_i) = \\frac{\\eta_i k_i(t)}{\\sum_j \\eta_j k_j(t)}$$\nWhere $\\eta_i$ is the fitness of node $i$.\nDegree distribution: $$P(k) \\sim k^{-\\gamma} \\int \\eta^{\\gamma-1} \\rho(\\eta) , d\\eta$$\nWhere $\\rho(\\eta)$ is the fitness distribution.\n18.4 Multilayer and Multiplex Networks Multilayer Network Analysis Supra-adjacency Matrix Supra-adjacency matrix: $$A = \\bigoplus_{\\alpha} A^{\\alpha} + \\bigoplus_{\\alpha \\neq \\beta} C^{\\alpha \\beta}$$\nWhere:\n$A^{\\alpha}$: Adjacency matrix of layer $\\alpha$ $C^{\\alpha \\beta}$: Inter-layer coupling matrix Multilayer Measures Multilayer degree: $$k_i = \\sum_{\\alpha} k_i^{\\alpha}$$\nParticipation coefficient: $$P_i = 1 - \\sum_{\\alpha} \\left(\\frac{k_i^{\\alpha}}{k_i}\\right)^2$$\nMultilayer clustering: $$C_i = \\frac{\\sum_{\\alpha} C_i^{\\alpha}}{L}$$\nMultiplex Networks Multiplex Structure Multiplex network: Each layer represents different type of relationship\nMathematical representation: $$A_{ij}^{\\alpha \\beta} = \\begin{cases} A_{ij}^{\\alpha} \u0026amp; \\text{if } \\alpha = \\beta \\ 0 \u0026amp; \\text{if } \\alpha \\neq \\beta \\end{cases}$$\nMultiplex Measures Overlapping degree: $$o_i = \\sum_{\\alpha} \\mathbb{I}(k_i^{\\alpha} \u0026gt; 0)$$\nMultiplex clustering: $$C_i = \\frac{\\sum_{\\alpha} C_i^{\\alpha}}{L}$$\nMultiplex PageRank: $$PR_i^{\\alpha} = (1-d) \\frac{1}{nL} + d \\sum_{j, \\beta} \\frac{A_{ij}^{\\alpha \\beta} PR_j^{\\beta}}{k_j^{\\beta}}$$\n18.5 Applications to Materials Science AI-Driven Materials Design Network-Based Property Prediction Property prediction: $$P = f(\\text{network features}) + \\epsilon$$\nWhere:\n$P$: Material property $f$: Machine learning function $\\text{network features}$: Network descriptors $\\epsilon$: Error term Network features:\nTopological: Degree, clustering, path length Spectral: Eigenvalues, eigenvectors Dynamic: Synchronization, percolation Materials Discovery High-throughput screening:\nGenerate: Generate large number of candidate materials Predict: Predict properties using ML models Filter: Filter promising candidates Validate: Validate predictions experimentally Inverse design:\nSpecify: Specify desired properties Optimize: Optimize network structure Generate: Generate material structure Validate: Validate experimentally Quantum Materials Quantum Network Models Superconductors:\nNodes: Cooper pairs Edges: Josephson junctions Properties: Zero resistance, Meissner effect Topological insulators:\nNodes: Electronic states Edges: Hopping integrals Properties: Topological protection, edge states Quantum dots:\nNodes: Quantum states Edges: Tunneling Properties: Quantized energy levels Quantum Applications Quantum sensors:\nSensitivity: Higher than classical sensors Precision: Better than classical sensors Applications: Magnetic field sensing, temperature sensing Quantum computers:\nSpeed: Exponential speedup for certain problems Applications: Cryptography, optimization, simulation Challenges: Error correction, scalability 18.6 Open Problems and Challenges Theoretical Challenges Network Dynamics Open problems:\nSynchronization: General conditions for synchronization Percolation: Critical behavior in complex networks Cascades: Prediction and control of cascades Evolution: Understanding network evolution Mathematical challenges:\nNonlinear dynamics: Complex nonlinear systems Stochastic processes: Random network evolution Phase transitions: Critical phenomena Stability: Network stability analysis Network Inference Open problems:\nMissing data: Inference from partial observations Noisy data: Inference from noisy observations Dynamic networks: Inference from temporal data Multilayer networks: Inference from multilayer data Mathematical challenges:\nStatistical inference: Bayesian methods Machine learning: Deep learning approaches Optimization: Non-convex optimization Validation: Model validation and selection Practical Challenges Scalability Computational challenges:\nLarge networks: Networks with millions of nodes Real-time analysis: Analysis in real-time Memory requirements: Efficient memory usage Parallel computing: Distributed algorithms Solutions:\nApproximation algorithms: Fast approximate methods Sampling: Network sampling techniques Distributed computing: Parallel algorithms Cloud computing: Scalable infrastructure Data Quality Data challenges:\nIncomplete data: Missing nodes and edges Noisy data: Measurement errors Bias: Sampling bias Privacy: Privacy concerns Solutions:\nData cleaning: Preprocessing techniques Imputation: Missing data imputation Validation: Data validation methods Privacy: Privacy-preserving methods Code Example: Future Directions import networkx as nx import numpy as np import matplotlib.pyplot as plt from sklearn.neural_network import MLPRegressor from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error, r2_score import torch import torch.nn as nn import torch.optim as optim class GraphNeuralNetwork(nn.Module): \u0026#34;\u0026#34;\u0026#34;Simple Graph Neural Network implementation\u0026#34;\u0026#34;\u0026#34; def __init__(self, input_dim, hidden_dim, output_dim): super(GraphNeuralNetwork, self).__init__() self.input_dim = input_dim self.hidden_dim = hidden_dim self.output_dim = output_dim # Graph convolution layers self.conv1 = nn.Linear(input_dim, hidden_dim) self.conv2 = nn.Linear(hidden_dim, hidden_dim) self.conv3 = nn.Linear(hidden_dim, output_dim) # Activation functions self.relu = nn.ReLU() self.dropout = nn.Dropout(0.5) def forward(self, x, adj): \u0026#34;\u0026#34;\u0026#34;Forward pass\u0026#34;\u0026#34;\u0026#34; # First convolution x = self.conv1(x) x = self.relu(x) x = self.dropout(x) # Graph convolution x = torch.matmul(adj, x) # Second convolution x = self.conv2(x) x = self.relu(x) x = self.dropout(x) # Graph convolution x = torch.matmul(adj, x) # Output layer x = self.conv3(x) return x def generate_network_features(G): \u0026#34;\u0026#34;\u0026#34;Generate network features for machine learning\u0026#34;\u0026#34;\u0026#34; n = G.number_of_nodes() features = np.zeros((n, 10)) # 10 features per node # Degree features degrees = [G.degree(i) for i in G.nodes()] features[:, 0] = degrees features[:, 1] = np.array(degrees) / np.max(degrees) if np.max(degrees) \u0026gt; 0 else 0 # Clustering features clustering = nx.clustering(G) features[:, 2] = [clustering[i] for i in G.nodes()] # Centrality features betweenness = nx.betweenness_centrality(G) closeness = nx.closeness_centrality(G) eigenvector = nx.eigenvector_centrality(G, max_iter=1000) features[:, 3] = [betweenness[i] for i in G.nodes()] features[:, 4] = [closeness[i] for i in G.nodes()] features[:, 5] = [eigenvector[i] for i in G.nodes()] # Path length features if nx.is_connected(G): path_lengths = dict(nx.all_pairs_shortest_path_length(G)) avg_path_lengths = [np.mean(list(path_lengths[i].values())) for i in G.nodes()] features[:, 6] = avg_path_lengths else: features[:, 6] = 0 # Spectral features L = nx.laplacian_matrix(G).toarray() eigenvals = np.linalg.eigvals(L) eigenvals = np.real(eigenvals) eigenvals = np.sort(eigenvals) features[:, 7] = eigenvals[1] if len(eigenvals) \u0026gt; 1 else 0 # Algebraic connectivity features[:, 8] = eigenvals[-1] if len(eigenvals) \u0026gt; 0 else 0 # Largest eigenvalue features[:, 9] = np.sum(eigenvals) # Trace return features def predict_network_properties(G, target_property=\u0026#39;efficiency\u0026#39;): \u0026#34;\u0026#34;\u0026#34;Predict network properties using machine learning\u0026#34;\u0026#34;\u0026#34; # Generate features features = generate_network_features(G) # Generate target values if target_property == \u0026#39;efficiency\u0026#39;: target = nx.global_efficiency(G) elif target_property == \u0026#39;clustering\u0026#39;: target = nx.average_clustering(G) elif target_property == \u0026#39;path_length\u0026#39;: target = nx.average_shortest_path_length(G) if nx.is_connected(G) else 0 else: target = 0 # Create training data X = features y = np.full((G.number_of_nodes(), 1), target) # Train model model = MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=1000, random_state=42) model.fit(X, y) # Predict y_pred = model.predict(X) return model, y_pred, target def simulate_quantum_walk(G, steps=100): \u0026#34;\u0026#34;\u0026#34;Simulate quantum walk on network\u0026#34;\u0026#34;\u0026#34; n = G.number_of_nodes() # Create adjacency matrix A = nx.adjacency_matrix(G).toarray() # Normalize adjacency matrix D = np.diag(np.sum(A, axis=1)) D_inv = np.linalg.inv(D) A_norm = D_inv @ A # Create unitary evolution operator U = np.eye(n) + 1j * A_norm U = U / np.linalg.norm(U) # Initial state (uniform superposition) psi = np.ones(n) / np.sqrt(n) # Simulate quantum walk psi_history = [psi.copy()] for t in range(steps): psi = U @ psi psi_history.append(psi.copy()) return psi_history def analyze_temporal_network(networks): \u0026#34;\u0026#34;\u0026#34;Analyze temporal network properties\u0026#34;\u0026#34;\u0026#34; n = len(networks) properties = { \u0026#39;density\u0026#39;: [], \u0026#39;clustering\u0026#39;: [], \u0026#39;efficiency\u0026#39;: [], \u0026#39;path_length\u0026#39;: [] } for G in networks: properties[\u0026#39;density\u0026#39;].append(nx.density(G)) properties[\u0026#39;clustering\u0026#39;].append(nx.average_clustering(G)) properties[\u0026#39;efficiency\u0026#39;].append(nx.global_efficiency(G)) if nx.is_connected(G): properties[\u0026#39;path_length\u0026#39;].append(nx.average_shortest_path_length(G)) else: properties[\u0026#39;path_length\u0026#39;].append(0) return properties def plot_future_directions(G, title=\u0026#34;Future Directions in Network Science\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Plot future directions analysis\u0026#34;\u0026#34;\u0026#34; fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12)) # Network visualization pos = nx.spring_layout(G, k=1, iterations=50) nx.draw(G, pos, ax=ax1, node_color=\u0026#39;lightblue\u0026#39;, edge_color=\u0026#39;gray\u0026#39;, alpha=0.6, node_size=50) ax1.set_title(\u0026#39;Network Structure\u0026#39;) ax1.axis(\u0026#39;off\u0026#39;) # Machine learning prediction model, y_pred, target = predict_network_properties(G, \u0026#39;efficiency\u0026#39;) ax2.scatter([target] * G.number_of_nodes(), y_pred, alpha=0.7, s=100) ax2.plot([0, 1], [0, 1], \u0026#39;r--\u0026#39;, alpha=0.5) ax2.set_xlabel(\u0026#39;Actual Efficiency\u0026#39;) ax2.set_ylabel(\u0026#39;Predicted Efficiency\u0026#39;) ax2.set_title(\u0026#39;ML Property Prediction\u0026#39;) ax2.grid(True, alpha=0.3) # Quantum walk simulation psi_history = simulate_quantum_walk(G, steps=50) probabilities = [np.abs(psi)**2 for psi in psi_history] time_steps = range(len(probabilities)) for i in range(min(5, G.number_of_nodes())): probs = [p[i] for p in probabilities] ax3.plot(time_steps, probs, alpha=0.7, label=f\u0026#39;Node {i}\u0026#39;) ax3.set_xlabel(\u0026#39;Time Steps\u0026#39;) ax3.set_ylabel(\u0026#39;Probability\u0026#39;) ax3.set_title(\u0026#39;Quantum Walk Simulation\u0026#39;) ax3.legend() ax3.grid(True, alpha=0.3) # Temporal network analysis # Generate temporal networks temporal_networks = [] for t in range(10): G_temp = G.copy() # Randomly remove some edges edges_to_remove = np.random.choice(list(G_temp.edges()), size=int(0.1 * G_temp.number_of_edges()), replace=False) G_temp.remove_edges_from(edges_to_remove) temporal_networks.append(G_temp) temporal_properties = analyze_temporal_network(temporal_networks) time_steps = range(len(temporal_networks)) ax4.plot(time_steps, temporal_properties[\u0026#39;efficiency\u0026#39;], \u0026#39;b-\u0026#39;, label=\u0026#39;Efficiency\u0026#39;, linewidth=2) ax4.plot(time_steps, temporal_properties[\u0026#39;clustering\u0026#39;], \u0026#39;r-\u0026#39;, label=\u0026#39;Clustering\u0026#39;, linewidth=2) ax4.set_xlabel(\u0026#39;Time Steps\u0026#39;) ax4.set_ylabel(\u0026#39;Property Value\u0026#39;) ax4.set_title(\u0026#39;Temporal Network Properties\u0026#39;) ax4.legend() ax4.grid(True, alpha=0.3) plt.tight_layout() plt.show() # Example: Future directions analysis G = nx.barabasi_albert_graph(50, 3) # Machine learning analysis model, y_pred, target = predict_network_properties(G, \u0026#39;efficiency\u0026#39;) print(f\u0026#34;Target efficiency: {target:.3f}\u0026#34;) print(f\u0026#34;Predicted efficiency: {np.mean(y_pred):.3f}\u0026#34;) # Quantum walk analysis psi_history = simulate_quantum_walk(G, steps=50) final_probabilities = np.abs(psi_history[-1])**2 print(f\u0026#34;Quantum walk final probabilities: {final_probabilities[:5]}\u0026#34;) # Plot results plot_future_directions(G, \u0026#34;Future Directions in Network Science\u0026#34;) Key Takeaways Machine learning: Graph neural networks and deep learning on networks Quantum networks: Quantum graph theory and quantum applications Temporal networks: Dynamic network analysis and modeling Multilayer networks: Complex network structures and analysis Materials science: AI-driven materials design and quantum materials Open problems: Theoretical and practical challenges Future research: Emerging trends and research directions References Newman, M. E. J. (2010). Networks: An Introduction. Oxford University Press. Kipf, T. N., \u0026amp; Welling, M. (2016). Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907. Veličković, P., et al. (2017). Graph attention networks. arXiv preprint arXiv:1710.10903. Nielsen, M. A., \u0026amp; Chuang, I. L. (2010). Quantum Computation and Quantum Information. Cambridge University Press. The future of network science lies in the integration of machine learning, quantum computing, and advanced mathematical techniques, with important applications in materials science and beyond.\n","permalink":"https://Linlin-resh.github.io/posts/reading-notes-newman-ch18/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eChapter 18 of Newman\u0026rsquo;s \u003cem\u003eNetworks: An Introduction\u003c/em\u003e explores \u003cstrong\u003efuture directions\u003c/strong\u003e in network science - the emerging trends, open problems, and research frontiers that will shape the field in the coming years. This chapter covers machine learning, quantum networks, and other cutting-edge developments.\u003c/p\u003e\n\u003ch2 id=\"181-machine-learning-on-networks\"\u003e18.1 Machine Learning on Networks\u003c/h2\u003e\n\u003ch3 id=\"graph-neural-networks\"\u003eGraph Neural Networks\u003c/h3\u003e\n\u003ch4 id=\"graph-convolutional-networks-gcns\"\u003eGraph Convolutional Networks (GCNs)\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eLayer update\u003c/strong\u003e:\n$$H^{(l+1)} = \\sigma(\\tilde{A} H^{(l)} W^{(l)})$$\u003c/p\u003e\n\u003cp\u003eWhere:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\tilde{A} = D^{-1/2} A D^{-1/2}$: Normalized adjacency matrix\u003c/li\u003e\n\u003cli\u003e$H^{(l)}$: Node features at layer $l$\u003c/li\u003e\n\u003cli\u003e$W^{(l)}$: Weight matrix at layer $l$\u003c/li\u003e\n\u003cli\u003e$\\sigma$: Activation function\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eApplications\u003c/strong\u003e:\u003c/p\u003e","title":"Reading Notes: Newman's Networks Chapter 18 - Future Directions"},{"content":"Introduction Chapter 2 of Newman\u0026rsquo;s Networks: An Introduction focuses on technological networks - the man-made infrastructure networks that form the backbone of modern society. These networks exhibit fascinating structural properties that differ significantly from random networks, with important implications for their design, operation, and robustness.\n2.1 The Internet Network Structure The Internet is a hierarchical network with multiple layers:\nPhysical layer: Fiber optic cables, routers, switches Logical layer: IP addresses and routing protocols Application layer: Web servers, email servers, etc. Topological Properties Degree Distribution The Internet\u0026rsquo;s degree distribution follows a power law:\n$$P(k) \\sim k^{-\\gamma}$$\nWhere $\\gamma \\approx 2.2$ for the Internet router network.\nMathematical Analysis For a power-law distribution with exponent $\\gamma$:\nFirst moment (average degree): $\\langle k \\rangle = \\int_{k_{\\min}}^{\\infty} k \\cdot P(k) , dk$ Second moment: $\\langle k^2 \\rangle = \\int_{k_{\\min}}^{\\infty} k^2 \\cdot P(k) , dk$ When $\\gamma \u0026lt; 3$, the second moment diverges, indicating scale-free behavior.\nReal-World Measurements AS-level network: ~50,000 autonomous systems Router-level network: ~200,000 routers Average degree: ~3-4 connections per router Clustering coefficient: ~0.3 (much higher than random networks) Robustness Analysis Random Failures The Internet exhibits robustness against random failures:\n$$P_{\\text{connectivity}} = 1 - \\exp\\left(-\\frac{\\langle k^2 \\rangle}{\\langle k \\rangle}\\right)$$\nTargeted Attacks However, it\u0026rsquo;s vulnerable to targeted attacks on high-degree nodes:\nRemoving top 1% of nodes can fragment the network Critical infrastructure (major ISPs) are prime targets 2.2 Power Grids Network Representation Power grids can be modeled as networks where:\nNodes: Power plants, substations, transformers Edges: Transmission lines, distribution lines Weights: Electrical capacity, impedance Structural Properties Hierarchical Organization Power grids exhibit three-tier hierarchy:\nTransmission network: High-voltage long-distance lines Sub-transmission network: Medium-voltage regional distribution Distribution network: Low-voltage local distribution Mathematical Modeling The electrical flow through the network follows Kirchhoff\u0026rsquo;s laws:\n$$\\sum_{j} I_{ij} = 0 \\quad \\text{(current conservation)}$$\n$$\\sum_{\\text{loop}} V_{ij} = 0 \\quad \\text{(voltage conservation)}$$\nWhere $I_{ij}$ is current and $V_{ij}$ is voltage between nodes $i$ and $j$.\nCascading Failures Load Redistribution Model When a line fails, its load redistributes to other lines:\n$$L_i(t+1) = L_i(t) + \\sum_{j \\in \\text{failed}} \\frac{L_j(t)}{|\\text{neighbors}(j)|}$$\nCritical Threshold The system becomes unstable when:\n$$\\frac{\\text{total load}}{\\text{total capacity}} \u0026gt; \\theta_c$$\nWhere $\\theta_c \\approx 0.6$ for typical power grids.\nReal-World Examples North American Power Grid Nodes: ~15,000 substations Edges: ~20,000 transmission lines Average degree: ~2.7 Clustering coefficient: ~0.08 European Power Grid Nodes: ~3,000 substations Edges: ~4,000 transmission lines Average degree: ~2.7 Clustering coefficient: ~0.1 2.3 Transportation Networks Road Networks Network Properties Nodes: Intersections, highway interchanges Edges: Road segments Weights: Travel time, distance, capacity Scale-Free Characteristics Many road networks exhibit scale-free properties:\nHighway networks: $\\gamma \\approx 2.0-2.5$ Urban road networks: $\\gamma \\approx 2.5-3.0$ Efficiency Metrics Network efficiency measures how well the network facilitates movement:\n$$E = \\frac{1}{n(n-1)} \\sum_{i \\neq j} \\frac{1}{d_{ij}}$$\nWhere $d_{ij}$ is the shortest path length between nodes $i$ and $j$.\nAirline Networks Hub-and-Spoke Structure Airlines use hub-and-spoke topology:\nHub airports: Major connection points (high degree) Spoke airports: Regional airports (low degree) Scale-free distribution: $\\gamma \\approx 1.8-2.2$ Mathematical Analysis The betweenness centrality of a node measures its importance as a connector:\n$$C_B(v) = \\sum_{s \\neq v \\neq t} \\frac{\\sigma_{st}(v)}{\\sigma_{st}}$$\nWhere $\\sigma_{st}(v)$ is the number of shortest paths between $s$ and $t$ that pass through $v$.\nRailway Networks Network Characteristics Nodes: Stations, junctions Edges: Railway tracks Weights: Travel time, frequency Small-World Properties Railway networks often exhibit small-world characteristics:\nHigh clustering: Stations in the same region are well-connected Short path lengths: Efficient long-distance connections 2.4 Communication Networks Telephone Networks Historical Evolution Circuit-switched networks: Traditional telephone systems Packet-switched networks: Modern IP-based systems Mobile networks: Cellular infrastructure Network Topology Traditional telephone networks have hierarchical structure:\nLocal exchanges: Connect to subscribers Tandem exchanges: Connect local exchanges Toll exchanges: Long-distance connections Mobile Networks Cellular Architecture Base stations: Network nodes Cells: Coverage areas Handoffs: Dynamic edge creation/deletion Mathematical Modeling The coverage area of a base station follows:\n$$A = \\pi R^2$$\nWhere $R$ is the cell radius, determined by:\n$$R = \\sqrt{\\frac{P_t G_t G_r \\lambda^2}{(4\\pi)^2 P_r}}$$\n$P_t$: Transmit power $G_t, G_r$: Antenna gains $\\lambda$: Wavelength $P_r$: Required received power 2.5 Applications to Materials Science Nanowire Networks Network Formation Silver nanowire networks can be modeled as random geometric graphs:\nNodes: Nanowire junctions Edges: Nanowire segments Weights: Electrical resistance Percolation Theory The percolation threshold determines when the network becomes conductive:\n$$p_c = \\frac{1}{\\langle k \\rangle}$$\nFor nanowire networks, this corresponds to the critical density for electrical connectivity.\nElectrical Properties The conductivity of the network follows:\n$$\\sigma \\sim (p - p_c)^t$$\nWhere $t \\approx 2.0$ is the conductivity exponent.\nSmart Materials Self-Assembling Networks Materials that form networks through self-assembly:\nBlock copolymers: Form ordered network structures Liquid crystals: Create defect networks Colloidal systems: Generate percolating networks Code Example: Network Analysis import networkx as nx import numpy as np import matplotlib.pyplot as plt def analyze_technological_network(G, network_type=\u0026#34;generic\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Analyze technological network properties\u0026#34;\u0026#34;\u0026#34; # Basic statistics n = G.number_of_nodes() m = G.number_of_edges() density = 2 * m / (n * (n - 1)) # Degree analysis degrees = [d for n, d in G.degree()] avg_degree = np.mean(degrees) # Power-law fitting (simplified) degree_counts = np.bincount(degrees) non_zero = degree_counts[degree_counts \u0026gt; 0] if len(non_zero) \u0026gt; 1: # Simple power-law estimation x = np.arange(1, len(non_zero) + 1) y = non_zero[1:] if len(y) \u0026gt; 0: log_x = np.log(x) log_y = np.log(y) # Linear regression in log space gamma = -np.polyfit(log_x, log_y, 1)[0] else: gamma = None else: gamma = None # Clustering and path length clustering = nx.average_clustering(G) if nx.is_connected(G): avg_path_length = nx.average_shortest_path_length(G) diameter = nx.diameter(G) else: # Analyze largest component largest_cc = max(nx.connected_components(G), key=len) subgraph = G.subgraph(largest_cc) avg_path_length = nx.average_shortest_path_length(subgraph) diameter = nx.diameter(subgraph) # Robustness analysis # Random node removal random_removal = [] for p in np.linspace(0, 0.5, 11): n_remove = int(p * n) if n_remove \u0026gt; 0: nodes_to_remove = np.random.choice(list(G.nodes()), n_remove, replace=False) G_temp = G.copy() G_temp.remove_nodes_from(nodes_to_remove) if G_temp.number_of_nodes() \u0026gt; 0: largest_cc = max(nx.connected_components(G_temp), key=len) random_removal.append(len(largest_cc) / n) else: random_removal.append(0) else: random_removal.append(1.0) return { \u0026#39;network_type\u0026#39;: network_type, \u0026#39;nodes\u0026#39;: n, \u0026#39;edges\u0026#39;: m, \u0026#39;density\u0026#39;: density, \u0026#39;avg_degree\u0026#39;: avg_degree, \u0026#39;gamma_estimate\u0026#39;: gamma, \u0026#39;clustering\u0026#39;: clustering, \u0026#39;avg_path_length\u0026#39;: avg_path_length, \u0026#39;diameter\u0026#39;: diameter, \u0026#39;random_removal\u0026#39;: random_removal } # Example: Analyze a scale-free network G = nx.barabasi_albert_graph(1000, 3) results = analyze_technological_network(G, \u0026#34;scale-free\u0026#34;) print(f\u0026#34;Network Analysis Results:\u0026#34;) for key, value in results.items(): if key != \u0026#39;random_removal\u0026#39;: print(f\u0026#34;{key}: {value}\u0026#34;) Key Takeaways Technological networks are scale-free: Most exhibit power-law degree distributions Hierarchical organization: Many have clear hierarchical structures Robustness-vulnerability trade-off: Robust to random failures, vulnerable to targeted attacks Small-world properties: Short path lengths despite large size Real-world applications: Direct relevance to materials science and engineering Mathematical modeling: Network theory provides powerful tools for analysis References Newman, M. E. J. (2010). Networks: An Introduction. Oxford University Press. Albert, R., Jeong, H., \u0026amp; Barabási, A. L. (2000). Error and attack tolerance of complex networks. Nature, 406(6794), 378-382. Crucitti, P., Latora, V., \u0026amp; Marchiori, M. (2004). Model for cascading failures in complex networks. Physical Review E, 69(4), 045104. This chapter demonstrates how network theory applies to real-world technological systems, providing both theoretical insights and practical applications for materials science research.\n","permalink":"https://Linlin-resh.github.io/posts/reading-notes-newman-ch2/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eChapter 2 of Newman\u0026rsquo;s \u003cem\u003eNetworks: An Introduction\u003c/em\u003e focuses on \u003cstrong\u003etechnological networks\u003c/strong\u003e - the man-made infrastructure networks that form the backbone of modern society. These networks exhibit fascinating structural properties that differ significantly from random networks, with important implications for their design, operation, and robustness.\u003c/p\u003e\n\u003ch2 id=\"21-the-internet\"\u003e2.1 The Internet\u003c/h2\u003e\n\u003ch3 id=\"network-structure\"\u003eNetwork Structure\u003c/h3\u003e\n\u003cp\u003eThe Internet is a \u003cstrong\u003ehierarchical network\u003c/strong\u003e with multiple layers:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003ePhysical layer\u003c/strong\u003e: Fiber optic cables, routers, switches\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLogical layer\u003c/strong\u003e: IP addresses and routing protocols\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eApplication layer\u003c/strong\u003e: Web servers, email servers, etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"topological-properties\"\u003eTopological Properties\u003c/h3\u003e\n\u003ch4 id=\"degree-distribution\"\u003eDegree Distribution\u003c/h4\u003e\n\u003cp\u003eThe Internet\u0026rsquo;s degree distribution follows a \u003cstrong\u003epower law\u003c/strong\u003e:\u003c/p\u003e","title":"Reading Notes: Newman's Networks Chapter 2 - Technological Networks"},{"content":"Introduction Chapter 3 of Newman\u0026rsquo;s Networks: An Introduction explores social networks - the webs of relationships that connect individuals in society. Social networks exhibit unique structural properties that reflect human behavior patterns and have profound implications for information spread, influence, and social dynamics.\n3.1 Data Collection Methods Survey-Based Methods Interviews and Questionnaires Face-to-face interviews provide high-quality data but are:\nExpensive: Require trained interviewers Time-consuming: Limited sample sizes Subject to bias: Social desirability effects Mathematical framework for survey design:\n$$n = \\frac{z^2 p(1-p)}{e^2}$$\nWhere:\n$n$: Required sample size $z$: Confidence level (e.g., 1.96 for 95%) $p$: Expected proportion $e$: Margin of error Online Surveys Advantages:\nCost-effective: Large sample sizes possible Anonymity: Reduced social desirability bias Geographic reach: Global data collection Challenges:\nSelection bias: Digital divide effects Response rates: Often low (\u0026lt; 20%) Data quality: Self-reported relationships Observational Methods Direct Observation Systematic observation of social interactions:\nTime sampling: Record interactions at regular intervals Event sampling: Record specific interaction types Focal sampling: Follow specific individuals Mathematical modeling of observation reliability:\n$$R = \\frac{2r_{12}}{1 + r_{12}}$$\nWhere $r_{12}$ is the correlation between two observers.\nDigital Traces Modern data sources:\nSocial media: Facebook, Twitter, LinkedIn Communication logs: Email, phone calls, texts Location data: GPS, check-ins, proximity Privacy considerations:\nAnonymization: Remove identifying information Consent: Informed consent for data use Ethics: Institutional review board approval 3.2 Network Sampling Strategies Snowball Sampling Method: Start with initial contacts, then ask them to nominate others.\nMathematical model:\n$$P(\\text{reach node } i) = 1 - (1 - p)^{d_i}$$\nWhere:\n$p$: Probability of being nominated $d_i$: Degree of node $i$ Advantages:\nHidden populations: Access hard-to-reach groups Cost-effective: Leverages existing relationships Disadvantages:\nBias: Favors high-degree nodes Clustering: May miss isolated components Contact Tracing Epidemiological approach:\nIndex case: Start with known infected individual Contact identification: Find all contacts Network expansion: Repeat for each contact Mathematical framework:\n$$R_0 = \\beta \\cdot \\tau \\cdot c$$\nWhere:\n$R_0$: Basic reproduction number $\\beta$: Transmission probability $\\tau$: Infectious period $c$: Contact rate Random Walk Sampling Method: Start at random node, follow random edges.\nStationary distribution:\n$$\\pi_i = \\frac{k_i}{2m}$$\nWhere $k_i$ is the degree of node $i$ and $m$ is the total number of edges.\nAdvantages:\nUnbiased: Proportional to degree Efficient: No need for complete network 3.3 Structural Properties of Social Networks Degree Distribution Power-Law Behavior Many social networks exhibit scale-free degree distributions:\n$$P(k) \\sim k^{-\\gamma}$$\nWhere $\\gamma \\approx 2.0-3.0$ for social networks.\nMathematical Analysis Cumulative distribution function:\n$$P(K \\geq k) = \\int_k^{\\infty} P(k\u0026rsquo;) , dk\u0026rsquo; \\sim k^{-(\\gamma-1)}$$\nMoment analysis:\nFirst moment: $\\langle k \\rangle = \\int k P(k) , dk$ Second moment: $\\langle k^2 \\rangle = \\int k^2 P(k) , dk$ Clustering Coefficient Local Clustering Definition:\n$$C_i = \\frac{2e_i}{k_i(k_i-1)}$$\nWhere $e_i$ is the number of edges between neighbors of node $i$.\nGlobal Clustering Average clustering:\n$$C = \\frac{1}{n} \\sum_{i=1}^{n} C_i$$\nTransitivity:\n$$T = \\frac{3 \\times \\text{number of triangles}}{\\text{number of connected triples}}$$\nSocial Interpretation High clustering: Dense social groups, strong local connections Low clustering: Sparse connections, weak social ties Path Length and Small-World Effect Milgram\u0026rsquo;s Experiment Six degrees of separation:\nTarget: Random person in Boston Method: Mail forwarding through acquaintances Result: Average path length ≈ 6 Mathematical Framework Small-world property:\n$$L \\sim \\log n \\quad \\text{and} \\quad C \\gg C_{\\text{random}}$$\nWhere:\n$L$: Average path length $C$: Clustering coefficient $C_{\\text{random}}$: Clustering in random network Network Efficiency Global efficiency:\n$$E = \\frac{1}{n(n-1)} \\sum_{i \\neq j} \\frac{1}{d_{ij}}$$\nLocal efficiency:\n$$E_{\\text{local}} = \\frac{1}{n} \\sum_{i} E_i$$\nWhere $E_i$ is the efficiency of the subgraph of neighbors of node $i$.\n3.4 Community Structure Modularity Definition:\n$$Q = \\frac{1}{2m} \\sum_{ij} \\left[ A_{ij} - \\frac{k_i k_j}{2m} \\right] \\delta(c_i, c_j)$$\nWhere:\n$A_{ij}$: Adjacency matrix $k_i, k_j$: Degrees of nodes $i, j$ $c_i, c_j$: Community assignments $\\delta(c_i, c_j)$: Kronecker delta Interpretation $Q \u0026gt; 0$: More edges within communities than expected by chance $Q = 0$: Random network structure $Q \u0026lt; 0$: Fewer edges within communities than expected Community Detection Algorithms Girvan-Newman Algorithm Method:\nCalculate betweenness centrality for all edges Remove edge with highest betweenness Recalculate betweenness for remaining edges Repeat until no edges remain Betweenness centrality:\n$$C_B(e) = \\sum_{s \\neq t} \\frac{\\sigma_{st}(e)}{\\sigma_{st}}$$\nWhere $\\sigma_{st}(e)$ is the number of shortest paths between $s$ and $t$ that pass through edge $e$.\nSpectral Clustering Laplacian matrix:\n$$L = D - A$$\nWhere $D$ is the degree matrix and $A$ is the adjacency matrix.\nNormalized Laplacian:\n$$L_{\\text{norm}} = D^{-1/2} L D^{-1/2}$$\nEigenvalue decomposition:\n$$L_{\\text{norm}} = U \\Lambda U^T$$\nUse the first $k$ eigenvectors to cluster nodes.\n3.5 Influence and Centrality Centrality Measures Degree Centrality Definition:\n$$C_D(i) = \\frac{k_i}{n-1}$$\nInterpretation: Number of direct connections\nBetweenness Centrality Definition:\n$$C_B(i) = \\sum_{s \\neq i \\neq t} \\frac{\\sigma_{st}(i)}{\\sigma_{st}}$$\nInterpretation: How often node $i$ lies on shortest paths\nCloseness Centrality Definition:\n$$C_C(i) = \\frac{n-1}{\\sum_{j \\neq i} d_{ij}}$$\nInterpretation: Average distance to all other nodes\nEigenvector Centrality Definition:\n$$x_i = \\frac{1}{\\lambda} \\sum_{j} A_{ij} x_j$$\nMatrix form:\n$$A \\mathbf{x} = \\lambda \\mathbf{x}$$\nInterpretation: Importance based on connections to important nodes\nInfluence Models Linear Threshold Model Activation rule:\n$$\\sum_{j \\in \\text{active neighbors}} w_{ji} \\geq \\theta_i$$\nWhere:\n$w_{ji}$: Influence weight from $j$ to $i$ $\\theta_i$: Threshold for node $i$ Independent Cascade Model Activation probability:\n$$P(\\text{activate } j \\text{ from } i) = p_{ij}$$\nCascade dynamics:\n$$P(\\text{node } j \\text{ activated}) = 1 - \\prod_{i \\in \\text{active}} (1 - p_{ij})$$\n3.6 Applications to Materials Science Collaboration Networks Scientific Collaboration Network structure:\nNodes: Researchers Edges: Co-authorship relationships Weights: Number of joint publications Mathematical properties:\nDegree distribution: Power-law with $\\gamma \\approx 2.1$ Clustering: High ($C \\approx 0.3$) Path length: Short ($L \\approx 6$) Research Impact Citation networks:\nIn-degree: Number of citations received Out-degree: Number of papers cited PageRank: Influence based on citation network Knowledge Networks Technology Transfer Network structure:\nNodes: Institutions, companies Edges: Technology transfer relationships Weights: Transfer frequency/value Diffusion models:\nBass model: $S(t) = S_{\\max} \\frac{1 - e^{-(p+q)t}}{1 + \\frac{q}{p} e^{-(p+q)t}}$ SIR model: Susceptible → Infected → Recovered Code Example: Social Network Analysis import networkx as nx import numpy as np import matplotlib.pyplot as plt from collections import Counter def analyze_social_network(G, network_name=\u0026#34;social\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Comprehensive social network analysis\u0026#34;\u0026#34;\u0026#34; # Basic statistics n = G.number_of_nodes() m = G.number_of_edges() density = 2 * m / (n * (n - 1)) # Degree analysis degrees = [d for n, d in G.degree()] degree_dist = Counter(degrees) # Power-law fitting if len(degree_dist) \u0026gt; 1: k_values = list(degree_dist.keys()) counts = list(degree_dist.values()) # Log-log regression log_k = np.log(k_values[1:]) # Skip k=0 log_counts = np.log(counts[1:]) if len(log_k) \u0026gt; 1: gamma = -np.polyfit(log_k, log_counts, 1)[0] else: gamma = None else: gamma = None # Clustering analysis local_clustering = nx.clustering(G) global_clustering = nx.average_clustering(G) # Path length analysis if nx.is_connected(G): avg_path_length = nx.average_shortest_path_length(G) diameter = nx.diameter(G) else: # Analyze largest component largest_cc = max(nx.connected_components(G), key=len) subgraph = G.subgraph(largest_cc) avg_path_length = nx.average_shortest_path_length(subgraph) diameter = nx.diameter(subgraph) # Centrality measures degree_centrality = nx.degree_centrality(G) betweenness_centrality = nx.betweenness_centrality(G) closeness_centrality = nx.closeness_centrality(G) eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000) # Community detection try: communities = nx.community.greedy_modularity_communities(G) modularity = nx.community.modularity(G, communities) except: communities = None modularity = None # Small-world coefficient if n \u0026gt; 1: # Compare to random network p = density random_G = nx.erdos_renyi_graph(n, p) random_clustering = nx.average_clustering(random_G) random_path_length = nx.average_shortest_path_length(random_G) if nx.is_connected(random_G) else float(\u0026#39;inf\u0026#39;) small_world_coeff = (global_clustering / random_clustering) / (avg_path_length / random_path_length) else: small_world_coeff = None return { \u0026#39;network_name\u0026#39;: network_name, \u0026#39;nodes\u0026#39;: n, \u0026#39;edges\u0026#39;: m, \u0026#39;density\u0026#39;: density, \u0026#39;avg_degree\u0026#39;: np.mean(degrees), \u0026#39;gamma_estimate\u0026#39;: gamma, \u0026#39;global_clustering\u0026#39;: global_clustering, \u0026#39;avg_path_length\u0026#39;: avg_path_length, \u0026#39;diameter\u0026#39;: diameter, \u0026#39;small_world_coeff\u0026#39;: small_world_coeff, \u0026#39;modularity\u0026#39;: modularity, \u0026#39;num_communities\u0026#39;: len(communities) if communities else None, \u0026#39;degree_centrality\u0026#39;: degree_centrality, \u0026#39;betweenness_centrality\u0026#39;: betweenness_centrality, \u0026#39;closeness_centrality\u0026#39;: closeness_centrality, \u0026#39;eigenvector_centrality\u0026#39;: eigenvector_centrality } def plot_degree_distribution(G, title=\u0026#34;Degree Distribution\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Plot degree distribution in log-log scale\u0026#34;\u0026#34;\u0026#34; degrees = [d for n, d in G.degree()] degree_counts = Counter(degrees) k_values = list(degree_counts.keys()) counts = list(degree_counts.values()) plt.figure(figsize=(10, 6)) plt.loglog(k_values, counts, \u0026#39;bo\u0026#39;, markersize=8) plt.xlabel(\u0026#39;Degree k\u0026#39;) plt.ylabel(\u0026#39;Count\u0026#39;) plt.title(title) plt.grid(True, alpha=0.3) plt.show() # Example: Analyze a social network G = nx.barabasi_albert_graph(1000, 3) results = analyze_social_network(G, \u0026#34;collaboration_network\u0026#34;) print(\u0026#34;Social Network Analysis Results:\u0026#34;) for key, value in results.items(): if key not in [\u0026#39;degree_centrality\u0026#39;, \u0026#39;betweenness_centrality\u0026#39;, \u0026#39;closeness_centrality\u0026#39;, \u0026#39;eigenvector_centrality\u0026#39;]: print(f\u0026#34;{key}: {value}\u0026#34;) # Plot degree distribution plot_degree_distribution(G, \u0026#34;Collaboration Network Degree Distribution\u0026#34;) Key Takeaways Data collection challenges: Social network data is difficult to collect and often biased Scale-free structure: Most social networks follow power-law degree distributions Small-world effect: Short path lengths with high clustering are common Community structure: Social networks exhibit strong modular organization Centrality matters: Different centrality measures capture different aspects of influence Applications: Network analysis provides insights into collaboration and knowledge transfer References Newman, M. E. J. (2010). Networks: An Introduction. Oxford University Press. Milgram, S. (1967). The small world problem. Psychology Today, 2(1), 60-67. Watts, D. J., \u0026amp; Strogatz, S. H. (1998). Collective dynamics of \u0026lsquo;small-world\u0026rsquo; networks. Nature, 393(6684), 440-442. Girvan, M., \u0026amp; Newman, M. E. J. (2002). Community structure in social and biological networks. Proceedings of the National Academy of Sciences, 99(12), 7821-7826. Social networks provide a rich source of data for understanding human behavior and social dynamics, with important applications in materials science collaboration and knowledge transfer.\n","permalink":"https://Linlin-resh.github.io/posts/reading-notes-newman-ch3/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eChapter 3 of Newman\u0026rsquo;s \u003cem\u003eNetworks: An Introduction\u003c/em\u003e explores \u003cstrong\u003esocial networks\u003c/strong\u003e - the webs of relationships that connect individuals in society. Social networks exhibit unique structural properties that reflect human behavior patterns and have profound implications for information spread, influence, and social dynamics.\u003c/p\u003e\n\u003ch2 id=\"31-data-collection-methods\"\u003e3.1 Data Collection Methods\u003c/h2\u003e\n\u003ch3 id=\"survey-based-methods\"\u003eSurvey-Based Methods\u003c/h3\u003e\n\u003ch4 id=\"interviews-and-questionnaires\"\u003eInterviews and Questionnaires\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eFace-to-face interviews\u003c/strong\u003e provide high-quality data but are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eExpensive\u003c/strong\u003e: Require trained interviewers\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eTime-consuming\u003c/strong\u003e: Limited sample sizes\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSubject to bias\u003c/strong\u003e: Social desirability effects\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eMathematical framework\u003c/strong\u003e for survey design:\u003c/p\u003e","title":"Reading Notes: Newman's Networks Chapter 3 - Social Networks"},{"content":"Introduction Chapter 4 of Newman\u0026rsquo;s Networks: An Introduction examines information networks - the complex webs that connect information, knowledge, and ideas. These networks, including the World Wide Web and citation networks, exhibit unique structural properties that reflect how information is created, linked, and accessed in our digital age.\n4.1 The World Wide Web Network Structure The Web is a directed network where:\nNodes: Web pages (HTML documents) Edges: Hyperlinks (directional connections) Scale: Billions of pages, trillions of links Topological Properties Degree Distributions The Web exhibits power-law degree distributions for both in-degree and out-degree:\nIn-degree distribution (pages linking to a page): $$P_{\\text{in}}(k) \\sim k^{-\\gamma_{\\text{in}}}$$\nWhere $\\gamma_{\\text{in}} \\approx 2.1$ for the Web.\nOut-degree distribution (pages linked by a page): $$P_{\\text{out}}(k) \\sim k^{-\\gamma_{\\text{out}}}$$\nWhere $\\gamma_{\\text{out}} \\approx 2.7$ for the Web.\nMathematical Analysis Cumulative distribution functions:\n$$P_{\\text{in}}(K \\geq k) \\sim k^{-(\\gamma_{\\text{in}}-1)}$$ $$P_{\\text{out}}(K \\geq k) \\sim k^{-(\\gamma_{\\text{out}}-1)}$$\nMoment analysis:\nFirst moments: $\\langle k_{\\text{in}} \\rangle = \\langle k_{\\text{out}} \\rangle = \\frac{m}{n}$ Second moments: $\\langle k_{\\text{in}}^2 \\rangle$, $\\langle k_{\\text{out}}^2 \\rangle$ Web Graph Components Strongly Connected Components A strongly connected component (SCC) is a set of nodes where every node is reachable from every other node.\nWeb structure:\nGiant SCC: ~30% of all pages In-component: Pages that can reach the giant SCC Out-component: Pages reachable from the giant SCC Tendrils: Small disconnected components Mathematical Framework Reachability matrix: $$R_{ij} = \\begin{cases} 1 \u0026amp; \\text{if there exists a path from } i \\text{ to } j \\ 0 \u0026amp; \\text{otherwise} \\end{cases}$$\nStrong connectivity: $$S_{ij} = R_{ij} \\cdot R_{ji}$$\nPageRank Algorithm Mathematical Definition PageRank of page $i$:\n$$PR(i) = \\frac{1-d}{n} + d \\sum_{j \\in \\text{in}(i)} \\frac{PR(j)}{k_{\\text{out}}(j)}$$\nWhere:\n$d$: Damping factor (typically 0.85) $n$: Total number of pages $\\text{in}(i)$: Pages linking to page $i$ $k_{\\text{out}}(j)$: Out-degree of page $j$ Matrix Form PageRank vector: $$\\mathbf{PR} = (1-d) \\frac{\\mathbf{1}}{n} + d \\mathbf{M} \\mathbf{PR}$$\nWhere $\\mathbf{M}$ is the stochastic matrix: $$M_{ij} = \\frac{A_{ji}}{k_{\\text{out}}(j)}$$\nEigenvalue equation: $$\\mathbf{PR} = \\mathbf{M}^T \\mathbf{PR}$$\nPower Iteration Method Iterative solution: $$\\mathbf{PR}^{(t+1)} = (1-d) \\frac{\\mathbf{1}}{n} + d \\mathbf{M}^T \\mathbf{PR}^{(t)}$$\nConvergence criterion: $$||\\mathbf{PR}^{(t+1)} - \\mathbf{PR}^{(t)}||_1 \u0026lt; \\epsilon$$\nWeb Crawling and Sampling Breadth-First Search (BFS) Algorithm:\nStart with seed pages Visit all neighbors of current pages Add unvisited pages to queue Repeat until queue is empty Bias: Favors high-degree pages\nRandom Walk Sampling Random walk with transition probabilities: $$P_{ij} = \\frac{A_{ij}}{k_{\\text{out}}(i)}$$\nStationary distribution: $$\\pi_i = \\frac{k_{\\text{in}}(i)}{\\sum_j k_{\\text{in}}(j)}$$\nMetropolis-Hastings correction: $$P_{\\text{accept}} = \\min\\left(1, \\frac{k_{\\text{out}}(j)}{k_{\\text{out}}(i)}\\right)$$\n4.2 Citation Networks Network Structure Citation networks are directed acyclic graphs (DAGs) where:\nNodes: Academic papers Edges: Citations (paper A cites paper B) Properties: Acyclic (no paper cites itself directly or indirectly) Degree Distributions In-Degree (Citations Received) Power-law distribution: $$P_{\\text{in}}(k) \\sim k^{-\\gamma_{\\text{citations}}}$$\nWhere $\\gamma_{\\text{citations}} \\approx 3.0$ for citation networks.\nOut-Degree (References Made) Distribution: $$P_{\\text{out}}(k) \\sim \\text{Poisson}(\\lambda)$$\nWhere $\\lambda \\approx 20-30$ references per paper.\nCitation Dynamics Aging Effect Citation rate as a function of time: $$r(t) = r_0 e^{-\\alpha t}$$\nWhere:\n$r_0$: Initial citation rate $\\alpha$: Aging parameter $t$: Time since publication Cumulative Citations Total citations after time $T$: $$C(T) = \\int_0^T r(t) , dt = \\frac{r_0}{\\alpha}(1 - e^{-\\alpha T})$$\nLong-term behavior: $$C(\\infty) = \\frac{r_0}{\\alpha}$$\nImpact Metrics H-Index Definition: A scientist has index $h$ if $h$ of their papers have at least $h$ citations each.\nMathematical formulation: $$h = \\max{k : \\text{number of papers with } \\geq k \\text{ citations} \\geq k}$$\nG-Index Definition: The largest number $g$ such that the top $g$ papers have at least $g^2$ total citations.\nMathematical formulation: $$g = \\max{k : \\sum_{i=1}^k c_i \\geq k^2}$$\nWhere $c_i$ are citations sorted in descending order.\nPageRank for Citations Citation PageRank: $$CPR(i) = \\frac{1-d}{n} + d \\sum_{j \\in \\text{cites}(i)} \\frac{CPR(j)}{k_{\\text{out}}(j)}$$\nAdvantages over simple citation count:\nConsiders quality of citing papers Reduces self-citation effects Accounts for citation patterns 4.3 Other Information Networks Patent Citation Networks Network Properties Nodes: Patents Edges: Patent citations Scale: Millions of patents, tens of millions of citations Temporal: Citations can only go forward in time Innovation Metrics Patent impact: $$I_i = \\sum_{j \\in \\text{cited by } i} w_j \\cdot CPR(j)$$\nWhere $w_j$ is the weight of patent $j$.\nSoftware Dependency Networks Network Structure Nodes: Software packages Edges: Dependencies (package A depends on package B) Properties: Directed, often acyclic Vulnerability Analysis Cascade failure probability: $$P_{\\text{fail}}(i) = 1 - \\prod_{j \\in \\text{dependencies}(i)} (1 - P_{\\text{fail}}(j))$$\nSystem reliability: $$R = \\prod_{i \\in \\text{critical packages}} (1 - P_{\\text{fail}}(i))$$\nKnowledge Graphs Semantic Networks Nodes: Concepts, entities Edges: Semantic relationships Properties: Multi-relational, weighted Graph Embeddings Node2Vec algorithm: $$\\max_f \\sum_{u \\in V} \\log P(N_S(u)|f(u))$$\nWhere $N_S(u)$ is the neighborhood of node $u$ under sampling strategy $S$.\n4.4 Information Spreading Models SIR Model for Information Model Dynamics States:\nSusceptible (S): Haven\u0026rsquo;t received information Infected (I): Have information and can spread it Recovered (R): Have information but won\u0026rsquo;t spread it Mathematical Formulation Differential equations: $$\\frac{dS}{dt} = -\\beta SI$$ $$\\frac{dI}{dt} = \\beta SI - \\gamma I$$ $$\\frac{dR}{dt} = \\gamma I$$\nConservation: $S + I + R = N$ (constant)\nBasic Reproduction Number $$R_0 = \\frac{\\beta N}{\\gamma}$$\nEpidemic threshold: $R_0 \u0026gt; 1$\nComplex Contagion Threshold Models Activation condition: $$\\sum_{j \\in \\text{active neighbors}} w_{ji} \\geq \\theta_i$$\nWhere:\n$w_{ji}$: Influence weight $\\theta_i$: Activation threshold Mathematical Analysis Cascade condition: $$\\frac{\\langle k^2 \\rangle}{\\langle k \\rangle} \u0026gt; \\frac{1}{\\theta}$$\nWhere $\\theta$ is the average threshold.\n4.5 Applications to Materials Science Scientific Collaboration Networks Co-authorship Networks Network structure:\nNodes: Researchers Edges: Co-authorship relationships Weights: Number of joint publications Mathematical properties:\nDegree distribution: Power-law with $\\gamma \\approx 2.1$ Clustering: High ($C \\approx 0.3$) Path length: Short ($L \\approx 6$) Knowledge Transfer Information flow between research groups: $$F_{ij} = \\sum_{k} \\frac{A_{ik} A_{jk}}{k_k}$$\nWhere $A_{ik}$ is the collaboration strength between groups $i$ and $k$.\nResearch Impact Networks Citation Networks in Materials Science Top journals:\nNature Materials Advanced Materials Materials Today Journal of Materials Chemistry Impact factors: $$IF = \\frac{\\text{Citations in year } t}{\\text{Articles published in years } t-1, t-2}$$\nCollaboration Patterns Geographic clustering:\nLocal collaboration: Same institution National collaboration: Same country International collaboration: Different countries Mathematical modeling: $$P(\\text{collaboration}) = \\frac{1}{1 + e^{-\\alpha \\cdot \\text{distance}}}$$\nCode Example: Information Network Analysis import networkx as nx import numpy as np import matplotlib.pyplot as plt from collections import Counter def analyze_information_network(G, network_type=\u0026#34;web\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Analyze information network properties\u0026#34;\u0026#34;\u0026#34; # Basic statistics n = G.number_of_nodes() m = G.number_of_edges() density = m / (n * (n - 1)) # Degree analysis in_degrees = [d for n, d in G.in_degree()] out_degrees = [d for n, d in G.out_degree()] # Power-law fitting for in-degree if len(in_degrees) \u0026gt; 1: in_degree_dist = Counter(in_degrees) k_values = list(in_degree_dist.keys()) counts = list(in_degree_dist.values()) if len(k_values) \u0026gt; 1: log_k = np.log(k_values[1:]) # Skip k=0 log_counts = np.log(counts[1:]) if len(log_k) \u0026gt; 1: gamma_in = -np.polyfit(log_k, log_counts, 1)[0] else: gamma_in = None else: gamma_in = None else: gamma_in = None # Strongly connected components sccs = list(nx.strongly_connected_components(G)) scc_sizes = [len(scc) for scc in sccs] giant_scc_size = max(scc_sizes) if scc_sizes else 0 # PageRank analysis try: pagerank = nx.pagerank(G, alpha=0.85) pagerank_values = list(pagerank.values()) max_pagerank = max(pagerank_values) min_pagerank = min(pagerank_values) except: pagerank = None max_pagerank = min_pagerank = None # Hubs and authorities (HITS algorithm) try: hubs, authorities = nx.hits(G) hub_values = list(hubs.values()) authority_values = list(authorities.values()) except: hub_values = authority_values = None # Clustering (for undirected version) G_undirected = G.to_undirected() clustering = nx.average_clustering(G_undirected) # Path length analysis if nx.is_weakly_connected(G): avg_path_length = nx.average_shortest_path_length(G.to_undirected()) else: # Analyze largest weakly connected component largest_wcc = max(nx.weakly_connected_components(G), key=len) subgraph = G.subgraph(largest_wcc).to_undirected() avg_path_length = nx.average_shortest_path_length(subgraph) return { \u0026#39;network_type\u0026#39;: network_type, \u0026#39;nodes\u0026#39;: n, \u0026#39;edges\u0026#39;: m, \u0026#39;density\u0026#39;: density, \u0026#39;avg_in_degree\u0026#39;: np.mean(in_degrees), \u0026#39;avg_out_degree\u0026#39;: np.mean(out_degrees), \u0026#39;gamma_in_estimate\u0026#39;: gamma_in, \u0026#39;num_sccs\u0026#39;: len(sccs), \u0026#39;giant_scc_size\u0026#39;: giant_scc_size, \u0026#39;giant_scc_fraction\u0026#39;: giant_scc_size / n if n \u0026gt; 0 else 0, \u0026#39;max_pagerank\u0026#39;: max_pagerank, \u0026#39;min_pagerank\u0026#39;: min_pagerank, \u0026#39;clustering\u0026#39;: clustering, \u0026#39;avg_path_length\u0026#39;: avg_path_length } def plot_degree_distributions(G, title=\u0026#34;Information Network Degree Distributions\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Plot in-degree and out-degree distributions\u0026#34;\u0026#34;\u0026#34; in_degrees = [d for n, d in G.in_degree()] out_degrees = [d for n, d in G.out_degree()] fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6)) # In-degree distribution in_degree_dist = Counter(in_degrees) k_values = list(in_degree_dist.keys()) counts = list(in_degree_dist.values()) ax1.loglog(k_values, counts, \u0026#39;bo\u0026#39;, markersize=8) ax1.set_xlabel(\u0026#39;In-Degree k\u0026#39;) ax1.set_ylabel(\u0026#39;Count\u0026#39;) ax1.set_title(\u0026#39;In-Degree Distribution\u0026#39;) ax1.grid(True, alpha=0.3) # Out-degree distribution out_degree_dist = Counter(out_degrees) k_values = list(out_degree_dist.keys()) counts = list(out_degree_dist.values()) ax2.loglog(k_values, counts, \u0026#39;ro\u0026#39;, markersize=8) ax2.set_xlabel(\u0026#39;Out-Degree k\u0026#39;) ax2.set_ylabel(\u0026#39;Count\u0026#39;) ax2.set_title(\u0026#39;Out-Degree Distribution\u0026#39;) ax2.grid(True, alpha=0.3) plt.suptitle(title) plt.tight_layout() plt.show() def analyze_citation_network(citations): \u0026#34;\u0026#34;\u0026#34;Analyze citation network properties\u0026#34;\u0026#34;\u0026#34; # Create citation network G = nx.DiGraph() for citing, cited in citations: G.add_edge(citing, cited) # Basic analysis results = analyze_information_network(G, \u0026#34;citation\u0026#34;) # Citation-specific metrics in_degrees = [d for n, d in G.in_degree()] # H-index calculation def calculate_h_index(citations): citations_sorted = sorted(citations, reverse=True) h = 0 for i, c in enumerate(citations_sorted): if c \u0026gt;= i + 1: h = i + 1 else: break return h h_index = calculate_h_index(in_degrees) # G-index calculation def calculate_g_index(citations): citations_sorted = sorted(citations, reverse=True) g = 0 total_citations = 0 for i, c in enumerate(citations_sorted): total_citations += c if total_citations \u0026gt;= (i + 1) ** 2: g = i + 1 else: break return g g_index = calculate_g_index(in_degrees) results[\u0026#39;h_index\u0026#39;] = h_index results[\u0026#39;g_index\u0026#39;] = g_index return results # Example: Analyze a web-like network G = nx.scale_free_graph(1000, alpha=0.41, beta=0.54, gamma=0.05, delta_in=0.2, delta_out=0) results = analyze_information_network(G, \u0026#34;web_simulation\u0026#34;) print(\u0026#34;Information Network Analysis Results:\u0026#34;) for key, value in results.items(): print(f\u0026#34;{key}: {value}\u0026#34;) # Plot degree distributions plot_degree_distributions(G, \u0026#34;Simulated Web Network\u0026#34;) Key Takeaways Directed networks: Information networks are typically directed with asymmetric relationships Power-law distributions: Both in-degree and out-degree often follow power laws Scale-free structure: Few highly connected nodes, many sparsely connected nodes PageRank importance: Algorithm for ranking nodes based on network structure Citation dynamics: Citations follow predictable patterns over time Impact metrics: H-index, G-index, and PageRank provide different measures of impact Applications: Direct relevance to scientific collaboration and knowledge transfer References Newman, M. E. J. (2010). Networks: An Introduction. Oxford University Press. Page, L., Brin, S., Motwani, R., \u0026amp; Winograd, T. (1999). The PageRank citation ranking: Bringing order to the web. Stanford InfoLab. Barabási, A. L., \u0026amp; Albert, R. (1999). Emergence of scaling in random networks. Science, 286(5439), 509-512. Hirsch, J. E. (2005). An index to quantify an individual\u0026rsquo;s scientific research output. Proceedings of the National Academy of Sciences, 102(46), 16569-16572. Information networks provide insights into how knowledge is created, linked, and accessed, with important applications in scientific collaboration and research impact assessment.\n","permalink":"https://Linlin-resh.github.io/posts/reading-notes-newman-ch4/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eChapter 4 of Newman\u0026rsquo;s \u003cem\u003eNetworks: An Introduction\u003c/em\u003e examines \u003cstrong\u003einformation networks\u003c/strong\u003e - the complex webs that connect information, knowledge, and ideas. These networks, including the World Wide Web and citation networks, exhibit unique structural properties that reflect how information is created, linked, and accessed in our digital age.\u003c/p\u003e\n\u003ch2 id=\"41-the-world-wide-web\"\u003e4.1 The World Wide Web\u003c/h2\u003e\n\u003ch3 id=\"network-structure\"\u003eNetwork Structure\u003c/h3\u003e\n\u003cp\u003eThe Web is a \u003cstrong\u003edirected network\u003c/strong\u003e where:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eNodes\u003c/strong\u003e: Web pages (HTML documents)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEdges\u003c/strong\u003e: Hyperlinks (directional connections)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eScale\u003c/strong\u003e: Billions of pages, trillions of links\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"topological-properties\"\u003eTopological Properties\u003c/h3\u003e\n\u003ch4 id=\"degree-distributions\"\u003eDegree Distributions\u003c/h4\u003e\n\u003cp\u003eThe Web exhibits \u003cstrong\u003epower-law degree distributions\u003c/strong\u003e for both in-degree and out-degree:\u003c/p\u003e","title":"Reading Notes: Newman's Networks Chapter 4 - Information Networks"},{"content":"Introduction Chapter 5 of Newman\u0026rsquo;s Networks: An Introduction explores biological networks - the complex webs of interactions that govern life at multiple scales. From molecular interactions to ecosystem dynamics, biological networks exhibit unique structural properties that reflect evolutionary pressures and functional constraints.\n5.1 Protein-Protein Interaction Networks Network Structure Protein-protein interaction (PPI) networks represent:\nNodes: Proteins Edges: Physical interactions between proteins Scale: ~20,000 human proteins, ~100,000 interactions Properties: Undirected, weighted (interaction strength) Topological Properties Degree Distribution PPI networks exhibit power-law degree distributions:\n$$P(k) \\sim k^{-\\gamma}$$\nWhere $\\gamma \\approx 2.0-2.5$ for most PPI networks.\nMathematical Analysis Cumulative distribution: $$P(K \\geq k) \\sim k^{-(\\gamma-1)}$$\nMoment analysis:\nFirst moment: $\\langle k \\rangle = \\frac{2m}{n}$ Second moment: $\\langle k^2 \\rangle = \\int k^2 P(k) , dk$ Scale-Free Behavior Scale-free networks have:\nInfinite variance when $\\gamma \\leq 3$ Heavy-tailed distribution Hub proteins with many interactions Functional Analysis Essential Proteins Essentiality correlates with network properties:\nHigh-degree proteins: More likely to be essential High betweenness: Critical for network connectivity High clustering: Part of functional modules Mathematical Framework Essentiality probability: $$P(\\text{essential}|k) = \\frac{1}{1 + e^{-\\alpha(k - k_0)}}$$\nWhere:\n$\\alpha$: Sensitivity parameter $k_0$: Threshold degree Gene Ontology Analysis Functional enrichment: $$P(\\text{GO term}|k) = \\frac{\\text{Proteins with GO term and degree } k}{\\text{Proteins with degree } k}$$\nNetwork Motifs Common Motifs Three-node motifs:\nFeed-forward loop: A→B→C, A→C Feedback loop: A→B→C→A Bi-fan: A→B, A→C, D→B, D→C Mathematical counting: $$N_{\\text{motif}} = \\sum_{i,j,k} A_{ij} A_{jk} A_{ik}$$\nStatistical Significance Z-score: $$Z = \\frac{N_{\\text{observed}} - \\langle N_{\\text{random}} \\rangle}{\\sigma_{N_{\\text{random}}}}$$\nP-value: $$P = P(N_{\\text{random}} \\geq N_{\\text{observed}})$$\n5.2 Metabolic Networks Network Representation Metabolic networks consist of:\nNodes: Metabolites (small molecules) Edges: Biochemical reactions Scale: ~1,000-5,000 metabolites, ~2,000-10,000 reactions Properties: Bipartite (metabolites ↔ reactions) Bipartite Network Analysis Projection Networks Metabolite-metabolite network: $$A_{ij} = \\sum_r B_{ir} B_{jr}$$\nWhere $B_{ir} = 1$ if metabolite $i$ participates in reaction $r$.\nReaction-reaction network: $$A_{rs} = \\sum_i B_{ir} B_{is}$$\nMathematical Properties Degree distributions:\nMetabolite degrees: Power-law with $\\gamma \\approx 2.0$ Reaction degrees: More uniform distribution Flux Analysis Flux Balance Analysis (FBA) Objective function: $$\\max \\sum_i v_i$$\nConstraints: $$\\sum_j S_{ij} v_j = 0 \\quad \\text{(mass balance)}$$ $$v_j^{\\min} \\leq v_j \\leq v_j^{\\max} \\quad \\text{(flux bounds)}$$\nWhere:\n$v_j$: Flux through reaction $j$ $S_{ij}$: Stoichiometric matrix $S_{ij} \u0026gt; 0$: Metabolite $i$ is produced by reaction $j$ $S_{ij} \u0026lt; 0$: Metabolite $i$ is consumed by reaction $j$ Elementary Flux Modes Definition: Minimal sets of reactions that can operate in steady state.\nMathematical formulation: $$\\mathbf{v} = \\sum_k \\alpha_k \\mathbf{e}_k$$\nWhere $\\mathbf{e}_k$ are elementary flux modes and $\\alpha_k \\geq 0$.\nNetwork Robustness Knockout Analysis Single knockout: $$R_i = \\frac{F_{\\text{wt}} - F_{\\text{ko}i}}{F{\\text{wt}}}$$\nWhere:\n$F_{\\text{wt}}$: Wild-type flux $F_{\\text{ko}_i}$: Flux after knocking out reaction $i$ Double knockout: $$R_{ij} = \\frac{F_{\\text{wt}} - F_{\\text{ko}{ij}}}{F{\\text{wt}}}$$\nSynthetic Lethality Definition: Two reactions are synthetically lethal if:\nSingle knockouts are viable Double knockout is lethal Mathematical condition: $$R_i \u0026lt; \\theta \\text{ and } R_j \u0026lt; \\theta \\text{ but } R_{ij} \u0026gt; \\theta$$\nWhere $\\theta$ is the viability threshold.\n5.3 Neural Networks Brain Connectivity Structural Networks Neural networks represent:\nNodes: Neurons or brain regions Edges: Synaptic connections or white matter tracts Scale: ~86 billion neurons, ~100 trillion synapses Properties: Directed, weighted, dynamic Network Properties Degree distributions:\nIn-degree: Input connections to a neuron Out-degree: Output connections from a neuron Power-law: $\\gamma \\approx 2.0-2.5$ Clustering coefficient: $$C_i = \\frac{2e_i}{k_i(k_i-1)}$$\nSmall-world properties:\nHigh clustering: Local connectivity Short path length: Global efficiency Functional Networks Correlation Networks Functional connectivity: $$r_{ij} = \\frac{\\langle x_i x_j \\rangle - \\langle x_i \\rangle \\langle x_j \\rangle}{\\sigma_i \\sigma_j}$$\nWhere $x_i$ is the activity of region $i$.\nNetwork construction: $$A_{ij} = \\begin{cases} 1 \u0026amp; \\text{if } |r_{ij}| \u0026gt; \\theta \\ 0 \u0026amp; \\text{otherwise} \\end{cases}$$\nCommunity Structure Modularity: $$Q = \\frac{1}{2m} \\sum_{ij} \\left[ A_{ij} - \\frac{k_i k_j}{2m} \\right] \\delta(c_i, c_j)$$\nFunctional modules:\nVisual cortex: Visual processing Motor cortex: Motor control Prefrontal cortex: Executive functions Synchronization Kuramoto Model Phase dynamics: $$\\frac{d\\theta_i}{dt} = \\omega_i + \\frac{K}{N} \\sum_{j=1}^N A_{ij} \\sin(\\theta_j - \\theta_i)$$\nWhere:\n$\\theta_i$: Phase of oscillator $i$ $\\omega_i$: Natural frequency $K$: Coupling strength $A_{ij}$: Adjacency matrix Synchronization Order Parameter Global order parameter: $$r = \\left| \\frac{1}{N} \\sum_{j=1}^N e^{i\\theta_j} \\right|$$\nLocal order parameter: $$r_i = \\left| \\frac{1}{k_i} \\sum_{j \\in \\mathcal{N}_i} e^{i\\theta_j} \\right|$$\nCritical Coupling Synchronization threshold: $$K_c = \\frac{2}{\\pi g(0)} \\frac{\\langle k^2 \\rangle}{\\langle k \\rangle}$$\nWhere $g(0)$ is the frequency distribution at zero.\n5.4 Ecological Networks Food Webs Network Structure Food webs represent:\nNodes: Species Edges: Trophic interactions (predator-prey relationships) Scale: 10-1000 species, 100-10,000 interactions Properties: Directed, weighted (interaction strength) Trophic Levels Trophic level of species $i$: $$TL_i = 1 + \\frac{\\sum_j A_{ji} TL_j}{\\sum_j A_{ji}}$$\nWhere $A_{ji} = 1$ if species $j$ preys on species $i$.\nNetwork Properties Connectance: $$C = \\frac{L}{S(S-1)}$$\nWhere:\n$L$: Number of links $S$: Number of species Link density: $$LD = \\frac{L}{S}$$\nAverage path length: $$L = \\frac{1}{S(S-1)} \\sum_{i \\neq j} d_{ij}$$\nStability Analysis May\u0026rsquo;s Stability Criterion Random matrix theory: $$\\lambda_{\\max} = \\sqrt{SC}$$\nWhere:\n$\\lambda_{\\max}$: Largest eigenvalue $S$: Number of species $C$: Connectance Stability condition: $$\\lambda_{\\max} \u0026lt; 1$$\nStructural Stability Robustness to species removal: $$R = \\frac{1}{S} \\sum_{i=1}^S R_i$$\nWhere $R_i$ is the fraction of species remaining after removing species $i$.\nBiodiversity and Network Structure Species-Area Relationships Power-law relationship: $$S = cA^z$$\nWhere:\n$S$: Number of species $A$: Area $c$: Constant $z$: Scaling exponent (typically 0.2-0.3) Network Robustness Cascade effects: $$P(\\text{extinction}|k) = 1 - (1-p)^k$$\nWhere $p$ is the probability of extinction due to one lost connection.\n5.5 Applications to Materials Science Biomimetic Networks Self-Assembling Materials Network formation in self-assembling systems:\nBlock copolymers: Form ordered network structures Liquid crystals: Create defect networks Colloidal systems: Generate percolating networks Mathematical modeling: $$P(\\text{connection}) = \\frac{1}{1 + e^{-\\beta(E - \\mu)}}$$\nWhere:\n$E$: Interaction energy $\\mu$: Chemical potential $\\beta$: Inverse temperature Protein-Inspired Materials Network properties of protein-inspired materials:\nHierarchical structure: Multiple length scales Self-healing: Dynamic network reformation Responsive behavior: Network properties change with environment Network-Based Drug Design Target Identification Network-based drug targets:\nEssential proteins: High-degree nodes Bottleneck proteins: High betweenness centrality Module hubs: High clustering coefficient Mathematical framework: $$T_i = \\alpha \\cdot k_i + \\beta \\cdot C_B(i) + \\gamma \\cdot C_i$$\nWhere $T_i$ is the target score for protein $i$.\nDrug Repurposing Network-based drug repurposing:\nSimilarity networks: Drug-drug similarity Target networks: Protein-protein interactions Disease networks: Disease-disease associations Code Example: Biological Network Analysis import networkx as nx import numpy as np import matplotlib.pyplot as plt from collections import Counter def analyze_biological_network(G, network_type=\u0026#34;ppi\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Analyze biological network properties\u0026#34;\u0026#34;\u0026#34; # Basic statistics n = G.number_of_nodes() m = G.number_of_edges() density = 2 * m / (n * (n - 1)) # Degree analysis degrees = [d for n, d in G.degree()] degree_dist = Counter(degrees) # Power-law fitting if len(degree_dist) \u0026gt; 1: k_values = list(degree_dist.keys()) counts = list(degree_dist.values()) if len(k_values) \u0026gt; 1: log_k = np.log(k_values[1:]) # Skip k=0 log_counts = np.log(counts[1:]) if len(log_k) \u0026gt; 1: gamma = -np.polyfit(log_k, log_counts, 1)[0] else: gamma = None else: gamma = None else: gamma = None # Clustering analysis clustering = nx.average_clustering(G) local_clustering = nx.clustering(G) # Path length analysis if nx.is_connected(G): avg_path_length = nx.average_shortest_path_length(G) diameter = nx.diameter(G) else: # Analyze largest component largest_cc = max(nx.connected_components(G), key=len) subgraph = G.subgraph(largest_cc) avg_path_length = nx.average_shortest_path_length(subgraph) diameter = nx.diameter(subgraph) # Centrality measures degree_centrality = nx.degree_centrality(G) betweenness_centrality = nx.betweenness_centrality(G) closeness_centrality = nx.closeness_centrality(G) # Essentiality analysis (simplified) # High-degree nodes are more likely to be essential essentiality_scores = {} for node in G.nodes(): k = G.degree(node) # Simple model: essentiality increases with degree essentiality_scores[node] = 1 / (1 + np.exp(-0.1 * (k - 10))) # Network motifs (simplified) # Count triangles triangles = sum(nx.triangles(G).values()) / 3 # Community detection try: communities = nx.community.greedy_modularity_communities(G) modularity = nx.community.modularity(G, communities) except: communities = None modularity = None return { \u0026#39;network_type\u0026#39;: network_type, \u0026#39;nodes\u0026#39;: n, \u0026#39;edges\u0026#39;: m, \u0026#39;density\u0026#39;: density, \u0026#39;avg_degree\u0026#39;: np.mean(degrees), \u0026#39;gamma_estimate\u0026#39;: gamma, \u0026#39;clustering\u0026#39;: clustering, \u0026#39;avg_path_length\u0026#39;: avg_path_length, \u0026#39;diameter\u0026#39;: diameter, \u0026#39;triangles\u0026#39;: triangles, \u0026#39;modularity\u0026#39;: modularity, \u0026#39;num_communities\u0026#39;: len(communities) if communities else None, \u0026#39;essentiality_scores\u0026#39;: essentiality_scores } def analyze_metabolic_network(metabolites, reactions): \u0026#34;\u0026#34;\u0026#34;Analyze metabolic network using bipartite representation\u0026#34;\u0026#34;\u0026#34; # Create bipartite network B = nx.Graph() # Add metabolite nodes for metabolite in metabolites: B.add_node(metabolite, bipartite=0) # Add reaction nodes for reaction in reactions: B.add_node(reaction, bipartite=1) # Add edges (metabolite-reaction connections) for reaction, metabolite_list in reactions.items(): for metabolite in metabolite_list: B.add_edge(metabolite, reaction) # Project to metabolite-metabolite network metabolite_nodes = [n for n, d in B.nodes(data=True) if d[\u0026#39;bipartite\u0026#39;] == 0] G = nx.Graph() G.add_nodes_from(metabolite_nodes) # Add edges between metabolites that share reactions for metabolite1 in metabolite_nodes: for metabolite2 in metabolite_nodes: if metabolite1 != metabolite2: shared_reactions = set(B.neighbors(metabolite1)) \u0026amp; set(B.neighbors(metabolite2)) if shared_reactions: G.add_edge(metabolite1, metabolite2, weight=len(shared_reactions)) # Analyze projected network results = analyze_biological_network(G, \u0026#34;metabolic\u0026#34;) # Metabolic-specific metrics results[\u0026#39;num_metabolites\u0026#39;] = len(metabolites) results[\u0026#39;num_reactions\u0026#39;] = len(reactions) results[\u0026#39;metabolite_degree_dist\u0026#39;] = dict(G.degree()) return results def plot_network_properties(G, title=\u0026#34;Biological Network Properties\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Plot various network properties\u0026#34;\u0026#34;\u0026#34; fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12)) # Degree distribution degrees = [d for n, d in G.degree()] degree_dist = Counter(degrees) k_values = list(degree_dist.keys()) counts = list(degree_dist.values()) ax1.loglog(k_values, counts, \u0026#39;bo\u0026#39;, markersize=8) ax1.set_xlabel(\u0026#39;Degree k\u0026#39;) ax1.set_ylabel(\u0026#39;Count\u0026#39;) ax1.set_title(\u0026#39;Degree Distribution\u0026#39;) ax1.grid(True, alpha=0.3) # Clustering coefficient distribution clustering = nx.clustering(G) clustering_values = list(clustering.values()) ax2.hist(clustering_values, bins=20, alpha=0.7, edgecolor=\u0026#39;black\u0026#39;) ax2.set_xlabel(\u0026#39;Clustering Coefficient\u0026#39;) ax2.set_ylabel(\u0026#39;Count\u0026#39;) ax2.set_title(\u0026#39;Clustering Coefficient Distribution\u0026#39;) ax2.grid(True, alpha=0.3) # Betweenness centrality betweenness = nx.betweenness_centrality(G) betweenness_values = list(betweenness.values()) ax3.hist(betweenness_values, bins=20, alpha=0.7, edgecolor=\u0026#39;black\u0026#39;) ax3.set_xlabel(\u0026#39;Betweenness Centrality\u0026#39;) ax3.set_ylabel(\u0026#39;Count\u0026#39;) ax3.set_title(\u0026#39;Betweenness Centrality Distribution\u0026#39;) ax3.grid(True, alpha=0.3) # Network visualization (simplified) pos = nx.spring_layout(G, k=1, iterations=50) nx.draw(G, pos, ax=ax4, node_size=50, node_color=\u0026#39;lightblue\u0026#39;, edge_color=\u0026#39;gray\u0026#39;, alpha=0.6) ax4.set_title(\u0026#39;Network Visualization\u0026#39;) ax4.axis(\u0026#39;off\u0026#39;) plt.suptitle(title) plt.tight_layout() plt.show() # Example: Analyze a protein-protein interaction network G = nx.barabasi_albert_graph(1000, 3) results = analyze_biological_network(G, \u0026#34;ppi\u0026#34;) print(\u0026#34;Biological Network Analysis Results:\u0026#34;) for key, value in results.items(): if key != \u0026#39;essentiality_scores\u0026#39;: print(f\u0026#34;{key}: {value}\u0026#34;) # Plot network properties plot_network_properties(G, \u0026#34;Protein-Protein Interaction Network\u0026#34;) Key Takeaways Scale-free structure: Most biological networks exhibit power-law degree distributions Functional modules: Biological networks show strong community structure Robustness: Networks are robust to random failures but vulnerable to targeted attacks Evolutionary constraints: Network structure reflects evolutionary pressures Multi-scale organization: Biological networks operate at multiple length scales Dynamic behavior: Network properties change over time Applications: Network analysis provides insights into disease mechanisms and drug design References Newman, M. E. J. (2010). Networks: An Introduction. Oxford University Press. Barabási, A. L., \u0026amp; Oltvai, Z. N. (2004). Network biology: understanding the cell\u0026rsquo;s functional organization. Nature Reviews Genetics, 5(2), 101-113. Sporns, O. (2011). Networks of the Brain. MIT Press. Pimm, S. L. (1982). Food Webs. Chapman and Hall. Biological networks provide insights into the complex interactions that govern life, with important applications in understanding disease mechanisms and designing biomimetic materials.\n","permalink":"https://Linlin-resh.github.io/posts/reading-notes-newman-ch5/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eChapter 5 of Newman\u0026rsquo;s \u003cem\u003eNetworks: An Introduction\u003c/em\u003e explores \u003cstrong\u003ebiological networks\u003c/strong\u003e - the complex webs of interactions that govern life at multiple scales. From molecular interactions to ecosystem dynamics, biological networks exhibit unique structural properties that reflect evolutionary pressures and functional constraints.\u003c/p\u003e\n\u003ch2 id=\"51-protein-protein-interaction-networks\"\u003e5.1 Protein-Protein Interaction Networks\u003c/h2\u003e\n\u003ch3 id=\"network-structure\"\u003eNetwork Structure\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eProtein-protein interaction (PPI) networks\u003c/strong\u003e represent:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eNodes\u003c/strong\u003e: Proteins\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEdges\u003c/strong\u003e: Physical interactions between proteins\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eScale\u003c/strong\u003e: ~20,000 human proteins, ~100,000 interactions\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eProperties\u003c/strong\u003e: Undirected, weighted (interaction strength)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"topological-properties\"\u003eTopological Properties\u003c/h3\u003e\n\u003ch4 id=\"degree-distribution\"\u003eDegree Distribution\u003c/h4\u003e\n\u003cp\u003ePPI networks exhibit \u003cstrong\u003epower-law degree distributions\u003c/strong\u003e:\u003c/p\u003e","title":"Reading Notes: Newman's Networks Chapter 5 - Biological Networks"},{"content":"Introduction Chapter 6 of Newman\u0026rsquo;s Networks: An Introduction establishes the mathematical foundations of network analysis. This chapter provides the essential mathematical tools and concepts needed to understand and analyze complex networks, from basic graph theory to advanced matrix methods.\n6.1 Graph Theory Fundamentals Basic Definitions Graph Components A graph $G = (V, E)$ consists of:\nVertices (nodes): $V = {v_1, v_2, \\ldots, v_n}$ Edges (links): $E = {(v_i, v_j) : v_i, v_j \\in V}$ Order: $n = |V|$ (number of vertices) Size: $m = |E|$ (number of edges) Graph Types Undirected graph: Edges have no direction\nEdge: $(v_i, v_j) = (v_j, v_i)$ Degree: $k_i = |{j : (v_i, v_j) \\in E}|$ Directed graph (digraph): Edges have direction\nEdge: $(v_i, v_j) \\neq (v_j, v_i)$ In-degree: $k_i^{\\text{in}} = |{j : (v_j, v_i) \\in E}|$ Out-degree: $k_i^{\\text{out}} = |{j : (v_i, v_j) \\in E}|$ Weighted graph: Edges have weights\nWeight: $w_{ij} \\in \\mathbb{R}$ for edge $(v_i, v_j)$ Weighted degree: $k_i^w = \\sum_{j} w_{ij}$ Graph Properties Connectivity Path: Sequence of vertices $(v_1, v_2, \\ldots, v_k)$ where $(v_i, v_{i+1}) \\in E$\nWalk: Path where vertices can be repeated\nTrail: Path where edges can be repeated\nCycle: Path where $v_1 = v_k$ and all other vertices are distinct\nConnected graph: Path exists between any two vertices\nStrongly connected: Directed path exists between any two vertices\nDistance and Diameter Distance: $d_{ij} = \\min{\\text{length of path from } i \\text{ to } j}$\nDiameter: $D = \\max_{i,j} d_{ij}$\nAverage path length: $L = \\frac{1}{n(n-1)} \\sum_{i \\neq j} d_{ij}$\nClustering Local clustering coefficient: $$C_i = \\frac{2e_i}{k_i(k_i-1)}$$\nWhere $e_i$ is the number of edges between neighbors of vertex $i$.\nGlobal clustering coefficient: $$C = \\frac{1}{n} \\sum_{i=1}^n C_i$$\nTransitivity: $$T = \\frac{3 \\times \\text{number of triangles}}{\\text{number of connected triples}}$$\n6.2 Matrix Representations Adjacency Matrix Definition For an undirected graph: $$A_{ij} = \\begin{cases} 1 \u0026amp; \\text{if } (v_i, v_j) \\in E \\ 0 \u0026amp; \\text{otherwise} \\end{cases}$$\nFor a directed graph: $$A_{ij} = \\begin{cases} 1 \u0026amp; \\text{if } (v_i, v_j) \\in E \\ 0 \u0026amp; \\text{otherwise} \\end{cases}$$\nProperties Undirected graphs:\nSymmetric: $A = A^T$ Trace: $\\text{tr}(A) = 0$ (no self-loops) Sum of rows: $\\sum_j A_{ij} = k_i$ (degree of vertex $i$) Directed graphs:\nAsymmetric: $A \\neq A^T$ (in general) Row sums: $\\sum_j A_{ij} = k_i^{\\text{out}}$ (out-degree) Column sums: $\\sum_i A_{ij} = k_j^{\\text{in}}$ (in-degree) Powers of Adjacency Matrix $A^2$ interpretation: $$(A^2){ij} = \\sum_k A{ik} A_{kj} = \\text{number of paths of length 2 from } i \\text{ to } j$$\nGeneral power: $$(A^k)_{ij} = \\text{number of paths of length } k \\text{ from } i \\text{ to } j$$\nReachability matrix: $$R = \\sum_{k=1}^{n-1} A^k$$\nWhere $R_{ij} = 1$ if there exists a path from $i$ to $j$.\nLaplacian Matrix Definition Laplacian matrix: $$L = D - A$$\nWhere $D$ is the degree matrix: $$D_{ij} = \\begin{cases} k_i \u0026amp; \\text{if } i = j \\ 0 \u0026amp; \\text{otherwise} \\end{cases}$$\nProperties Eigenvalues:\nSmallest eigenvalue: $\\lambda_1 = 0$ (always) Multiplicity of 0: Number of connected components Second smallest eigenvalue: $\\lambda_2 \u0026gt; 0$ if graph is connected Eigenvectors:\nFirst eigenvector: $\\mathbf{v}_1 = \\frac{1}{\\sqrt{n}} \\mathbf{1}$ (constant vector) Fiedler vector: Second eigenvector (used for graph partitioning) Normalized Laplacian Symmetric normalized Laplacian: $$L_{\\text{sym}} = D^{-1/2} L D^{-1/2} = I - D^{-1/2} A D^{-1/2}$$\nRandom walk normalized Laplacian: $$L_{\\text{rw}} = D^{-1} L = I - D^{-1} A$$\nIncidence Matrix Definition Incidence matrix $B$: $$B_{ie} = \\begin{cases} 1 \u0026amp; \\text{if vertex } i \\text{ is incident to edge } e \\ 0 \u0026amp; \\text{otherwise} \\end{cases}$$\nProperties Dimensions: $n \\times m$ (vertices × edges)\nRank: $\\text{rank}(B) = n - c$ where $c$ is the number of connected components\nLaplacian relationship: $$L = B B^T$$\n6.3 Spectral Graph Theory Eigenvalue Analysis Adjacency Matrix Eigenvalues Spectral radius: $\\rho(A) = \\max_i |\\lambda_i|$\nPerron-Frobenius theorem: For connected graphs, the largest eigenvalue is:\nReal and positive: $\\lambda_1 \u0026gt; 0$ Simple: Multiplicity 1 Positive eigenvector: All components \u0026gt; 0 Laplacian Eigenvalues Spectral gap: $\\lambda_2 - \\lambda_1 = \\lambda_2$\nCheeger\u0026rsquo;s inequality: $$\\frac{\\lambda_2}{2} \\leq h(G) \\leq \\sqrt{2\\lambda_2}$$\nWhere $h(G)$ is the Cheeger constant: $$h(G) = \\min_{S} \\frac{|\\partial S|}{\\min(|S|, |V \\setminus S|)}$$\nGraph Partitioning Spectral Partitioning Algorithm:\nCompute Fiedler vector (second eigenvector of Laplacian) Sort vertices by Fiedler vector values Cut at median value Mathematical foundation: $$\\min_{\\mathbf{x}} \\mathbf{x}^T L \\mathbf{x} \\quad \\text{subject to } \\mathbf{x}^T \\mathbf{1} = 0, \\mathbf{x}^T \\mathbf{x} = n$$\nSolution: Fiedler vector\nRatio Cut Ratio cut: $$\\text{RatioCut}(S, T) = \\frac{\\text{cut}(S, T)}{|S|} + \\frac{\\text{cut}(S, T)}{|T|}$$\nSpectral relaxation: $$\\min_{\\mathbf{x}} \\mathbf{x}^T L \\mathbf{x} \\quad \\text{subject to } \\mathbf{x}^T \\mathbf{1} = 0, \\mathbf{x}^T \\mathbf{x} = n$$\n6.4 Random Walks Transition Matrix Definition Transition matrix: $$P_{ij} = \\frac{A_{ij}}{k_i}$$\nProperties:\nRow stochastic: $\\sum_j P_{ij} = 1$ Eigenvalues: $1 = \\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_n \\geq -1$ Stationary Distribution Stationary distribution: $$\\pi_i = \\frac{k_i}{2m}$$\nVerification: $$\\sum_i \\pi_i P_{ij} = \\sum_i \\frac{k_i}{2m} \\frac{A_{ij}}{k_i} = \\frac{1}{2m} \\sum_i A_{ij} = \\frac{k_j}{2m} = \\pi_j$$\nMixing Time Definition Mixing time: $$\\tau_{\\text{mix}} = \\min{t : \\max_i ||P^t(i, \\cdot) - \\pi||_1 \\leq \\epsilon}$$\nSpectral bound: $$\\tau_{\\text{mix}} \\leq \\frac{1}{1 - \\lambda_2} \\log\\left(\\frac{1}{\\epsilon \\pi_{\\min}}\\right)$$\nWhere $\\pi_{\\min} = \\min_i \\pi_i$.\nPageRank Definition PageRank: $$\\mathbf{PR} = (1-d) \\frac{\\mathbf{1}}{n} + d P^T \\mathbf{PR}$$\nMatrix form: $$\\mathbf{PR} = (1-d) \\frac{\\mathbf{1}}{n} + d \\mathbf{M} \\mathbf{PR}$$\nWhere $\\mathbf{M}$ is the stochastic matrix: $$M_{ij} = \\frac{A_{ji}}{k_j^{\\text{out}}}$$\nPower Iteration Iterative solution: $$\\mathbf{PR}^{(t+1)} = (1-d) \\frac{\\mathbf{1}}{n} + d \\mathbf{M}^T \\mathbf{PR}^{(t)}$$\nConvergence: $$||\\mathbf{PR}^{(t+1)} - \\mathbf{PR}^{(t)}||_1 \u0026lt; \\epsilon$$\n6.5 Centrality Measures Degree Centrality Definition: $$C_D(i) = \\frac{k_i}{n-1}$$\nNormalized: $0 \\leq C_D(i) \\leq 1$\nBetweenness Centrality Definition: $$C_B(i) = \\sum_{s \\neq i \\neq t} \\frac{\\sigma_{st}(i)}{\\sigma_{st}}$$\nWhere:\n$\\sigma_{st}$: Number of shortest paths from $s$ to $t$ $\\sigma_{st}(i)$: Number of shortest paths from $s$ to $t$ passing through $i$ Normalized: $$C_B^{\\text{norm}}(i) = \\frac{C_B(i)}{(n-1)(n-2)/2}$$\nCloseness Centrality Definition: $$C_C(i) = \\frac{n-1}{\\sum_{j \\neq i} d_{ij}}$$\nNormalized: $0 \\leq C_C(i) \\leq 1$\nEigenvector Centrality Definition: $$x_i = \\frac{1}{\\lambda} \\sum_j A_{ij} x_j$$\nMatrix form: $$A \\mathbf{x} = \\lambda \\mathbf{x}$$\nSolution: Principal eigenvector of adjacency matrix\nKatz Centrality Definition: $$x_i = \\alpha \\sum_j A_{ij} x_j + \\beta$$\nMatrix form: $$\\mathbf{x} = \\alpha A \\mathbf{x} + \\beta \\mathbf{1}$$\nSolution: $$\\mathbf{x} = \\beta (I - \\alpha A)^{-1} \\mathbf{1}$$\nConvergence: Requires $\\alpha \u0026lt; \\frac{1}{\\rho(A)}$\n6.6 Applications to Materials Science Network Topology in Materials Atomic Networks Network representation:\nNodes: Atoms Edges: Chemical bonds Weights: Bond strength, distance Mathematical analysis:\nDegree distribution: Coordination number distribution Clustering: Local atomic environment Path length: Atomic connectivity Defect Networks Network properties:\nConnectivity: Defect percolation Clustering: Defect clustering Centrality: Critical defects Mathematical framework: $$P(\\text{percolation}) = 1 - \\exp\\left(-\\frac{\\langle k^2 \\rangle}{\\langle k \\rangle} p\\right)$$\nNetwork-Based Materials Design Structure-Property Relationships Network descriptors:\nAverage degree: $\\langle k \\rangle$ Clustering coefficient: $C$ Path length: $L$ Centrality measures: $C_D, C_B, C_C$ Property prediction: $$P = f(\\langle k \\rangle, C, L, \\ldots)$$\nOptimization Objective function: $$\\min_{\\text{network}} \\sum_i w_i |P_i - P_i^{\\text{target}}|^2$$\nConstraints:\nConnectivity: Network must be connected Degree bounds: $k_{\\min} \\leq k_i \\leq k_{\\max}$ Clustering bounds: $C_{\\min} \\leq C \\leq C_{\\max}$ Code Example: Mathematical Foundations import networkx as nx import numpy as np import matplotlib.pyplot as plt from scipy.linalg import eig from scipy.sparse import csgraph def analyze_graph_matrices(G): \u0026#34;\u0026#34;\u0026#34;Analyze graph using matrix representations\u0026#34;\u0026#34;\u0026#34; # Adjacency matrix A = nx.adjacency_matrix(G).toarray() n = A.shape[0] # Degree matrix degrees = [d for n, d in G.degree()] D = np.diag(degrees) # Laplacian matrix L = D - A # Normalized Laplacian D_sqrt_inv = np.diag(1.0 / np.sqrt(degrees)) L_norm = D_sqrt_inv @ L @ D_sqrt_inv # Eigenvalue analysis eigenvals_A, eigenvecs_A = eig(A) eigenvals_L, eigenvecs_L = eig(L) eigenvals_L_norm, eigenvecs_L_norm = eig(L_norm) # Sort eigenvalues eigenvals_A = np.real(eigenvals_A) eigenvals_L = np.real(eigenvals_L) eigenvals_L_norm = np.real(eigenvals_L_norm) idx_A = np.argsort(eigenvals_A)[::-1] idx_L = np.argsort(eigenvals_L) idx_L_norm = np.argsort(eigenvals_L_norm) eigenvals_A = eigenvals_A[idx_A] eigenvals_L = eigenvals_L[idx_L] eigenvals_L_norm = eigenvals_L_norm[idx_L_norm] return { \u0026#39;adjacency_matrix\u0026#39;: A, \u0026#39;degree_matrix\u0026#39;: D, \u0026#39;laplacian_matrix\u0026#39;: L, \u0026#39;normalized_laplacian\u0026#39;: L_norm, \u0026#39;adjacency_eigenvalues\u0026#39;: eigenvals_A, \u0026#39;laplacian_eigenvalues\u0026#39;: eigenvals_L, \u0026#39;normalized_laplacian_eigenvalues\u0026#39;: eigenvals_L_norm } def compute_centrality_measures(G): \u0026#34;\u0026#34;\u0026#34;Compute various centrality measures\u0026#34;\u0026#34;\u0026#34; # Degree centrality degree_centrality = nx.degree_centrality(G) # Betweenness centrality betweenness_centrality = nx.betweenness_centrality(G) # Closeness centrality closeness_centrality = nx.closeness_centrality(G) # Eigenvector centrality eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000) # Katz centrality katz_centrality = nx.katz_centrality(G, alpha=0.1) # PageRank pagerank = nx.pagerank(G, alpha=0.85) return { \u0026#39;degree_centrality\u0026#39;: degree_centrality, \u0026#39;betweenness_centrality\u0026#39;: betweenness_centrality, \u0026#39;closeness_centrality\u0026#39;: closeness_centrality, \u0026#39;eigenvector_centrality\u0026#39;: eigenvector_centrality, \u0026#39;katz_centrality\u0026#39;: katz_centrality, \u0026#39;pagerank\u0026#39;: pagerank } def analyze_spectral_properties(G): \u0026#34;\u0026#34;\u0026#34;Analyze spectral properties of the graph\u0026#34;\u0026#34;\u0026#34; # Matrix analysis matrix_results = analyze_graph_matrices(G) # Spectral gap laplacian_eigenvals = matrix_results[\u0026#39;laplacian_eigenvalues\u0026#39;] spectral_gap = laplacian_eigenvals[1] - laplacian_eigenvals[0] # Cheeger constant (approximation) cheeger_constant = spectral_gap / 2 # Mixing time (approximation) if spectral_gap \u0026gt; 0: mixing_time = 1 / spectral_gap else: mixing_time = float(\u0026#39;inf\u0026#39;) # Fiedler vector L = matrix_results[\u0026#39;laplacian_matrix\u0026#39;] eigenvals, eigenvecs = eig(L) eigenvals = np.real(eigenvals) eigenvecs = np.real(eigenvecs) idx = np.argsort(eigenvals) fiedler_vector = eigenvecs[:, idx[1]] return { \u0026#39;spectral_gap\u0026#39;: spectral_gap, \u0026#39;cheeger_constant\u0026#39;: cheeger_constant, \u0026#39;mixing_time\u0026#39;: mixing_time, \u0026#39;fiedler_vector\u0026#39;: fiedler_vector } def plot_spectral_analysis(G, title=\u0026#34;Spectral Analysis\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Plot spectral analysis results\u0026#34;\u0026#34;\u0026#34; matrix_results = analyze_graph_matrices(G) fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12)) # Adjacency matrix eigenvalues eigenvals_A = matrix_results[\u0026#39;adjacency_eigenvalues\u0026#39;] ax1.plot(eigenvals_A, \u0026#39;bo-\u0026#39;, markersize=8) ax1.set_xlabel(\u0026#39;Index\u0026#39;) ax1.set_ylabel(\u0026#39;Eigenvalue\u0026#39;) ax1.set_title(\u0026#39;Adjacency Matrix Eigenvalues\u0026#39;) ax1.grid(True, alpha=0.3) # Laplacian matrix eigenvalues eigenvals_L = matrix_results[\u0026#39;laplacian_eigenvalues\u0026#39;] ax2.plot(eigenvals_L, \u0026#39;ro-\u0026#39;, markersize=8) ax2.set_xlabel(\u0026#39;Index\u0026#39;) ax2.set_ylabel(\u0026#39;Eigenvalue\u0026#39;) ax2.set_title(\u0026#39;Laplacian Matrix Eigenvalues\u0026#39;) ax2.grid(True, alpha=0.3) # Normalized Laplacian eigenvalues eigenvals_L_norm = matrix_results[\u0026#39;normalized_laplacian_eigenvalues\u0026#39;] ax3.plot(eigenvals_L_norm, \u0026#39;go-\u0026#39;, markersize=8) ax3.set_xlabel(\u0026#39;Index\u0026#39;) ax3.set_ylabel(\u0026#39;Eigenvalue\u0026#39;) ax3.set_title(\u0026#39;Normalized Laplacian Eigenvalues\u0026#39;) ax3.grid(True, alpha=0.3) # Fiedler vector spectral_results = analyze_spectral_properties(G) fiedler_vector = spectral_results[\u0026#39;fiedler_vector\u0026#39;] ax4.plot(fiedler_vector, \u0026#39;mo-\u0026#39;, markersize=8) ax4.set_xlabel(\u0026#39;Node Index\u0026#39;) ax4.set_ylabel(\u0026#39;Fiedler Vector Value\u0026#39;) ax4.set_title(\u0026#39;Fiedler Vector (Second Eigenvector)\u0026#39;) ax4.grid(True, alpha=0.3) plt.suptitle(title) plt.tight_layout() plt.show() # Example: Analyze a scale-free network G = nx.barabasi_albert_graph(100, 3) # Matrix analysis matrix_results = analyze_graph_matrices(G) print(\u0026#34;Matrix Analysis Results:\u0026#34;) print(f\u0026#34;Spectral gap: {matrix_results[\u0026#39;laplacian_eigenvalues\u0026#39;][1] - matrix_results[\u0026#39;laplacian_eigenvalues\u0026#39;][0]:.4f}\u0026#34;) # Centrality measures centrality_results = compute_centrality_measures(G) print(\u0026#34;\\nCentrality Measures (top 5 nodes):\u0026#34;) for measure, values in centrality_results.items(): sorted_nodes = sorted(values.items(), key=lambda x: x[1], reverse=True)[:5] print(f\u0026#34;{measure}: {sorted_nodes}\u0026#34;) # Spectral analysis spectral_results = analyze_spectral_properties(G) print(f\u0026#34;\\nSpectral Properties:\u0026#34;) print(f\u0026#34;Cheeger constant: {spectral_results[\u0026#39;cheeger_constant\u0026#39;]:.4f}\u0026#34;) print(f\u0026#34;Mixing time: {spectral_results[\u0026#39;mixing_time\u0026#39;]:.4f}\u0026#34;) # Plot results plot_spectral_analysis(G, \u0026#34;Scale-Free Network Spectral Analysis\u0026#34;) Key Takeaways Matrix representations: Adjacency, Laplacian, and incidence matrices provide different views of network structure Spectral analysis: Eigenvalues and eigenvectors reveal important network properties Centrality measures: Different measures capture different aspects of node importance Random walks: Provide insights into network dynamics and mixing Graph partitioning: Spectral methods are effective for community detection Mathematical rigor: Solid mathematical foundation enables advanced network analysis Applications: Matrix methods are essential for materials science applications References Newman, M. E. J. (2010). Networks: An Introduction. Oxford University Press. Chung, F. R. K. (1997). Spectral Graph Theory. American Mathematical Society. Godsil, C., \u0026amp; Royle, G. (2001). Algebraic Graph Theory. Springer. Horn, R. A., \u0026amp; Johnson, C. R. (2012). Matrix Analysis. Cambridge University Press. The mathematical foundations of network analysis provide the essential tools for understanding complex systems, with direct applications to materials science and engineering.\n","permalink":"https://Linlin-resh.github.io/posts/reading-notes-newman-ch6/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eChapter 6 of Newman\u0026rsquo;s \u003cem\u003eNetworks: An Introduction\u003c/em\u003e establishes the \u003cstrong\u003emathematical foundations\u003c/strong\u003e of network analysis. This chapter provides the essential mathematical tools and concepts needed to understand and analyze complex networks, from basic graph theory to advanced matrix methods.\u003c/p\u003e\n\u003ch2 id=\"61-graph-theory-fundamentals\"\u003e6.1 Graph Theory Fundamentals\u003c/h2\u003e\n\u003ch3 id=\"basic-definitions\"\u003eBasic Definitions\u003c/h3\u003e\n\u003ch4 id=\"graph-components\"\u003eGraph Components\u003c/h4\u003e\n\u003cp\u003eA \u003cstrong\u003egraph\u003c/strong\u003e $G = (V, E)$ consists of:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eVertices (nodes)\u003c/strong\u003e: $V = {v_1, v_2, \\ldots, v_n}$\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eEdges (links)\u003c/strong\u003e: $E = {(v_i, v_j) : v_i, v_j \\in V}$\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOrder\u003c/strong\u003e: $n = |V|$ (number of vertices)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSize\u003c/strong\u003e: $m = |E|$ (number of edges)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"graph-types\"\u003eGraph Types\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eUndirected graph\u003c/strong\u003e: Edges have no direction\u003c/p\u003e","title":"Reading Notes: Newman's Networks Chapter 6 - Mathematical Foundations"},{"content":"Introduction Chapter 7 of Newman\u0026rsquo;s Networks: An Introduction focuses on network measures and metrics - the quantitative tools used to characterize and compare networks. This chapter provides a comprehensive overview of the mathematical foundations and practical applications of network measurement.\n7.1 Basic Network Measures Degree-Based Measures Degree Distribution Definition: The probability distribution of node degrees.\nMathematical formulation: $$P(k) = \\frac{\\text{Number of nodes with degree } k}{n}$$\nProperties:\nNormalization: $\\sum_{k=0}^{\\infty} P(k) = 1$ Average degree: $\\langle k \\rangle = \\sum_{k=0}^{\\infty} k P(k)$ Second moment: $\\langle k^2 \\rangle = \\sum_{k=0}^{\\infty} k^2 P(k)$ Degree Moments First moment (average degree): $$\\langle k \\rangle = \\frac{2m}{n}$$\nSecond moment: $$\\langle k^2 \\rangle = \\frac{1}{n} \\sum_{i=1}^n k_i^2$$\nDegree variance: $$\\sigma_k^2 = \\langle k^2 \\rangle - \\langle k \\rangle^2$$\nCoefficient of variation: $$CV = \\frac{\\sigma_k}{\\langle k \\rangle}$$\nDegree Correlation Assortativity coefficient: $$r = \\frac{\\sum_{ij} (A_{ij} - \\frac{k_i k_j}{2m}) k_i k_j}{\\sum_{ij} (k_i \\delta_{ij} - \\frac{k_i k_j}{2m}) k_i k_j}$$\nInterpretation:\n$r \u0026gt; 0$: Assortative (high-degree nodes connect to high-degree nodes) $r \u0026lt; 0$: Disassortative (high-degree nodes connect to low-degree nodes) $r = 0$: No correlation Clustering Measures Local Clustering Coefficient Definition: $$C_i = \\frac{2e_i}{k_i(k_i-1)}$$\nWhere $e_i$ is the number of edges between neighbors of node $i$.\nProperties:\n$0 \\leq C_i \\leq 1$ $C_i = 1$: Complete subgraph $C_i = 0$: No triangles Global Clustering Coefficient Average clustering: $$C = \\frac{1}{n} \\sum_{i=1}^n C_i$$\nTransitivity: $$T = \\frac{3 \\times \\text{number of triangles}}{\\text{number of connected triples}}$$\nRelationship: $$C = T \\quad \\text{for undirected graphs}$$\nClustering Distribution Clustering coefficient distribution: $$P(C) = \\frac{\\text{Number of nodes with clustering } C}{n}$$\nAverage clustering by degree: $$C(k) = \\frac{1}{n_k} \\sum_{i: k_i = k} C_i$$\nWhere $n_k$ is the number of nodes with degree $k$.\nPath Length Measures Shortest Path Length Definition: $d_{ij} = \\min{\\text{length of path from } i \\text{ to } j}$\nProperties:\n$d_{ii} = 0$ (self-distance) $d_{ij} = d_{ji}$ (symmetric for undirected graphs) $d_{ij} \\leq d_{ik} + d_{kj}$ (triangle inequality) Average Path Length Definition: $$L = \\frac{1}{n(n-1)} \\sum_{i \\neq j} d_{ij}$$\nFor disconnected graphs: $$L = \\frac{1}{n(n-1)} \\sum_{i \\neq j} \\frac{d_{ij}}{R_{ij}}$$\nWhere $R_{ij} = 1$ if nodes $i$ and $j$ are connected, 0 otherwise.\nNetwork Diameter Definition: $D = \\max_{i,j} d_{ij}$\nEffective diameter: 90th percentile of path length distribution\nMathematical formulation: $$D_{90} = \\min{d : P(d_{ij} \\leq d) \\geq 0.9}$$\n7.2 Centrality Measures Degree Centrality Definition: $$C_D(i) = \\frac{k_i}{n-1}$$\nNormalized: $0 \\leq C_D(i) \\leq 1$\nInterpretation: Direct connections\nBetweenness Centrality Definition: $$C_B(i) = \\sum_{s \\neq i \\neq t} \\frac{\\sigma_{st}(i)}{\\sigma_{st}}$$\nWhere:\n$\\sigma_{st}$: Number of shortest paths from $s$ to $t$ $\\sigma_{st}(i)$: Number of shortest paths from $s$ to $t$ passing through $i$ Normalized: $$C_B^{\\text{norm}}(i) = \\frac{C_B(i)}{(n-1)(n-2)/2}$$\nWeighted version: $$C_B^w(i) = \\sum_{s \\neq i \\neq t} \\frac{\\sigma_{st}^w(i)}{\\sigma_{st}^w} \\cdot \\frac{w_{st}}{w_{\\max}}$$\nCloseness Centrality Definition: $$C_C(i) = \\frac{n-1}{\\sum_{j \\neq i} d_{ij}}$$\nNormalized: $0 \\leq C_C(i) \\leq 1$\nHarmonic closeness: $$C_H(i) = \\sum_{j \\neq i} \\frac{1}{d_{ij}}$$\nEigenvector Centrality Definition: $$x_i = \\frac{1}{\\lambda} \\sum_j A_{ij} x_j$$\nMatrix form: $$A \\mathbf{x} = \\lambda \\mathbf{x}$$\nSolution: Principal eigenvector of adjacency matrix\nWeighted version: $$x_i = \\frac{1}{\\lambda} \\sum_j w_{ij} x_j$$\nKatz Centrality Definition: $$x_i = \\alpha \\sum_j A_{ij} x_j + \\beta$$\nMatrix form: $$\\mathbf{x} = \\alpha A \\mathbf{x} + \\beta \\mathbf{1}$$\nSolution: $$\\mathbf{x} = \\beta (I - \\alpha A)^{-1} \\mathbf{1}$$\nConvergence: Requires $\\alpha \u0026lt; \\frac{1}{\\rho(A)}$\nPageRank Definition: $$\\mathbf{PR} = (1-d) \\frac{\\mathbf{1}}{n} + d P^T \\mathbf{PR}$$\nMatrix form: $$\\mathbf{PR} = (1-d) \\frac{\\mathbf{1}}{n} + d \\mathbf{M} \\mathbf{PR}$$\nWhere $\\mathbf{M}$ is the stochastic matrix: $$M_{ij} = \\frac{A_{ji}}{k_j^{\\text{out}}}$$\n7.3 Community Structure Measures Modularity Definition: $$Q = \\frac{1}{2m} \\sum_{ij} \\left[ A_{ij} - \\frac{k_i k_j}{2m} \\right] \\delta(c_i, c_j)$$\nWhere:\n$A_{ij}$: Adjacency matrix $k_i, k_j$: Degrees of nodes $i, j$ $c_i, c_j$: Community assignments $\\delta(c_i, c_j)$: Kronecker delta Properties:\n$Q \\in [-1, 1]$ $Q \u0026gt; 0$: More edges within communities than expected by chance $Q = 0$: Random network structure $Q \u0026lt; 0$: Fewer edges within communities than expected Modularity Resolution Limit Resolution limit: $$Q_{\\text{max}} = 1 - \\frac{1}{2m} \\sum_{c} \\frac{k_c^2}{2m}$$\nWhere $k_c$ is the total degree of community $c$.\nSmall communities: May not be detected if $k_c \u0026lt; \\sqrt{2m}$\nConductance Definition: $$\\phi(S) = \\frac{\\text{cut}(S, \\bar{S})}{\\min(\\text{vol}(S), \\text{vol}(\\bar{S}))}$$\nWhere:\n$\\text{cut}(S, \\bar{S})$: Number of edges between $S$ and $\\bar{S}$ $\\text{vol}(S)$: Total degree of nodes in $S$ Properties:\n$0 \\leq \\phi(S) \\leq 1$ $\\phi(S) = 0$: No edges between communities $\\phi(S) = 1$: Maximum possible edges between communities Normalized Cut Definition: $$\\text{NCut}(S, T) = \\frac{\\text{cut}(S, T)}{\\text{vol}(S)} + \\frac{\\text{cut}(S, T)}{\\text{vol}(T)}$$\nSpectral relaxation: $$\\min_{\\mathbf{x}} \\mathbf{x}^T L \\mathbf{x} \\quad \\text{subject to } \\mathbf{x}^T \\mathbf{1} = 0, \\mathbf{x}^T \\mathbf{x} = n$$\n7.4 Network Robustness Measures Connectivity Measures Node Connectivity Definition: Minimum number of nodes that must be removed to disconnect the graph.\nMathematical formulation: $$\\kappa(G) = \\min_{S \\subset V} |S| \\quad \\text{subject to } G \\setminus S \\text{ is disconnected}$$\nEdge Connectivity Definition: Minimum number of edges that must be removed to disconnect the graph.\nMathematical formulation: $$\\lambda(G) = \\min_{F \\subset E} |F| \\quad \\text{subject to } G \\setminus F \\text{ is disconnected}$$\nMenger\u0026rsquo;s Theorem Node connectivity: $$\\kappa(G) = \\min_{s,t} \\text{number of node-disjoint paths from } s \\text{ to } t$$\nEdge connectivity: $$\\lambda(G) = \\min_{s,t} \\text{number of edge-disjoint paths from } s \\text{ to } t$$\nRobustness to Attacks Random Failure Robustness measure: $$R_{\\text{random}} = \\frac{1}{n} \\sum_{i=1}^n \\frac{S_i}{n}$$\nWhere $S_i$ is the size of the largest component after removing node $i$.\nTargeted Attack Robustness measure: $$R_{\\text{targeted}} = \\frac{1}{n} \\sum_{i=1}^n \\frac{S_i^{(i)}}{n}$$\nWhere $S_i^{(i)}$ is the size of the largest component after removing the $i$-th most important node.\nAttack Tolerance Tolerance threshold: $$p_c = 1 - \\frac{1}{\\kappa - 1}$$\nWhere $\\kappa = \\frac{\\langle k^2 \\rangle}{\\langle k \\rangle}$ is the degree ratio.\nPercolation Analysis Bond Percolation Percolation probability: $$P(p) = 1 - \\sum_{k=0}^{\\infty} P(k) (1-p)^k$$\nCritical threshold: $$p_c = \\frac{1}{\\kappa - 1}$$\nSite Percolation Percolation probability: $$P(p) = 1 - \\sum_{k=0}^{\\infty} P(k) (1-p)^k$$\nCritical threshold: $$p_c = \\frac{1}{\\kappa - 1}$$\n7.5 Network Comparison Measures Structural Similarity Graph Edit Distance Definition: Minimum number of operations to transform one graph into another.\nOperations:\nNode insertion/deletion: Cost $c_n$ Edge insertion/deletion: Cost $c_e$ Node relabeling: Cost $c_r$ Mathematical formulation: $$d_{GED}(G_1, G_2) = \\min_{\\text{operations}} \\sum_{i} c_i$$\nGraph Isomorphism Definition: Two graphs are isomorphic if there exists a bijection between their vertex sets that preserves adjacency.\nMathematical formulation: $$G_1 \\cong G_2 \\iff \\exists \\phi : V_1 \\to V_2 \\text{ such that } (u,v) \\in E_1 \\iff (\\phi(u), \\phi(v)) \\in E_2$$\nStatistical Similarity Degree Distribution Similarity Kolmogorov-Smirnov test: $$D = \\max_k |P_1(k) - P_2(k)|$$\nP-value: $$p = P(D \\geq D_{\\text{observed}})$$\nClustering Similarity Clustering coefficient difference: $$\\Delta C = |C_1 - C_2|$$\nClustering distribution similarity: $$D_C = \\max_C |P_1(C) - P_2(C)|$$\nNetwork Alignment Global Alignment Objective function: $$\\max_{\\phi} \\sum_{i,j} A_{1,ij} A_{2,\\phi(i)\\phi(j)}$$\nConstraints:\n$\\phi$ is a bijection $A_{1,ij}$ is the adjacency matrix of graph 1 $A_{2,\\phi(i)\\phi(j)}$ is the adjacency matrix of graph 2 Local Alignment Objective function: $$\\max_{S_1, S_2} \\sum_{i \\in S_1, j \\in S_2} A_{1,ij} A_{2,\\phi(i)\\phi(j)}$$\nConstraints:\n$S_1 \\subset V_1, S_2 \\subset V_2$ $|S_1| = |S_2|$ $\\phi : S_1 \\to S_2$ is a bijection 7.6 Applications to Materials Science Network Descriptors for Materials Structural Descriptors Coordination number distribution: $$P(k) = \\frac{\\text{Number of atoms with coordination } k}{n}$$\nBond length distribution: $$P(r) = \\frac{\\text{Number of bonds with length } r}{m}$$\nBond angle distribution: $$P(\\theta) = \\frac{\\text{Number of bond angles } \\theta}{n_{\\text{angles}}}$$\nTopological Descriptors Clustering coefficient: Local atomic environment\nPath length: Atomic connectivity\nCentrality measures: Critical atoms/defects\nProperty Prediction Structure-Property Relationships Linear regression: $$P = \\alpha_0 + \\alpha_1 \\langle k \\rangle + \\alpha_2 C + \\alpha_3 L + \\epsilon$$\nNonlinear regression: $$P = f(\\langle k \\rangle, C, L, \\ldots) + \\epsilon$$\nMachine learning: $$P = \\text{ML}(\\text{network features}) + \\epsilon$$\nFeature Selection Correlation analysis: $$r_{ij} = \\frac{\\text{Cov}(X_i, X_j)}{\\sqrt{\\text{Var}(X_i) \\text{Var}(X_j)}}$$\nMutual information: $$I(X_i; X_j) = \\sum_{x_i, x_j} P(x_i, x_j) \\log \\frac{P(x_i, x_j)}{P(x_i) P(x_j)}$$\nCode Example: Network Measures import networkx as nx import numpy as np import matplotlib.pyplot as plt from collections import Counter from scipy import stats def compute_network_measures(G): \u0026#34;\u0026#34;\u0026#34;Compute comprehensive network measures\u0026#34;\u0026#34;\u0026#34; # Basic statistics n = G.number_of_nodes() m = G.number_of_edges() density = 2 * m / (n * (n - 1)) # Degree measures degrees = [d for n, d in G.degree()] degree_dist = Counter(degrees) # Degree moments avg_degree = np.mean(degrees) degree_variance = np.var(degrees) degree_std = np.std(degrees) coefficient_of_variation = degree_std / avg_degree if avg_degree \u0026gt; 0 else 0 # Degree correlation (assortativity) assortativity = nx.degree_assortativity_coefficient(G) # Clustering measures local_clustering = nx.clustering(G) global_clustering = nx.average_clustering(G) transitivity = nx.transitivity(G) # Path length measures if nx.is_connected(G): avg_path_length = nx.average_shortest_path_length(G) diameter = nx.diameter(G) else: # Analyze largest component largest_cc = max(nx.connected_components(G), key=len) subgraph = G.subgraph(largest_cc) avg_path_length = nx.average_shortest_path_length(subgraph) diameter = nx.diameter(subgraph) # Centrality measures degree_centrality = nx.degree_centrality(G) betweenness_centrality = nx.betweenness_centrality(G) closeness_centrality = nx.closeness_centrality(G) eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000) # Connectivity measures node_connectivity = nx.node_connectivity(G) edge_connectivity = nx.edge_connectivity(G) # Community structure try: communities = nx.community.greedy_modularity_communities(G) modularity = nx.community.modularity(G, communities) except: communities = None modularity = None return { \u0026#39;basic_stats\u0026#39;: { \u0026#39;nodes\u0026#39;: n, \u0026#39;edges\u0026#39;: m, \u0026#39;density\u0026#39;: density }, \u0026#39;degree_measures\u0026#39;: { \u0026#39;avg_degree\u0026#39;: avg_degree, \u0026#39;degree_variance\u0026#39;: degree_variance, \u0026#39;degree_std\u0026#39;: degree_std, \u0026#39;coefficient_of_variation\u0026#39;: coefficient_of_variation, \u0026#39;assortativity\u0026#39;: assortativity }, \u0026#39;clustering_measures\u0026#39;: { \u0026#39;global_clustering\u0026#39;: global_clustering, \u0026#39;transitivity\u0026#39;: transitivity, \u0026#39;local_clustering\u0026#39;: local_clustering }, \u0026#39;path_measures\u0026#39;: { \u0026#39;avg_path_length\u0026#39;: avg_path_length, \u0026#39;diameter\u0026#39;: diameter }, \u0026#39;centrality_measures\u0026#39;: { \u0026#39;degree_centrality\u0026#39;: degree_centrality, \u0026#39;betweenness_centrality\u0026#39;: betweenness_centrality, \u0026#39;closeness_centrality\u0026#39;: closeness_centrality, \u0026#39;eigenvector_centrality\u0026#39;: eigenvector_centrality }, \u0026#39;connectivity_measures\u0026#39;: { \u0026#39;node_connectivity\u0026#39;: node_connectivity, \u0026#39;edge_connectivity\u0026#39;: edge_connectivity }, \u0026#39;community_measures\u0026#39;: { \u0026#39;modularity\u0026#39;: modularity, \u0026#39;num_communities\u0026#39;: len(communities) if communities else None } } def compare_networks(G1, G2): \u0026#34;\u0026#34;\u0026#34;Compare two networks using various measures\u0026#34;\u0026#34;\u0026#34; # Compute measures for both networks measures1 = compute_network_measures(G1) measures2 = compute_network_measures(G2) # Compare basic statistics basic_comparison = {} for key in measures1[\u0026#39;basic_stats\u0026#39;]: val1 = measures1[\u0026#39;basic_stats\u0026#39;][key] val2 = measures2[\u0026#39;basic_stats\u0026#39;][key] basic_comparison[key] = { \u0026#39;G1\u0026#39;: val1, \u0026#39;G2\u0026#39;: val2, \u0026#39;difference\u0026#39;: abs(val1 - val2), \u0026#39;relative_difference\u0026#39;: abs(val1 - val2) / max(val1, val2) if max(val1, val2) \u0026gt; 0 else 0 } # Compare degree distributions degrees1 = [d for n, d in G1.degree()] degrees2 = [d for n, d in G2.degree()] # Kolmogorov-Smirnov test ks_statistic, ks_pvalue = stats.ks_2samp(degrees1, degrees2) # Compare clustering distributions clustering1 = list(nx.clustering(G1).values()) clustering2 = list(nx.clustering(G2).values()) clustering_ks_statistic, clustering_ks_pvalue = stats.ks_2samp(clustering1, clustering2) return { \u0026#39;basic_comparison\u0026#39;: basic_comparison, \u0026#39;degree_distribution_ks\u0026#39;: { \u0026#39;statistic\u0026#39;: ks_statistic, \u0026#39;pvalue\u0026#39;: ks_pvalue }, \u0026#39;clustering_distribution_ks\u0026#39;: { \u0026#39;statistic\u0026#39;: clustering_ks_statistic, \u0026#39;pvalue\u0026#39;: clustering_ks_pvalue } } def plot_network_measures(G, title=\u0026#34;Network Measures\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Plot various network measures\u0026#34;\u0026#34;\u0026#34; measures = compute_network_measures(G) fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12)) # Degree distribution degrees = [d for n, d in G.degree()] degree_dist = Counter(degrees) k_values = list(degree_dist.keys()) counts = list(degree_dist.values()) ax1.loglog(k_values, counts, \u0026#39;bo\u0026#39;, markersize=8) ax1.set_xlabel(\u0026#39;Degree k\u0026#39;) ax1.set_ylabel(\u0026#39;Count\u0026#39;) ax1.set_title(\u0026#39;Degree Distribution\u0026#39;) ax1.grid(True, alpha=0.3) # Clustering coefficient distribution clustering = list(measures[\u0026#39;clustering_measures\u0026#39;][\u0026#39;local_clustering\u0026#39;].values()) ax2.hist(clustering, bins=20, alpha=0.7, edgecolor=\u0026#39;black\u0026#39;) ax2.set_xlabel(\u0026#39;Clustering Coefficient\u0026#39;) ax2.set_ylabel(\u0026#39;Count\u0026#39;) ax2.set_title(\u0026#39;Clustering Coefficient Distribution\u0026#39;) ax2.grid(True, alpha=0.3) # Centrality measures comparison centrality_measures = measures[\u0026#39;centrality_measures\u0026#39;] centrality_names = list(centrality_measures.keys()) centrality_values = [list(values.values()) for values in centrality_measures.values()] # Plot top 10 nodes for each centrality measure for i, (name, values) in enumerate(centrality_measures.items()): sorted_values = sorted(values.items(), key=lambda x: x[1], reverse=True)[:10] nodes, scores = zip(*sorted_values) ax3.plot(nodes, scores, \u0026#39;o-\u0026#39;, label=name, markersize=6) ax3.set_xlabel(\u0026#39;Node Rank\u0026#39;) ax3.set_ylabel(\u0026#39;Centrality Score\u0026#39;) ax3.set_title(\u0026#39;Centrality Measures Comparison\u0026#39;) ax3.legend() ax3.grid(True, alpha=0.3) # Network visualization pos = nx.spring_layout(G, k=1, iterations=50) nx.draw(G, pos, ax=ax4, node_size=50, node_color=\u0026#39;lightblue\u0026#39;, edge_color=\u0026#39;gray\u0026#39;, alpha=0.6) ax4.set_title(\u0026#39;Network Visualization\u0026#39;) ax4.axis(\u0026#39;off\u0026#39;) plt.suptitle(title) plt.tight_layout() plt.show() # Example: Analyze a scale-free network G = nx.barabasi_albert_graph(1000, 3) measures = compute_network_measures(G) print(\u0026#34;Network Measures Analysis:\u0026#34;) for category, values in measures.items(): print(f\u0026#34;\\n{category.upper()}:\u0026#34;) for key, value in values.items(): if isinstance(value, dict): print(f\u0026#34; {key}: {len(value)} values\u0026#34;) else: print(f\u0026#34; {key}: {value}\u0026#34;) # Plot network measures plot_network_measures(G, \u0026#34;Scale-Free Network Measures\u0026#34;) Key Takeaways Comprehensive measurement: Network measures capture different aspects of network structure Mathematical foundations: Each measure has a solid mathematical basis Comparative analysis: Measures enable quantitative network comparison Robustness assessment: Network measures help evaluate system robustness Community detection: Modularity and related measures identify community structure Centrality importance: Different centrality measures capture different aspects of importance Applications: Network measures are essential for materials science applications References Newman, M. E. J. (2010). Networks: An Introduction. Oxford University Press. Wasserman, S., \u0026amp; Faust, K. (1994). Social Network Analysis: Methods and Applications. Cambridge University Press. Brandes, U., \u0026amp; Erlebach, T. (2005). Network Analysis: Methodological Foundations. Springer. Costa, L. da F., et al. (2007). Characterization of complex networks: A survey of measurements. Advances in Physics, 56(1), 167-242. Network measures and metrics provide the quantitative tools needed to characterize, compare, and analyze complex networks, with important applications in materials science and engineering.\n","permalink":"https://Linlin-resh.github.io/posts/reading-notes-newman-ch7/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eChapter 7 of Newman\u0026rsquo;s \u003cem\u003eNetworks: An Introduction\u003c/em\u003e focuses on \u003cstrong\u003enetwork measures and metrics\u003c/strong\u003e - the quantitative tools used to characterize and compare networks. This chapter provides a comprehensive overview of the mathematical foundations and practical applications of network measurement.\u003c/p\u003e\n\u003ch2 id=\"71-basic-network-measures\"\u003e7.1 Basic Network Measures\u003c/h2\u003e\n\u003ch3 id=\"degree-based-measures\"\u003eDegree-Based Measures\u003c/h3\u003e\n\u003ch4 id=\"degree-distribution\"\u003eDegree Distribution\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eDefinition\u003c/strong\u003e: The probability distribution of node degrees.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMathematical formulation\u003c/strong\u003e:\n$$P(k) = \\frac{\\text{Number of nodes with degree } k}{n}$$\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eProperties\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eNormalization\u003c/strong\u003e: $\\sum_{k=0}^{\\infty} P(k) = 1$\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eAverage degree\u003c/strong\u003e: $\\langle k \\rangle = \\sum_{k=0}^{\\infty} k P(k)$\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSecond moment\u003c/strong\u003e: $\\langle k^2 \\rangle = \\sum_{k=0}^{\\infty} k^2 P(k)$\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"degree-moments\"\u003eDegree Moments\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eFirst moment (average degree)\u003c/strong\u003e:\n$$\\langle k \\rangle = \\frac{2m}{n}$$\u003c/p\u003e","title":"Reading Notes: Newman's Networks Chapter 7 - Network Measures and Metrics"},{"content":"Introduction Chapter 8 of Newman\u0026rsquo;s Networks: An Introduction introduces random graph theory - the mathematical foundation for understanding network structure through probabilistic models. This chapter covers the classic Erdős-Rényi model, configuration models, and their applications to understanding real-world networks.\n8.1 Erdős-Rényi Model Model Definition The Erdős-Rényi random graph $G(n,p)$ consists of:\nn vertices p probability of edge existence between any two vertices Expected number of edges: $E[m] = \\binom{n}{2}p = \\frac{n(n-1)}{2}p$ Mathematical Properties Degree Distribution For large networks, the degree distribution follows a Poisson distribution:\n$$P(k) = \\frac{\\langle k \\rangle^k e^{-\\langle k \\rangle}}{k!}$$\nWhere $\\langle k \\rangle = (n-1)p$ is the average degree.\nProperties of Poisson Distribution Mean: $\\langle k \\rangle = (n-1)p$\nVariance: $\\sigma_k^2 = \\langle k \\rangle$\nStandard deviation: $\\sigma_k = \\sqrt{\\langle k \\rangle}$\nCoefficient of variation: $CV = \\frac{\\sigma_k}{\\langle k \\rangle} = \\frac{1}{\\sqrt{\\langle k \\rangle}}$\nClustering Coefficient Expected clustering coefficient: $$C = p = \\frac{\\langle k \\rangle}{n-1}$$\nFor large networks: $C \\to 0$ as $n \\to \\infty$\nPath Length Average path length: $$L \\sim \\frac{\\ln n}{\\ln \\langle k \\rangle}$$\nFor large networks: $L \\sim \\ln n$ (logarithmic growth)\nPhase Transitions Giant Component Emergence A phase transition occurs at the critical threshold:\n$$p_c = \\frac{1}{n-1} \\approx \\frac{1}{n}$$\nBelow threshold ($p \u0026lt; p_c$):\nOnly small, isolated components exist Largest component size: $O(\\ln n)$ At threshold ($p = p_c$):\nGiant component emerges Largest component size: $O(n^{2/3})$ Above threshold ($p \u0026gt; p_c$):\nGiant component dominates Largest component size: $O(n)$ Mathematical Analysis Component size distribution: $$P(s) \\sim s^{-3/2} e^{-s/s_0}$$\nWhere $s_0$ is the characteristic size.\nGiant component size: $$S = n \\left[1 - \\sum_{k=0}^{\\infty} P(k) u^k\\right]$$\nWhere $u$ satisfies: $$u = \\sum_{k=0}^{\\infty} \\frac{k P(k) u^{k-1}}{\\langle k \\rangle}$$\nConnectivity Probability of Connectivity For large networks: $$P(\\text{connected}) \\approx \\exp(-n e^{-\\langle k \\rangle})$$\nCritical threshold for connectivity: $$p_c^{\\text{conn}} = \\frac{\\ln n}{n}$$\nDiameter Expected diameter: $$D \\sim \\frac{\\ln n}{\\ln \\langle k \\rangle}$$\nFor large networks: $D \\sim \\ln n$\n8.2 Configuration Model Model Definition The configuration model generates random graphs with a prescribed degree sequence:\nInput: Degree sequence ${k_1, k_2, \\ldots, k_n}$ Constraint: $\\sum_{i=1}^n k_i$ must be even Method: Randomly connect stubs (half-edges) Mathematical Properties Degree Distribution Given degree sequence: $P(k) = \\frac{n_k}{n}$\nWhere $n_k$ is the number of nodes with degree $k$.\nExpected Number of Edges Total number of edges: $$m = \\frac{1}{2} \\sum_{i=1}^n k_i = \\frac{n \\langle k \\rangle}{2}$$\nClustering Coefficient Expected clustering coefficient: $$C = \\frac{\\langle k^2 \\rangle - \\langle k \\rangle}{n \\langle k \\rangle^2}$$\nFor large networks: $C \\to 0$ as $n \\to \\infty$\nPath Length Average path length: $$L \\sim \\frac{\\ln n}{\\ln \\langle k \\rangle}$$\nFor scale-free networks: $L \\sim \\frac{\\ln n}{\\ln \\ln n}$\nDegree Correlation Assortativity Degree correlation function: $$k_{nn}(k) = \\frac{\\sum_{k\u0026rsquo;} k\u0026rsquo; P(k\u0026rsquo;|k)}{\\sum_{k\u0026rsquo;} P(k\u0026rsquo;|k)}$$\nFor configuration model: $$k_{nn}(k) = \\frac{\\langle k^2 \\rangle}{\\langle k \\rangle}$$\nAssortativity coefficient: $$r = \\frac{\\langle k^2 \\rangle - \\langle k \\rangle^2}{\\langle k^3 \\rangle - \\langle k \\rangle^2}$$\nGiant Component Existence Condition Giant component exists if: $$\\frac{\\langle k^2 \\rangle}{\\langle k \\rangle} \u0026gt; 2$$\nCritical threshold: $$\\frac{\\langle k^2 \\rangle}{\\langle k \\rangle} = 2$$\nSize of Giant Component Giant component size: $$S = n \\left[1 - \\sum_{k=0}^{\\infty} P(k) u^k\\right]$$\nWhere $u$ satisfies: $$u = \\sum_{k=0}^{\\infty} \\frac{k P(k) u^{k-1}}{\\langle k \\rangle}$$\n8.3 Random Graph Variants Directed Random Graphs Model Definition Directed Erdős-Rényi model $G(n,p)$:\nn vertices p probability of directed edge from any vertex to any other Expected number of edges: $E[m] = n(n-1)p$ Degree Distributions In-degree distribution: $$P_{\\text{in}}(k) = \\frac{\\langle k_{\\text{in}} \\rangle^k e^{-\\langle k_{\\text{in}} \\rangle}}{k!}$$\nOut-degree distribution: $$P_{\\text{out}}(k) = \\frac{\\langle k_{\\text{out}} \\rangle^k e^{-\\langle k_{\\text{out}} \\rangle}}{k!}$$\nWhere $\\langle k_{\\text{in}} \\rangle = \\langle k_{\\text{out}} \\rangle = (n-1)p$.\nWeighted Random Graphs Model Definition Weighted random graph:\nn vertices p probability of edge existence w weight distribution for existing edges Weight Distribution Exponential weights: $$P(w) = \\lambda e^{-\\lambda w}$$\nPower-law weights: $$P(w) \\sim w^{-\\alpha}$$\nNetwork Properties Weighted degree: $$k_i^w = \\sum_{j} w_{ij}$$\nWeighted clustering: $$C_i^w = \\frac{\\sum_{j,k} w_{ij} w_{jk} w_{ki}}{\\sum_{j,k} w_{ij} w_{jk}}$$\nBipartite Random Graphs Model Definition Bipartite random graph $G(n_1, n_2, p)$:\nn₁ vertices in partition 1 n₂ vertices in partition 2 p probability of edge between partitions Degree Distributions Partition 1 degree distribution: $$P_1(k) = \\frac{(n_2 p)^k e^{-n_2 p}}{k!}$$\nPartition 2 degree distribution: $$P_2(k) = \\frac{(n_1 p)^k e^{-n_1 p}}{k!}$$\n8.4 Random Graph Algorithms Generation Algorithms Erdős-Rényi Generation Algorithm:\nInitialize empty graph with $n$ vertices For each pair $(i,j)$ where $i \u0026lt; j$: Generate random number $r \\sim U(0,1)$ If $r \u0026lt; p$, add edge $(i,j)$ Time complexity: $O(n^2)$\nConfiguration Model Generation Algorithm:\nCreate $k_i$ stubs for each vertex $i$ Randomly pair stubs Connect paired stubs to form edges Time complexity: $O(m)$ where $m$ is the number of edges\nSampling Algorithms Random Walk Sampling Algorithm:\nStart at random vertex At each step, move to random neighbor Repeat for desired number of steps Stationary distribution: $$\\pi_i = \\frac{k_i}{2m}$$\nMetropolis-Hastings Sampling Algorithm:\nStart at random vertex Propose move to neighbor with probability $1/k_i$ Accept with probability $\\min(1, k_i/k_j)$ Stationary distribution: Uniform\n8.5 Applications to Materials Science Percolation Theory Bond Percolation Model: Random removal of edges with probability $1-p$\nPercolation probability: $$P(p) = 1 - \\sum_{k=0}^{\\infty} P(k) (1-p)^k$$\nCritical threshold: $$p_c = \\frac{1}{\\kappa - 1}$$\nWhere $\\kappa = \\frac{\\langle k^2 \\rangle}{\\langle k \\rangle}$.\nSite Percolation Model: Random removal of vertices with probability $1-p$\nPercolation probability: $$P(p) = 1 - \\sum_{k=0}^{\\infty} P(k) (1-p)^k$$\nCritical threshold: $$p_c = \\frac{1}{\\kappa - 1}$$\nNanowire Networks Network Formation Random geometric graph:\nNodes: Nanowire junctions Edges: Nanowire segments Probability: $p = \\rho \\pi r^2$ where $\\rho$ is density and $r$ is interaction radius Electrical Properties Conductivity: $$\\sigma \\sim (p - p_c)^t$$\nWhere $t \\approx 2.0$ is the conductivity exponent.\nCritical density: $$\\rho_c = \\frac{1}{\\pi r^2 (\\kappa - 1)}$$\nDefect Networks Defect Clustering Clustering coefficient: $$C = \\frac{\\langle k^2 \\rangle - \\langle k \\rangle}{n \\langle k \\rangle^2}$$\nFor defect networks: $C \u0026gt; 0$ indicates clustering\nPercolation Threshold Defect percolation: $$p_c = \\frac{1}{\\kappa - 1}$$\nFor materials: $p_c$ determines critical defect concentration\nCode Example: Random Graph Analysis import networkx as nx import numpy as np import matplotlib.pyplot as plt from collections import Counter from scipy import stats def generate_erdos_renyi(n, p): \u0026#34;\u0026#34;\u0026#34;Generate Erdős-Rényi random graph\u0026#34;\u0026#34;\u0026#34; G = nx.erdos_renyi_graph(n, p) return G def generate_configuration_model(degree_sequence): \u0026#34;\u0026#34;\u0026#34;Generate configuration model random graph\u0026#34;\u0026#34;\u0026#34; G = nx.configuration_model(degree_sequence) # Remove self-loops and parallel edges G = nx.Graph(G) G.remove_edges_from(nx.selfloop_edges(G)) return G def analyze_random_graph(G, model_type=\u0026#34;ER\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Analyze random graph properties\u0026#34;\u0026#34;\u0026#34; # Basic statistics n = G.number_of_nodes() m = G.number_of_edges() density = 2 * m / (n * (n - 1)) # Degree analysis degrees = [d for n, d in G.degree()] degree_dist = Counter(degrees) # Poisson fitting for ER model if model_type == \u0026#34;ER\u0026#34;: avg_degree = np.mean(degrees) poisson_dist = stats.poisson(avg_degree) # Chi-square test observed = list(degree_dist.values()) expected = [poisson_dist.pmf(k) * n for k in degree_dist.keys()] chi2_stat, chi2_pvalue = stats.chisquare(observed, expected) else: chi2_stat = chi2_pvalue = None # Clustering analysis clustering = nx.average_clustering(G) # Path length analysis if nx.is_connected(G): avg_path_length = nx.average_shortest_path_length(G) diameter = nx.diameter(G) else: # Analyze largest component largest_cc = max(nx.connected_components(G), key=len) subgraph = G.subgraph(largest_cc) avg_path_length = nx.average_shortest_path_length(subgraph) diameter = nx.diameter(subgraph) # Giant component analysis components = list(nx.connected_components(G)) component_sizes = [len(comp) for comp in components] giant_component_size = max(component_sizes) if component_sizes else 0 giant_component_fraction = giant_component_size / n if n \u0026gt; 0 else 0 # Phase transition analysis avg_degree = np.mean(degrees) phase_transition_threshold = 1.0 # For ER model return { \u0026#39;model_type\u0026#39;: model_type, \u0026#39;nodes\u0026#39;: n, \u0026#39;edges\u0026#39;: m, \u0026#39;density\u0026#39;: density, \u0026#39;avg_degree\u0026#39;: avg_degree, \u0026#39;degree_variance\u0026#39;: np.var(degrees), \u0026#39;clustering\u0026#39;: clustering, \u0026#39;avg_path_length\u0026#39;: avg_path_length, \u0026#39;diameter\u0026#39;: diameter, \u0026#39;giant_component_size\u0026#39;: giant_component_size, \u0026#39;giant_component_fraction\u0026#39;: giant_component_fraction, \u0026#39;num_components\u0026#39;: len(components), \u0026#39;phase_transition_threshold\u0026#39;: phase_transition_threshold, \u0026#39;above_threshold\u0026#39;: avg_degree \u0026gt; phase_transition_threshold, \u0026#39;chi2_statistic\u0026#39;: chi2_stat, \u0026#39;chi2_pvalue\u0026#39;: chi2_pvalue } def analyze_phase_transition(n_values, p_values): \u0026#34;\u0026#34;\u0026#34;Analyze phase transition in random graphs\u0026#34;\u0026#34;\u0026#34; results = [] for n in n_values: for p in p_values: G = generate_erdos_renyi(n, p) analysis = analyze_random_graph(G, \u0026#34;ER\u0026#34;) results.append({ \u0026#39;n\u0026#39;: n, \u0026#39;p\u0026#39;: p, \u0026#39;giant_component_fraction\u0026#39;: analysis[\u0026#39;giant_component_fraction\u0026#39;], \u0026#39;avg_path_length\u0026#39;: analysis[\u0026#39;avg_path_length\u0026#39;], \u0026#39;clustering\u0026#39;: analysis[\u0026#39;clustering\u0026#39;] }) return results def plot_degree_distribution(G, model_type=\u0026#34;ER\u0026#34;, title=\u0026#34;Degree Distribution\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Plot degree distribution with theoretical comparison\u0026#34;\u0026#34;\u0026#34; degrees = [d for n, d in G.degree()] degree_dist = Counter(degrees) k_values = list(degree_dist.keys()) counts = list(degree_dist.values()) plt.figure(figsize=(10, 6)) # Plot observed distribution plt.bar(k_values, counts, alpha=0.7, label=\u0026#39;Observed\u0026#39;, color=\u0026#39;blue\u0026#39;) # Plot theoretical distribution if model_type == \u0026#34;ER\u0026#34;: avg_degree = np.mean(degrees) poisson_dist = stats.poisson(avg_degree) theoretical = [poisson_dist.pmf(k) * len(degrees) for k in k_values] plt.plot(k_values, theoretical, \u0026#39;ro-\u0026#39;, label=\u0026#39;Poisson\u0026#39;, markersize=8) plt.xlabel(\u0026#39;Degree k\u0026#39;) plt.ylabel(\u0026#39;Count\u0026#39;) plt.title(f\u0026#39;{title} - {model_type} Model\u0026#39;) plt.legend() plt.grid(True, alpha=0.3) plt.show() def plot_phase_transition(results): \u0026#34;\u0026#34;\u0026#34;Plot phase transition results\u0026#34;\u0026#34;\u0026#34; # Group results by n n_values = sorted(set(r[\u0026#39;n\u0026#39;] for r in results)) fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6)) for n in n_values: n_results = [r for r in results if r[\u0026#39;n\u0026#39;] == n] p_values = [r[\u0026#39;p\u0026#39;] for r in n_results] giant_fractions = [r[\u0026#39;giant_component_fraction\u0026#39;] for r in n_results] ax1.plot(p_values, giant_fractions, \u0026#39;o-\u0026#39;, label=f\u0026#39;n={n}\u0026#39;, markersize=8) ax1.set_xlabel(\u0026#39;Probability p\u0026#39;) ax1.set_ylabel(\u0026#39;Giant Component Fraction\u0026#39;) ax1.set_title(\u0026#39;Phase Transition: Giant Component\u0026#39;) ax1.legend() ax1.grid(True, alpha=0.3) for n in n_values: n_results = [r for r in results if r[\u0026#39;n\u0026#39;] == n] p_values = [r[\u0026#39;p\u0026#39;] for r in n_results] path_lengths = [r[\u0026#39;avg_path_length\u0026#39;] for r in n_results] ax2.plot(p_values, path_lengths, \u0026#39;o-\u0026#39;, label=f\u0026#39;n={n}\u0026#39;, markersize=8) ax2.set_xlabel(\u0026#39;Probability p\u0026#39;) ax2.set_ylabel(\u0026#39;Average Path Length\u0026#39;) ax2.set_title(\u0026#39;Phase Transition: Path Length\u0026#39;) ax2.legend() ax2.grid(True, alpha=0.3) plt.tight_layout() plt.show() # Example: Analyze Erdős-Rényi random graph n, p = 1000, 0.01 G_ER = generate_erdos_renyi(n, p) analysis_ER = analyze_random_graph(G_ER, \u0026#34;ER\u0026#34;) print(\u0026#34;Erdős-Rényi Random Graph Analysis:\u0026#34;) for key, value in analysis_ER.items(): print(f\u0026#34;{key}: {value}\u0026#34;) # Plot degree distribution plot_degree_distribution(G_ER, \u0026#34;ER\u0026#34;, \u0026#34;Erdős-Rényi Random Graph\u0026#34;) # Example: Analyze configuration model degree_sequence = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3] * 100 # Regular graph G_CM = generate_configuration_model(degree_sequence) analysis_CM = analyze_random_graph(G_CM, \u0026#34;CM\u0026#34;) print(\u0026#34;\\nConfiguration Model Analysis:\u0026#34;) for key, value in analysis_CM.items(): print(f\u0026#34;{key}: {value}\u0026#34;) # Plot degree distribution plot_degree_distribution(G_CM, \u0026#34;CM\u0026#34;, \u0026#34;Configuration Model\u0026#34;) # Example: Phase transition analysis n_values = [100, 500, 1000] p_values = np.linspace(0.001, 0.1, 20) phase_results = analyze_phase_transition(n_values, p_values) # Plot phase transition plot_phase_transition(phase_results) Key Takeaways Random graph models: Provide null models for understanding network structure Phase transitions: Critical thresholds determine network connectivity Degree distributions: Poisson for ER model, prescribed for configuration model Giant component: Emerges at critical threshold, dominates above threshold Mathematical analysis: Rigorous theory enables prediction of network properties Applications: Random graphs provide insights into materials science phenomena Percolation theory: Direct application to nanowire networks and defect systems References Newman, M. E. J. (2010). Networks: An Introduction. Oxford University Press. Erdős, P., \u0026amp; Rényi, A. (1959). On random graphs. Publicationes Mathematicae, 6, 290-297. Molloy, M., \u0026amp; Reed, B. (1995). A critical point for random graphs with a given degree sequence. Random Structures \u0026amp; Algorithms, 6(2-3), 161-180. Stauffer, D., \u0026amp; Aharony, A. (1994). Introduction to Percolation Theory. Taylor \u0026amp; Francis. Random graph theory provides the mathematical foundation for understanding network structure and behavior, with important applications in materials science and percolation phenomena.\n","permalink":"https://Linlin-resh.github.io/posts/reading-notes-newman-ch8/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eChapter 8 of Newman\u0026rsquo;s \u003cem\u003eNetworks: An Introduction\u003c/em\u003e introduces \u003cstrong\u003erandom graph theory\u003c/strong\u003e - the mathematical foundation for understanding network structure through probabilistic models. This chapter covers the classic Erdős-Rényi model, configuration models, and their applications to understanding real-world networks.\u003c/p\u003e\n\u003ch2 id=\"81-erdős-rényi-model\"\u003e8.1 Erdős-Rényi Model\u003c/h2\u003e\n\u003ch3 id=\"model-definition\"\u003eModel Definition\u003c/h3\u003e\n\u003cp\u003eThe \u003cstrong\u003eErdős-Rényi random graph\u003c/strong\u003e $G(n,p)$ consists of:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003en\u003c/strong\u003e vertices\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ep\u003c/strong\u003e probability of edge existence between any two vertices\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eExpected number of edges\u003c/strong\u003e: $E[m] = \\binom{n}{2}p = \\frac{n(n-1)}{2}p$\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"mathematical-properties\"\u003eMathematical Properties\u003c/h3\u003e\n\u003ch4 id=\"degree-distribution\"\u003eDegree Distribution\u003c/h4\u003e\n\u003cp\u003eFor large networks, the degree distribution follows a \u003cstrong\u003ePoisson distribution\u003c/strong\u003e:\u003c/p\u003e","title":"Reading Notes: Newman's Networks Chapter 8 - Random Graphs"},{"content":"Introduction Chapter 9 of Newman\u0026rsquo;s Networks: An Introduction explores network models - mathematical frameworks for generating networks with specific structural properties. This chapter covers small-world networks, scale-free networks, and other models that capture the essential features of real-world networks.\n9.1 Small-World Networks Watts-Strogatz Model Model Definition The Watts-Strogatz model generates networks with:\nHigh clustering coefficient (like regular lattices) Short average path length (like random graphs) Small-world property: $L \\sim \\ln n$ and $C \\gg C_{\\text{random}}$ Algorithm Start with regular lattice: $n$ nodes, each connected to $k$ nearest neighbors Rewire edges: With probability $p$, move each edge to a random new location Result: Interpolates between regular lattice ($p=0$) and random graph ($p=1$) Mathematical Properties Clustering coefficient: $$C(p) = \\frac{3(k-2)}{4(k-1)}(1-p)^3$$\nAverage path length: $$L(p) \\sim \\frac{n}{k} f(pkn)$$\nWhere $f(x)$ is a scaling function.\nSmall-world coefficient: $$S = \\frac{C/C_{\\text{random}}}{L/L_{\\text{random}}}$$\nPhase Transition Clustering Transition At $p=0$: $C = \\frac{3(k-2)}{4(k-1)}$ (regular lattice)\nAt $p=1$: $C \\approx \\frac{k}{n}$ (random graph)\nTransition point: $p \\sim \\frac{1}{n}$\nPath Length Transition At $p=0$: $L \\sim \\frac{n}{k}$ (regular lattice)\nAt $p=1$: $L \\sim \\frac{\\ln n}{\\ln k}$ (random graph)\nTransition point: $p \\sim \\frac{1}{n}$\nReal-World Applications Social Networks Six degrees of separation: Milgram\u0026rsquo;s experiment shows $L \\sim 6$\nHigh clustering: People tend to form dense local groups\nSmall-world property: $S \\gg 1$ for most social networks\nNeural Networks Brain connectivity: High local clustering, short global paths\nFunctional modules: Dense local connections within modules\nGlobal efficiency: Short paths between distant brain regions\n9.2 Scale-Free Networks Barabási-Albert Model Model Definition The Barabási-Albert model generates networks with:\nPower-law degree distribution: $P(k) \\sim k^{-\\gamma}$ Preferential attachment: New nodes prefer to connect to high-degree nodes Growth: Network grows by adding new nodes over time Algorithm Start: Small initial network (e.g., complete graph with $m_0$ nodes) Add node: At each step, add one new node Connect: New node connects to $m$ existing nodes Preferential attachment: Probability of connecting to node $i$ is proportional to $k_i$ Mathematical Analysis Degree distribution: $$P(k) = \\frac{2m^2}{k^3} \\sim k^{-3}$$\nExponent: $\\gamma = 3$ (universal for BA model)\nAverage degree: $\\langle k \\rangle = 2m$\nSecond moment: $\\langle k^2 \\rangle = \\infty$ (diverges)\nGeneralized Preferential Attachment Linear Preferential Attachment Attachment probability: $$\\Pi(k_i) = \\frac{k_i + a}{\\sum_j (k_j + a)}$$\nWhere $a$ is the initial attractiveness.\nDegree distribution: $$P(k) \\sim k^{-\\gamma}$$\nWhere $\\gamma = 3 + \\frac{a}{m}$.\nNonlinear Preferential Attachment Attachment probability: $$\\Pi(k_i) = \\frac{k_i^{\\alpha}}{\\sum_j k_j^{\\alpha}}$$\nDegree distribution:\n$\\alpha \u0026lt; 1$: Exponential distribution $\\alpha = 1$: Power-law with $\\gamma = 3$ $\\alpha \u0026gt; 1$: \u0026ldquo;Winner-takes-all\u0026rdquo; (gelation) Real-World Examples Internet AS-level network: $\\gamma \\approx 2.2$\nRouter-level network: $\\gamma \\approx 2.5$\nGrowth mechanism: New ISPs prefer to connect to well-connected ISPs\nWorld Wide Web In-degree distribution: $\\gamma \\approx 2.1$\nOut-degree distribution: $\\gamma \\approx 2.7$\nGrowth mechanism: New pages prefer to link to popular pages\nScientific Collaboration Degree distribution: $\\gamma \\approx 2.1$\nGrowth mechanism: New researchers prefer to collaborate with established researchers\n9.3 Other Network Models Configuration Model Model Definition Configuration model generates random graphs with:\nPrescribed degree sequence: ${k_1, k_2, \\ldots, k_n}$ Random connections: Edges are randomly placed between stubs No degree correlation: Degrees are uncorrelated Mathematical Properties Degree distribution: $P(k) = \\frac{n_k}{n}$\nClustering coefficient: $$C = \\frac{\\langle k^2 \\rangle - \\langle k \\rangle}{n \\langle k \\rangle^2}$$\nGiant component condition: $$\\frac{\\langle k^2 \\rangle}{\\langle k \\rangle} \u0026gt; 2$$\nExponential Random Graph Models Model Definition Exponential random graph models (ERGMs) generate networks with:\nPrescribed statistics: Mean degree, clustering, etc. Maximum entropy: Most random network consistent with constraints Exponential family: $P(G) = \\frac{1}{Z} \\exp(\\sum_i \\theta_i S_i(G))$ Mathematical Formulation Probability distribution: $$P(G) = \\frac{1}{Z} \\exp\\left(\\sum_i \\theta_i S_i(G)\\right)$$\nWhere:\n$Z$: Partition function $\\theta_i$: Parameters $S_i(G)$: Network statistics Partition function: $$Z = \\sum_G \\exp\\left(\\sum_i \\theta_i S_i(G)\\right)$$\nGeometric Random Graphs Model Definition Geometric random graph $G(n, r)$:\nn vertices placed randomly in space r connection radius Edge: $(i,j)$ exists if $d_{ij} \\leq r$ Mathematical Properties Expected degree: $$\\langle k \\rangle = (n-1) \\pi r^2$$\nClustering coefficient: $$C = \\frac{3}{4} - \\frac{3}{4\\pi} \\approx 0.586$$\nPath length: $L \\sim \\frac{1}{r}$ (for large $n$)\nHierarchical Networks Model Definition Hierarchical networks have:\nTree structure: Hierarchical organization Local clustering: Dense connections within levels Global efficiency: Short paths between distant nodes Mathematical Properties Degree distribution: $P(k) \\sim k^{-\\gamma}$ with $\\gamma = \\ln 3/\\ln 2 \\approx 1.585$\nClustering coefficient: $C(k) \\sim k^{-1}$\nPath length: $L \\sim \\ln n$\n9.4 Network Evolution Models Growing Networks Model Definition Growing networks evolve by:\nAdding nodes: New nodes join over time Adding edges: New edges form between existing nodes Removing nodes/edges: Nodes or edges may be removed Mathematical Analysis Master equation: $$\\frac{\\partial P(k,t)}{\\partial t} = \\frac{1}{t} \\left[P(k-1,t) - P(k,t)\\right] + \\delta_{k,1}$$\nSolution: $P(k,t) \\sim k^{-\\gamma}$ with $\\gamma = 3$\nAging and Preferential Attachment Model Definition Aging model combines:\nPreferential attachment: High-degree nodes attract more connections Aging: Older nodes become less attractive over time Mathematical Formulation Attachment probability: $$\\Pi(k_i, t_i) = \\frac{k_i e^{-\\alpha(t-t_i)}}{\\sum_j k_j e^{-\\alpha(t-t_j)}}$$\nWhere:\n$t_i$: Time when node $i$ was added $\\alpha$: Aging parameter Degree distribution: $$P(k) \\sim k^{-\\gamma} e^{-\\beta k}$$\nWhere $\\gamma$ and $\\beta$ depend on $\\alpha$.\nFitness Models Model Definition Fitness model assigns:\nFitness values: $\\eta_i$ for each node $i$ Attachment probability: $\\Pi(k_i, \\eta_i) = \\frac{\\eta_i k_i}{\\sum_j \\eta_j k_j}$ Mathematical Analysis Degree distribution: $$P(k) \\sim k^{-\\gamma} \\int \\eta^{\\gamma-1} \\rho(\\eta) , d\\eta$$\nWhere $\\rho(\\eta)$ is the fitness distribution.\n9.5 Applications to Materials Science Nanowire Network Formation Growth Model Nanowire growth can be modeled as:\nNodes: Nanowire junctions Edges: Nanowire segments Growth: New nanowires prefer to connect to existing junctions Mathematical model: $$\\Pi(k_i) = \\frac{k_i + a}{\\sum_j (k_j + a)}$$\nWhere $a$ represents the intrinsic connectivity of new junctions.\nPercolation Properties Critical density: $$\\rho_c = \\frac{1}{\\pi r^2 (\\kappa - 1)}$$\nWhere $\\kappa = \\frac{\\langle k^2 \\rangle}{\\langle k \\rangle}$.\nConductivity: $$\\sigma \\sim (\\rho - \\rho_c)^t$$\nWhere $t \\approx 2.0$ is the conductivity exponent.\nDefect Network Evolution Defect Clustering Defect clustering can be modeled using:\nPreferential attachment: Defects prefer to form near existing defects Aging: Old defects become less active over time Mathematical model: $$\\Pi(k_i, t_i) = \\frac{k_i e^{-\\alpha(t-t_i)}}{\\sum_j k_j e^{-\\alpha(t-t_j)}}$$\nPhase Transitions Defect percolation: $$P(\\text{percolation}) = 1 - \\sum_{k=0}^{\\infty} P(k) (1-p)^k$$\nCritical threshold: $$p_c = \\frac{1}{\\kappa - 1}$$\nSelf-Assembling Materials Network Formation Self-assembly can be modeled as:\nNodes: Building blocks (molecules, particles) Edges: Interactions between building blocks Growth: New building blocks join existing structures Mathematical model: $$\\Pi(k_i) = \\frac{k_i^{\\alpha}}{\\sum_j k_j^{\\alpha}}$$\nWhere $\\alpha$ controls the strength of preferential attachment.\nCode Example: Network Models import networkx as nx import numpy as np import matplotlib.pyplot as plt from collections import Counter def generate_watts_strogatz(n, k, p): \u0026#34;\u0026#34;\u0026#34;Generate Watts-Strogatz small-world network\u0026#34;\u0026#34;\u0026#34; G = nx.watts_strogatz_graph(n, k, p) return G def generate_barabasi_albert(n, m): \u0026#34;\u0026#34;\u0026#34;Generate Barabási-Albert scale-free network\u0026#34;\u0026#34;\u0026#34; G = nx.barabasi_albert_graph(n, m) return G def generate_configuration_model(degree_sequence): \u0026#34;\u0026#34;\u0026#34;Generate configuration model network\u0026#34;\u0026#34;\u0026#34; G = nx.configuration_model(degree_sequence) # Remove self-loops and parallel edges G = nx.Graph(G) G.remove_edges_from(nx.selfloop_edges(G)) return G def generate_geometric_random_graph(n, r): \u0026#34;\u0026#34;\u0026#34;Generate geometric random graph\u0026#34;\u0026#34;\u0026#34; # Generate random positions positions = np.random.uniform(0, 1, (n, 2)) # Create graph G = nx.Graph() G.add_nodes_from(range(n)) # Add edges based on distance for i in range(n): for j in range(i+1, n): dist = np.linalg.norm(positions[i] - positions[j]) if dist \u0026lt;= r: G.add_edge(i, j) return G def analyze_network_model(G, model_type=\u0026#34;generic\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Analyze network model properties\u0026#34;\u0026#34;\u0026#34; # Basic statistics n = G.number_of_nodes() m = G.number_of_edges() density = 2 * m / (n * (n - 1)) # Degree analysis degrees = [d for n, d in G.degree()] degree_dist = Counter(degrees) # Power-law fitting if len(degree_dist) \u0026gt; 1: k_values = list(degree_dist.keys()) counts = list(degree_dist.values()) if len(k_values) \u0026gt; 1: log_k = np.log(k_values[1:]) # Skip k=0 log_counts = np.log(counts[1:]) if len(log_k) \u0026gt; 1: gamma = -np.polyfit(log_k, log_counts, 1)[0] else: gamma = None else: gamma = None else: gamma = None # Clustering analysis clustering = nx.average_clustering(G) local_clustering = nx.clustering(G) # Path length analysis if nx.is_connected(G): avg_path_length = nx.average_shortest_path_length(G) diameter = nx.diameter(G) else: # Analyze largest component largest_cc = max(nx.connected_components(G), key=len) subgraph = G.subgraph(largest_cc) avg_path_length = nx.average_shortest_path_length(subgraph) diameter = nx.diameter(subgraph) # Small-world coefficient if n \u0026gt; 1: # Compare to random network p = density random_G = nx.erdos_renyi_graph(n, p) random_clustering = nx.average_clustering(random_G) random_path_length = nx.average_shortest_path_length(random_G) if nx.is_connected(random_G) else float(\u0026#39;inf\u0026#39;) small_world_coeff = (clustering / random_clustering) / (avg_path_length / random_path_length) else: small_world_coeff = None return { \u0026#39;model_type\u0026#39;: model_type, \u0026#39;nodes\u0026#39;: n, \u0026#39;edges\u0026#39;: m, \u0026#39;density\u0026#39;: density, \u0026#39;avg_degree\u0026#39;: np.mean(degrees), \u0026#39;gamma_estimate\u0026#39;: gamma, \u0026#39;clustering\u0026#39;: clustering, \u0026#39;avg_path_length\u0026#39;: avg_path_length, \u0026#39;diameter\u0026#39;: diameter, \u0026#39;small_world_coeff\u0026#39;: small_world_coeff } def compare_network_models(n, models): \u0026#34;\u0026#34;\u0026#34;Compare different network models\u0026#34;\u0026#34;\u0026#34; results = {} for model_name, model_params in models.items(): if model_name == \u0026#34;watts_strogatz\u0026#34;: G = generate_watts_strogatz(n, model_params[\u0026#39;k\u0026#39;], model_params[\u0026#39;p\u0026#39;]) elif model_name == \u0026#34;barabasi_albert\u0026#34;: G = generate_barabasi_albert(n, model_params[\u0026#39;m\u0026#39;]) elif model_name == \u0026#34;configuration_model\u0026#34;: G = generate_configuration_model(model_params[\u0026#39;degree_sequence\u0026#39;]) elif model_name == \u0026#34;geometric_random\u0026#34;: G = generate_geometric_random_graph(n, model_params[\u0026#39;r\u0026#39;]) else: continue analysis = analyze_network_model(G, model_name) results[model_name] = analysis return results def plot_network_models(models, title=\u0026#34;Network Models Comparison\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Plot comparison of network models\u0026#34;\u0026#34;\u0026#34; fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12)) # Degree distributions for model_name, G in models.items(): degrees = [d for n, d in G.degree()] degree_dist = Counter(degrees) k_values = list(degree_dist.keys()) counts = list(degree_dist.values()) ax1.loglog(k_values, counts, \u0026#39;o-\u0026#39;, label=model_name, markersize=6) ax1.set_xlabel(\u0026#39;Degree k\u0026#39;) ax1.set_ylabel(\u0026#39;Count\u0026#39;) ax1.set_title(\u0026#39;Degree Distributions\u0026#39;) ax1.legend() ax1.grid(True, alpha=0.3) # Clustering vs Path Length for model_name, G in models.items(): clustering = nx.average_clustering(G) if nx.is_connected(G): path_length = nx.average_shortest_path_length(G) else: largest_cc = max(nx.connected_components(G), key=len) subgraph = G.subgraph(largest_cc) path_length = nx.average_shortest_path_length(subgraph) ax2.scatter(path_length, clustering, label=model_name, s=100) ax2.set_xlabel(\u0026#39;Average Path Length\u0026#39;) ax2.set_ylabel(\u0026#39;Clustering Coefficient\u0026#39;) ax2.set_title(\u0026#39;Clustering vs Path Length\u0026#39;) ax2.legend() ax2.grid(True, alpha=0.3) # Network visualizations for i, (model_name, G) in enumerate(models.items()): ax = ax3 if i \u0026lt; 2 else ax4 if i \u0026gt;= 2: ax = ax4 pos = nx.spring_layout(G, k=1, iterations=50) nx.draw(G, pos, ax=ax, node_size=50, node_color=\u0026#39;lightblue\u0026#39;, edge_color=\u0026#39;gray\u0026#39;, alpha=0.6) ax.set_title(f\u0026#39;{model_name.title()} Network\u0026#39;) ax.axis(\u0026#39;off\u0026#39;) plt.suptitle(title) plt.tight_layout() plt.show() # Example: Compare different network models n = 1000 models = { \u0026#39;watts_strogatz\u0026#39;: generate_watts_strogatz(n, 6, 0.1), \u0026#39;barabasi_albert\u0026#39;: generate_barabasi_albert(n, 3), \u0026#39;configuration_model\u0026#39;: generate_configuration_model([3] * n), \u0026#39;geometric_random\u0026#39;: generate_geometric_random_graph(n, 0.1) } # Analyze models results = compare_network_models(n, { \u0026#39;watts_strogatz\u0026#39;: {\u0026#39;k\u0026#39;: 6, \u0026#39;p\u0026#39;: 0.1}, \u0026#39;barabasi_albert\u0026#39;: {\u0026#39;m\u0026#39;: 3}, \u0026#39;configuration_model\u0026#39;: {\u0026#39;degree_sequence\u0026#39;: [3] * n}, \u0026#39;geometric_random\u0026#39;: {\u0026#39;r\u0026#39;: 0.1} }) print(\u0026#34;Network Models Comparison:\u0026#34;) for model_name, analysis in results.items(): print(f\u0026#34;\\n{model_name.upper()}:\u0026#34;) for key, value in analysis.items(): print(f\u0026#34; {key}: {value}\u0026#34;) # Plot comparison plot_network_models(models, \u0026#34;Network Models Comparison\u0026#34;) Key Takeaways Small-world networks: Combine high clustering with short path lengths Scale-free networks: Exhibit power-law degree distributions through preferential attachment Network models: Provide frameworks for understanding real-world network structure Phase transitions: Critical thresholds determine network properties Growth mechanisms: Preferential attachment and aging affect network evolution Applications: Network models help understand materials science phenomena Mathematical analysis: Rigorous theory enables prediction of network properties References Newman, M. E. J. (2010). Networks: An Introduction. Oxford University Press. Watts, D. J., \u0026amp; Strogatz, S. H. (1998). Collective dynamics of \u0026lsquo;small-world\u0026rsquo; networks. Nature, 393(6684), 440-442. Barabási, A. L., \u0026amp; Albert, R. (1999). Emergence of scaling in random networks. Science, 286(5439), 509-512. Dorogovtsev, S. N., \u0026amp; Mendes, J. F. F. (2003). Evolution of Networks: From Biological Nets to the Internet and WWW. Oxford University Press. Network models provide powerful frameworks for understanding and generating networks with specific structural properties, with important applications in materials science and complex systems.\n","permalink":"https://Linlin-resh.github.io/posts/reading-notes-newman-ch9/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eChapter 9 of Newman\u0026rsquo;s \u003cem\u003eNetworks: An Introduction\u003c/em\u003e explores \u003cstrong\u003enetwork models\u003c/strong\u003e - mathematical frameworks for generating networks with specific structural properties. This chapter covers small-world networks, scale-free networks, and other models that capture the essential features of real-world networks.\u003c/p\u003e\n\u003ch2 id=\"91-small-world-networks\"\u003e9.1 Small-World Networks\u003c/h2\u003e\n\u003ch3 id=\"watts-strogatz-model\"\u003eWatts-Strogatz Model\u003c/h3\u003e\n\u003ch4 id=\"model-definition\"\u003eModel Definition\u003c/h4\u003e\n\u003cp\u003eThe \u003cstrong\u003eWatts-Strogatz model\u003c/strong\u003e generates networks with:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eHigh clustering coefficient\u003c/strong\u003e (like regular lattices)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eShort average path length\u003c/strong\u003e (like random graphs)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSmall-world property\u003c/strong\u003e: $L \\sim \\ln n$ and $C \\gg C_{\\text{random}}$\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"algorithm\"\u003eAlgorithm\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eStart with regular lattice\u003c/strong\u003e: $n$ nodes, each connected to $k$ nearest neighbors\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRewire edges\u003c/strong\u003e: With probability $p$, move each edge to a random new location\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eResult\u003c/strong\u003e: Interpolates between regular lattice ($p=0$) and random graph ($p=1$)\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch4 id=\"mathematical-properties\"\u003eMathematical Properties\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eClustering coefficient\u003c/strong\u003e:\n$$C(p) = \\frac{3(k-2)}{4(k-1)}(1-p)^3$$\u003c/p\u003e","title":"Reading Notes: Newman's Networks Chapter 9 - Network Models"},{"content":"Overview August 2025 has been a productive month with significant progress across multiple research areas. This post summarizes key developments, challenges encountered, and next steps for the coming months.\nMajor Achievements 1. Network Analysis Framework Completed: Developed a comprehensive framework for analyzing partially disordered networks in materials.\nKey Components:\n**Graph cbetween structure and electrical properties Challenges Overcome:\nSample preparation consistency issues Contact resistance minimization Statistical sampling requirements Technical Developments Graph Theory Applications Local Clustering Analysis Implemented algorithms to identify local structural motifs:\nTriangle counting for local order quantification Clustering coefficient distributions Community detection in disordered regions Mathematical Framework Developed new metrics for partially ordered systems: $$\\mathcal{D}i = \\frac{\\sum{j \\in \\mathcal{N}i} |\\mathbf{r}{ij} - \\mathbf{r}_{ij}^0|}{|\\mathcal{N}_i|}$$\nWhere:\n$\\mathcal{D}_i$: Local disorder at site $i$ $\\mathbf{r}_{ij}$: Actual distance between sites $i$ and $j$ $\\mathbf{r}_{ij}^0$: Ideal distance in ordered phase $\\mathcal{N}_i$: Neighborhood of site $i$ Machine Learning Integration Data Pipeline Autarning from ordered to disordered systems Uncertainty quantification in predictions Experimental Results Network Connectivity Studies Sample: Silver nanowire networks with controlled density **Menetwork formation Solution: Implemented statistical sampling with $N \\geq 30$ samples per condition Result: Reduced uncertainty from ±15% to ±5%\n2. Computational Scaling Problem: Network analysis becomes expensive for large systems ($N \u0026gt; 10^6$ nodes) Solution: Developed hierarchical analysis approach Result: 10x speedup with \u0026lt;2% accuracy loss\n3. Data Integration Problem: Multiple data sources with different formats Solution: Created unified data schema and API Result: Seamless integration of structural, electrical, and computational data\nNext Steps (September 2025) Immediate Priorities Complete nanowire network study with full disorder range Validate machine learning models on experimental data Begin collaboration with computational materials group Medium-term Goals Extend framework to other material systems Develop predictive models for material properties This progress report will be updated monthly. For detailed technical information, please refer to the linked repositories and publications.\n","permalink":"https://Linlin-resh.github.io/posts/progress-2025-08/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eAugust 2025 has been a productive month with significant progress across multiple research areas. This post summarizes key developments, challenges encountered, and next steps for the coming months.\u003c/p\u003e\n\u003ch2 id=\"major-achievements\"\u003eMajor Achievements\u003c/h2\u003e\n\u003ch3 id=\"1-network-analysis-framework\"\u003e1. Network Analysis Framework\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eCompleted\u003c/strong\u003e: Developed a comprehensive framework for analyzing partially disordered networks in materials.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eKey Components\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e**Graph cbetween structure and electrical properties\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eChallenges Overcome\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSample preparation consistency issues\u003c/li\u003e\n\u003cli\u003eContact resistance minimization\u003c/li\u003e\n\u003cli\u003eStatistical sampling requirements\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"technical-developments\"\u003eTechnical Developments\u003c/h2\u003e\n\u003ch3 id=\"graph-theory-applications\"\u003eGraph Theory Applications\u003c/h3\u003e\n\u003ch4 id=\"local-clustering-analysis\"\u003eLocal Clustering Analysis\u003c/h4\u003e\n\u003cp\u003eImplemented algorithms to identify local structural motifs:\u003c/p\u003e","title":"Research Progress Update: August 2025"},{"content":"Introduction This post provides practical code snippets for analyzing structural properties of graphs, particularly useful for materials science applications. These tools help identify structural motifs, quantify disorder, and analyze connectivity patterns.\nEssential Graph Analysis Functions 1. Structural Motif Detection Triangle Counting import networkx as nx import numpy as np from collections import defaultdict def count_triangles_by_node(G): \u0026#34;\u0026#34;\u0026#34;Count triangles containing each node\u0026#34;\u0026#34;\u0026#34; triangles = defaultdict(int) for node in G.nodes(): neighbors = list(G.neighbors(node)) for i in range(len(neighbors)): for j in range(i+1, len(neighbors)): if G.has_edge(neighbors[i], neighbors[j]): triangles[node] += 1 triangles[neighbors[i]] += 1 triangles[neighbors[j]] += 1 return dict(triangles) def clustering_coefficient_distribution(G): \u0026#34;\u0026#34;\u0026#34;Calculate clustering coefficient distribution\u0026#34;\u0026#34;\u0026#34; clustering = nx.clustering(G) values = list(clustering.values()) return { \u0026#39;mean\u0026#39;: np.mean(values), \u0026#39;std\u0026#39;: np.std(values), \u0026#39;min\u0026#39;: np.min(values), \u0026#39;max\u0026#39;: np.max(values), \u0026#39;distribution\u0026#39;: values } Clique Detection def find_maximal_cliques(G, min_size=3): \u0026#34;\u0026#34;\u0026#34;Find maximal cliques of given minimum size\u0026#34;\u0026#34;\u0026#34; cliques = [] for clique in nx.find_cliques(G): if len(clique) \u0026gt;= min_size: cliques.append(clique) return sorted(cliques, key=len, reverse=True) def clique_size_distribution(G): \u0026#34;\u0026#34;\u0026#34;Analyze distribution of clique sizes\u0026#34;\u0026#34;\u0026#34; clique_sizes = [len(clique) for clique in nx.find_cliques(G)] if not clique_sizes: return {\u0026#39;counts\u0026#39;: {}, \u0026#39;total\u0026#39;: 0} size_counts = defaultdict(int) for size in clique_sizes: size_counts[size] += 1 return { \u0026#39;counts\u0026#39;: dict(size_counts), \u0026#39;total\u0026#39;: len(clique_sizes), \u0026#39;max_size\u0026#39;: max(clique_sizes), \u0026#39;avg_size\u0026#39;: np.mean(clique_sizes) } 2. Disorder Quantification Local Order Parameters def calculate_local_order(G, reference_distances=None): \u0026#34;\u0026#34;\u0026#34;Calcul actual_distances.append(2.0) # Disconnected neighbors # Calculate reference distances (ideal ordered structure) if reference_distances is None: # Assume ideal structure has all neighbors connected reference_distances = [1.0] * len(actual_distances) # Calculat def global_disorder_metric(G): \u0026#34;\u0026#34;\u0026#34;Calculate global disorder metric for the entire graph\u0026#34;\u0026#34;\u0026#34; local_order = calculate_local_order(G) return np.mean(list(local_order.values())) Entropy-Based Disorder from scipy.stats import entropy def degree_entropy(G): \u0026#34;\u0026#34;\u0026#34;Calculate entropy of degree distribution as disorder measure\u0026#34;\u0026#34;\u0026#34; degree_sequence = [d for n, d in G.degree()] degree_counts = defaultdict(int) for degree in degree_sequence: degree_counts[degree] += 1 # Normalize to get probabilities total_nodes = len(G.nodes()) probabilities = [count/total_nodes for count in degree_counts.values()] # Bin the values for entropy calculation hist, _ = np.histogram(values, bins=20, density=True) hist = hist[hist \u0026gt; 0] # Remove zero bins return entropy(hist) 3. Connectivity Analysis Percolation Analysis def percolation_analysis(G, removal_fraction=0.1): \u0026#34;\u0026#34;\u0026#34;Analyze percolation behavior under random node removal\u0026#34;\u0026#34;\u0026#34; n_nodes = len(G.nodes()) n_remove = int(n_nodes * removal_fraction) # Randomly remove nodes nodes_to_remove = np.random.choice(list(G.nodes()), n_remove, replace=False) G_temp = G.copy() G_temp.remove_nodes_from(nodes_to_remove) # Analyze connectivity components = list(nx.connected_components(G_temp)) largest_component = max(components, key=len) return {= percolation_analysis(G, threshold) component_sizes.append(result[\u0026#39;largest_component_fraction\u0026#39;]) results.append({ \u0026#39;threshold\u0026#39;: threshold, \u0026#39;mean_size\u0026#39;: np.mean(component_sizes), \u0026#39;std_size\u0026#39;: np.std(component_sizes) }) return results 4. Visualization Tools Structural Motif Visualization import matplotlib.pyplot as plt def plot_triangle_distribution(G): \u0026#34;\u0026#34;\u0026#34;Plot distribution of triangles per node\u0026#34;\u0026#34;\u0026#34; triangles = count_triangles_by_node(G) values = list(triangles.values()) plt.figure(figsize=(10, 6)) plt.hist(values, bins=20, alpha=0.7, edgecolor=\u0026#39;black\u0026#39;) plt.xlabel(\u0026#39;Number of Triangles\u0026#39;) plt.\u0026#39;Clustering Coefficient Distribution\u0026#39;) plt.grid(True, alpha=0.3) plt.show() return { \u0026#39;mean\u0026#39;: np.mean(values), \u0026#39;median\u0026#39;: np.median(values), \u0026#39;std\u0026#39;: np.std(values) } Complete Analysis Pipeline Materials Network Analyzer class MaterialsNetworkAnalyzer: \u0026#34;\u0026#34;\u0026#34;Comprehensive analyzer for materials networks\u0026#34;\u0026#34;\u0026#34; def __init__(self, G): self.G = G self.results = {} def run_full_analysis(self): \u0026#34;\u0026#34;\u0026#34;Run complete structural analysis\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Running structural analysis...\u0026#34;) # Basic print(\u0026#34;Analysis complete!\u0026#34;) return self.results def generate_report(self): \u0026#34;\u0026#34;\u0026#34;Generate summary report\u0026#34;\u0026#34;\u0026#34; if not self.results: print(\u0026#34;Run analysis first!\u0026#34;) return print(\u0026#34;=== MATERIALS NETWORK ANALYSIS REPORT ===\\n\u0026#34;) # Basic statistics basic = self.results[\u0026#39;basic\u0026#39;] print(f\u0026#34;Network Size: {basic[\u0026#39;nodes\u0026#39;]} nodes, {basic[\u0026#39;edges\u0026#39;]} edges\u0026#34;) print(f\u0026#34;Density: {basic[\u0026#39;density\u0026#39;]:.4f}\u0026#34;) print(f\u0026#34;Average Degree: {basic[\u0026#39;average_degree\u0026#39;]:.2f}\u0026#34;) # Structural properties clustering = self.results[\u0026#39;clustering\u0026#39;] print(f\u0026#34;\\nClustering Coefficient: {clustering[\u0026#39;mean\u0026#39;]:.4f} ± {clustering[\u0026#39;std\u0026#39;]:.4f}\u0026#34;) # Disorder metrics print(f\u0026#34;Global Disorder: {self.results[\u0026#39;global_disorder\u0026#39;]:.4f}\u0026#34;) # Run analysis analyzer = MaterialsNetworkAnalyzer(G) results = analyzer.run_full_analysis() # Generate report analyzer.generate_report() Performance Tips Optimization Strategies Use NumPy arrays for large-scale computations Implement caching for repeated calculations Use sparse matrices for large networks Parallelize independent calculations Memory Management def memory_efficient_analysis(G): \u0026#34;\u0026#34;\u0026#34;Memory-efficient analysis for large networks\u0026#34;\u0026#34;\u0026#34; # Process in batches batch_size = 1000 nodes = list(G.nodes()) h_results = analyze_subgraph(batch_subgraph) results.append(batch_results) # Clear memory del batch_subgraph return combine_batch_results(results) Conclusion These tools provide a foundation for structural analysis of materials networks. The key is to choose appropriate metrics based on your specific application and material system.\nRemember: Always validate your analysis with known test cases and experimental data when possible.\nAll code snippets are tested and ready for use. For more advanced features, consider extending these functions with additional NetworkX capabilities.\n","permalink":"https://Linlin-resh.github.io/posts/tool-snippets-structural-gt/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis post provides practical code snippets for analyzing structural properties of graphs, particularly useful for materials science applications. These tools help identify structural motifs, quantify disorder, and analyze connectivity patterns.\u003c/p\u003e\n\u003ch2 id=\"essential-graph-analysis-functions\"\u003eEssential Graph Analysis Functions\u003c/h2\u003e\n\u003ch3 id=\"1-structural-motif-detection\"\u003e1. Structural Motif Detection\u003c/h3\u003e\n\u003ch4 id=\"triangle-counting\"\u003eTriangle Counting\u003c/h4\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003eimport\u003c/span\u003e networkx \u003cspan style=\"color:#ff79c6\"\u003eas\u003c/span\u003e nx\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003eimport\u003c/span\u003e numpy \u003cspan style=\"color:#ff79c6\"\u003eas\u003c/span\u003e np\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003efrom\u003c/span\u003e collections \u003cspan style=\"color:#ff79c6\"\u003eimport\u003c/span\u003e defaultdict\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#50fa7b\"\u003ecount_triangles_by_node\u003c/span\u003e(G):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u0026#34;\u0026#34;Count triangles containing each node\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    triangles \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e defaultdict(\u003cspan style=\"color:#8be9fd;font-style:italic\"\u003eint\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003efor\u003c/span\u003e node \u003cspan style=\"color:#ff79c6\"\u003ein\u003c/span\u003e G\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003enodes():\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        neighbors \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elist\u003c/span\u003e(G\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eneighbors(node))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#ff79c6\"\u003efor\u003c/span\u003e i \u003cspan style=\"color:#ff79c6\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003erange\u003c/span\u003e(\u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elen\u003c/span\u003e(neighbors)):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#ff79c6\"\u003efor\u003c/span\u003e j \u003cspan style=\"color:#ff79c6\"\u003ein\u003c/span\u003e \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003erange\u003c/span\u003e(i\u003cspan style=\"color:#ff79c6\"\u003e+\u003c/span\u003e\u003cspan style=\"color:#bd93f9\"\u003e1\u003c/span\u003e, \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elen\u003c/span\u003e(neighbors)):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                \u003cspan style=\"color:#ff79c6\"\u003eif\u003c/span\u003e G\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003ehas_edge(neighbors[i], neighbors[j]):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    triangles[node] \u003cspan style=\"color:#ff79c6\"\u003e+=\u003c/span\u003e \u003cspan style=\"color:#bd93f9\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    triangles[neighbors[i]] \u003cspan style=\"color:#ff79c6\"\u003e+=\u003c/span\u003e \u003cspan style=\"color:#bd93f9\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e                    triangles[neighbors[j]] \u003cspan style=\"color:#ff79c6\"\u003e+=\u003c/span\u003e \u003cspan style=\"color:#bd93f9\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003edict\u003c/span\u003e(triangles)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#50fa7b\"\u003eclustering_coefficient_distribution\u003c/span\u003e(G):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u0026#34;\u0026#34;Calculate clustering coefficient distribution\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    clustering \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e nx\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eclustering(G)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    values \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elist\u003c/span\u003e(clustering\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003evalues())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003ereturn\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;mean\u0026#39;\u003c/span\u003e: np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003emean(values),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;std\u0026#39;\u003c/span\u003e: np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003estd(values),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;min\u0026#39;\u003c/span\u003e: np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003emin(values),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;max\u0026#39;\u003c/span\u003e: np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003emax(values),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;distribution\u0026#39;\u003c/span\u003e: values\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4 id=\"clique-detection\"\u003eClique Detection\u003c/h4\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#50fa7b\"\u003efind_maximal_cliques\u003c/span\u003e(G, min_size\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#bd93f9\"\u003e3\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u0026#34;\u0026#34;Find maximal cliques of given minimum size\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    cliques \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e []\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003efor\u003c/span\u003e clique \u003cspan style=\"color:#ff79c6\"\u003ein\u003c/span\u003e nx\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003efind_cliques(G):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#ff79c6\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elen\u003c/span\u003e(clique) \u003cspan style=\"color:#ff79c6\"\u003e\u0026gt;=\u003c/span\u003e min_size:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            cliques\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eappend(clique)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003ereturn\u003c/span\u003e \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003esorted\u003c/span\u003e(cliques, key\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elen\u003c/span\u003e, reverse\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ff79c6\"\u003eTrue\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#50fa7b\"\u003eclique_size_distribution\u003c/span\u003e(G):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u0026#34;\u0026#34;Analyze distribution of clique sizes\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    clique_sizes \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e [\u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elen\u003c/span\u003e(clique) \u003cspan style=\"color:#ff79c6\"\u003efor\u003c/span\u003e clique \u003cspan style=\"color:#ff79c6\"\u003ein\u003c/span\u003e nx\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003efind_cliques(G)]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#ff79c6\"\u003enot\u003c/span\u003e clique_sizes:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#ff79c6\"\u003ereturn\u003c/span\u003e {\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;counts\u0026#39;\u003c/span\u003e: {}, \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;total\u0026#39;\u003c/span\u003e: \u003cspan style=\"color:#bd93f9\"\u003e0\u003c/span\u003e}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    size_counts \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e defaultdict(\u003cspan style=\"color:#8be9fd;font-style:italic\"\u003eint\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003efor\u003c/span\u003e size \u003cspan style=\"color:#ff79c6\"\u003ein\u003c/span\u003e clique_sizes:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        size_counts[size] \u003cspan style=\"color:#ff79c6\"\u003e+=\u003c/span\u003e \u003cspan style=\"color:#bd93f9\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003ereturn\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;counts\u0026#39;\u003c/span\u003e: \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003edict\u003c/span\u003e(size_counts),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;total\u0026#39;\u003c/span\u003e: \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elen\u003c/span\u003e(clique_sizes),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;max_size\u0026#39;\u003c/span\u003e: \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003emax\u003c/span\u003e(clique_sizes),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;avg_size\u0026#39;\u003c/span\u003e: np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003emean(clique_sizes)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"2-disorder-quantification\"\u003e2. Disorder Quantification\u003c/h3\u003e\n\u003ch4 id=\"local-order-parameters\"\u003eLocal Order Parameters\u003c/h4\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#50fa7b\"\u003ecalculate_local_order\u003c/span\u003e(G, reference_distances\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ff79c6\"\u003eNone\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u0026#34;\u0026#34;Calcul\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f1fa8c\"\u003e                    actual_distances.append(2.0)  # Disconnected neighbors\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f1fa8c\"\u003e        \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f1fa8c\"\u003e        # Calculate reference distances (ideal ordered structure)\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f1fa8c\"\u003e        if reference_distances is None:\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f1fa8c\"\u003e            # Assume ideal structure has all neighbors connected\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f1fa8c\"\u003e            reference_distances = [1.0] * len(actual_distances)\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f1fa8c\"\u003e        \n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f1fa8c\"\u003e        # Calculat\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f1fa8c\"\u003edef global_disorder_metric(G):\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f1fa8c\"\u003e    \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003eCalculate \u003cspan style=\"color:#ff79c6\"\u003eglobal\u003c/span\u003e disorder metric \u003cspan style=\"color:#ff79c6\"\u003efor\u003c/span\u003e the entire graph\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u0026#34;\u0026#34;\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f1fa8c\"\u003e    local_order = calculate_local_order(G)\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#f1fa8c\"\u003e    return np.mean(list(local_order.values()))\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch4 id=\"entropy-based-disorder\"\u003eEntropy-Based Disorder\u003c/h4\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003efrom\u003c/span\u003e scipy.stats \u003cspan style=\"color:#ff79c6\"\u003eimport\u003c/span\u003e entropy\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#50fa7b\"\u003edegree_entropy\u003c/span\u003e(G):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u0026#34;\u0026#34;Calculate entropy of degree distribution as disorder measure\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    degree_sequence \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e [d \u003cspan style=\"color:#ff79c6\"\u003efor\u003c/span\u003e n, d \u003cspan style=\"color:#ff79c6\"\u003ein\u003c/span\u003e G\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003edegree()]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    degree_counts \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e defaultdict(\u003cspan style=\"color:#8be9fd;font-style:italic\"\u003eint\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003efor\u003c/span\u003e degree \u003cspan style=\"color:#ff79c6\"\u003ein\u003c/span\u003e degree_sequence:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        degree_counts[degree] \u003cspan style=\"color:#ff79c6\"\u003e+=\u003c/span\u003e \u003cspan style=\"color:#bd93f9\"\u003e1\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#6272a4\"\u003e# Normalize to get probabilities\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    total_nodes \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elen\u003c/span\u003e(G\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003enodes())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    probabilities \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e [count\u003cspan style=\"color:#ff79c6\"\u003e/\u003c/span\u003etotal_nodes \u003cspan style=\"color:#ff79c6\"\u003efor\u003c/span\u003e count \u003cspan style=\"color:#ff79c6\"\u003ein\u003c/span\u003e degree_counts\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003evalues()]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#6272a4\"\u003e# Bin the values for entropy calculation\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    hist, _ \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003ehistogram(values, bins\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#bd93f9\"\u003e20\u003c/span\u003e, density\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ff79c6\"\u003eTrue\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    hist \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e hist[hist \u003cspan style=\"color:#ff79c6\"\u003e\u0026gt;\u003c/span\u003e \u003cspan style=\"color:#bd93f9\"\u003e0\u003c/span\u003e]  \u003cspan style=\"color:#6272a4\"\u003e# Remove zero bins\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003ereturn\u003c/span\u003e entropy(hist)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"3-connectivity-analysis\"\u003e3. Connectivity Analysis\u003c/h3\u003e\n\u003ch4 id=\"percolation-analysis\"\u003ePercolation Analysis\u003c/h4\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#50fa7b\"\u003epercolation_analysis\u003c/span\u003e(G, removal_fraction\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#bd93f9\"\u003e0.1\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u0026#34;\u0026#34;Analyze percolation behavior under random node removal\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    n_nodes \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elen\u003c/span\u003e(G\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003enodes())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    n_remove \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003eint\u003c/span\u003e(n_nodes \u003cspan style=\"color:#ff79c6\"\u003e*\u003c/span\u003e removal_fraction)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#6272a4\"\u003e# Randomly remove nodes\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    nodes_to_remove \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003erandom\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003echoice(\u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elist\u003c/span\u003e(G\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003enodes()), n_remove, replace\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#ff79c6\"\u003eFalse\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    G_temp \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e G\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003ecopy()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    G_temp\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eremove_nodes_from(nodes_to_remove)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#6272a4\"\u003e# Analyze connectivity\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    components \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elist\u003c/span\u003e(nx\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003econnected_components(G_temp))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    largest_component \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003emax\u003c/span\u003e(components, key\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elen\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003ereturn\u003c/span\u003e {\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e percolation_analysis(G, threshold)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            component_sizes\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eappend(result[\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;largest_component_fraction\u0026#39;\u003c/span\u003e])\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        results\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eappend({\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;threshold\u0026#39;\u003c/span\u003e: threshold,\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;mean_size\u0026#39;\u003c/span\u003e: np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003emean(component_sizes),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;std_size\u0026#39;\u003c/span\u003e: np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003estd(component_sizes)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        })\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003ereturn\u003c/span\u003e results\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch3 id=\"4-visualization-tools\"\u003e4. Visualization Tools\u003c/h3\u003e\n\u003ch4 id=\"structural-motif-visualization\"\u003eStructural Motif Visualization\u003c/h4\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003eimport\u003c/span\u003e matplotlib.pyplot \u003cspan style=\"color:#ff79c6\"\u003eas\u003c/span\u003e plt\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#50fa7b\"\u003eplot_triangle_distribution\u003c/span\u003e(G):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u0026#34;\u0026#34;Plot distribution of triangles per node\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    triangles \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e count_triangles_by_node(G)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    values \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elist\u003c/span\u003e(triangles\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003evalues())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003efigure(figsize\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e(\u003cspan style=\"color:#bd93f9\"\u003e10\u003c/span\u003e, \u003cspan style=\"color:#bd93f9\"\u003e6\u003c/span\u003e))\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003ehist(values, bins\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#bd93f9\"\u003e20\u003c/span\u003e, alpha\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#bd93f9\"\u003e0.7\u003c/span\u003e, edgecolor\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;black\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003exlabel(\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;Number of Triangles\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;Clustering Coefficient Distribution\u0026#39;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003egrid(\u003cspan style=\"color:#ff79c6\"\u003eTrue\u003c/span\u003e, alpha\u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e\u003cspan style=\"color:#bd93f9\"\u003e0.3\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    plt\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eshow()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003ereturn\u003c/span\u003e {\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;mean\u0026#39;\u003c/span\u003e: np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003emean(values),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;median\u0026#39;\u003c/span\u003e: np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003emedian(values),\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;std\u0026#39;\u003c/span\u003e: np\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003estd(values)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    }\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"complete-analysis-pipeline\"\u003eComplete Analysis Pipeline\u003c/h2\u003e\n\u003ch3 id=\"materials-network-analyzer\"\u003eMaterials Network Analyzer\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003eclass\u003c/span\u003e \u003cspan style=\"color:#50fa7b\"\u003eMaterialsNetworkAnalyzer\u003c/span\u003e:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u0026#34;\u0026#34;Comprehensive analyzer for materials networks\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#50fa7b\"\u003e__init__\u003c/span\u003e(\u003cspan style=\"font-style:italic\"\u003eself\u003c/span\u003e, G):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"font-style:italic\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eG \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e G\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"font-style:italic\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eresults \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e {}\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#50fa7b\"\u003erun_full_analysis\u003c/span\u003e(\u003cspan style=\"font-style:italic\"\u003eself\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u0026#34;\u0026#34;Run complete structural analysis\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003eprint\u003c/span\u003e(\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;Running structural analysis...\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#6272a4\"\u003e# Basic\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003eprint\u003c/span\u003e(\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;Analysis complete!\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#ff79c6\"\u003ereturn\u003c/span\u003e \u003cspan style=\"font-style:italic\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eresults\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#50fa7b\"\u003egenerate_report\u003c/span\u003e(\u003cspan style=\"font-style:italic\"\u003eself\u003c/span\u003e):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u0026#34;\u0026#34;Generate summary report\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#ff79c6\"\u003eif\u003c/span\u003e \u003cspan style=\"color:#ff79c6\"\u003enot\u003c/span\u003e \u003cspan style=\"font-style:italic\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eresults:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003eprint\u003c/span\u003e(\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;Run analysis first!\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e            \u003cspan style=\"color:#ff79c6\"\u003ereturn\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003eprint\u003c/span\u003e(\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;=== MATERIALS NETWORK ANALYSIS REPORT ===\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e\\n\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#6272a4\"\u003e# Basic statistics\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        basic \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e \u003cspan style=\"font-style:italic\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eresults[\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;basic\u0026#39;\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003eprint\u003c/span\u003e(\u003cspan style=\"color:#f1fa8c\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;Network Size: \u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e{\u003c/span\u003ebasic[\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;nodes\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f1fa8c\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e nodes, \u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e{\u003c/span\u003ebasic[\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;edges\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f1fa8c\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e edges\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003eprint\u003c/span\u003e(\u003cspan style=\"color:#f1fa8c\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;Density: \u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e{\u003c/span\u003ebasic[\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;density\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f1fa8c\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e.4f\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003eprint\u003c/span\u003e(\u003cspan style=\"color:#f1fa8c\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;Average Degree: \u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e{\u003c/span\u003ebasic[\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;average_degree\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f1fa8c\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e.2f\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#6272a4\"\u003e# Structural properties\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        clustering \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e \u003cspan style=\"font-style:italic\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eresults[\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;clustering\u0026#39;\u003c/span\u003e]\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003eprint\u003c/span\u003e(\u003cspan style=\"color:#f1fa8c\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e\\n\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003eClustering Coefficient: \u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e{\u003c/span\u003eclustering[\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;mean\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f1fa8c\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e.4f\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e ± \u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e{\u003c/span\u003eclustering[\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;std\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f1fa8c\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e.4f\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#6272a4\"\u003e# Disorder metrics\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003eprint\u003c/span\u003e(\u003cspan style=\"color:#f1fa8c\"\u003ef\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;Global Disorder: \u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e{\u003c/span\u003e\u003cspan style=\"font-style:italic\"\u003eself\u003c/span\u003e\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eresults[\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#39;global_disorder\u0026#39;\u003c/span\u003e]\u003cspan style=\"color:#f1fa8c\"\u003e:\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e.4f\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e}\u003c/span\u003e\u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u003c/span\u003e)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#6272a4\"\u003e# Run analysis\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    analyzer \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e MaterialsNetworkAnalyzer(G)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    results \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e analyzer\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003erun_full_analysis()\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#6272a4\"\u003e# Generate report\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    analyzer\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003egenerate_report()\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"performance-tips\"\u003ePerformance Tips\u003c/h2\u003e\n\u003ch3 id=\"optimization-strategies\"\u003eOptimization Strategies\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eUse NumPy arrays\u003c/strong\u003e for large-scale computations\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplement caching\u003c/strong\u003e for repeated calculations\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUse sparse matrices\u003c/strong\u003e for large networks\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eParallelize\u003c/strong\u003e independent calculations\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"memory-management\"\u003eMemory Management\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003edef\u003c/span\u003e \u003cspan style=\"color:#50fa7b\"\u003ememory_efficient_analysis\u003c/span\u003e(G):\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#f1fa8c\"\u003e\u0026#34;\u0026#34;\u0026#34;Memory-efficient analysis for large networks\u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#6272a4\"\u003e# Process in batches\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    batch_size \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#bd93f9\"\u003e1000\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    nodes \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003elist\u003c/span\u003e(G\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003enodes())\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    h_results \u003cspan style=\"color:#ff79c6\"\u003e=\u003c/span\u003e analyze_subgraph(batch_subgraph)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        results\u003cspan style=\"color:#ff79c6\"\u003e.\u003c/span\u003eappend(batch_results)\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#6272a4\"\u003e# Clear memory\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e        \u003cspan style=\"color:#ff79c6\"\u003edel\u003c/span\u003e batch_subgraph\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e    \u003cspan style=\"color:#ff79c6\"\u003ereturn\u003c/span\u003e combine_batch_results(results)\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eThese tools provide a foundation for structural analysis of materials networks. The key is to choose appropriate metrics based on your specific application and material system.\u003c/p\u003e","title":"Structural Graph Theory Tools: Code Snippets for Materials Analysis"},{"content":"The Core Idea Traditional Graph Neural Networks (GNNs) excel at capturing global graph structure, but what if we need to focus on local patterns that determine material properties? This post explores adapting Transformer architectures to local graph neighborhoods.\nWhy Local Graph Patterns Matter In Materials Science Local atomic arrangements often determine:\nCrystal structure stability Defect formation mechanisms Phase transition pathways Mechanical properties at nanoscale Cegrity Preserve edge weights and node features Positional Encoding for Graphs\nUse graph Laplacian eigenvectors as positional encoding Encode relative positions within local neighborhoods Handle variable neighborhood sizes Multi-Head Self-Attention\nApply attention within local neighborhoods Capture local structural motifs Weight interactions based on geometric relationships Mathematical Formulation For a node $v$ with local neighborhood $\\mathcal{N}_v$:\n$$\\text{Attention}(Q_v, K_{\\mathcal{N}v}, V{\\mathcal{N}v}) = \\text{softmax}\\left(\\frac{Q_v K{\\mathcal{N}v}^T}{\\sqrt{d_k}}\\right) V{\\mathcal{N}_v}$$\nWhere:\n$Q_v$: Query vector for node $v$ $K_{\\mathcal{N}v}, V{\\mathcal{N}_v}$: Key and Value matrices for neighborhood $d_k$: Dimension of key vectors Implementation Sketch import torch import torch.nn as nn import torch.nn.functional as F class LocalGraphTransformer(nn.Module): def __init__(self, d_model, n_heads, d_ff, max_neighbors=20): super().__init__() self. # Residual connection and normalization attended = self.norm1(local_features + attended) # Feed-forward network ffn_out = self.ffn(attended) # Final residual connection return self.norm2(attended + ffn_out) Potential Applications 1. Silver Nanowire Networks Local connectivity patterns affect electrical properties Defect clustering influences mechanical strength Interface structures determine thermal conductivity 2. Partially Disordered Materials Local order parameters in disordered regions Phase boundary structures and dynamics Defect-defect interactions at atomic scale ** (e.g., crystal defects) Compare with existing GNNs on local pattern recognition Scale to larger systems if promising Conclusion Adapting Transformer architectures to local graph structures could unlock new capabilities in materials science. The key insight is that local attention mechanisms might capture atomic-scale interactions better than global message passing.\nThis is a research idea in development. Feedback and collaboration welcome!\n","permalink":"https://Linlin-resh.github.io/posts/idea-transformer-for-local-graph/","summary":"\u003ch2 id=\"the-core-idea\"\u003eThe Core Idea\u003c/h2\u003e\n\u003cp\u003eTraditional \u003cstrong\u003eGraph Neural Networks (GNNs)\u003c/strong\u003e excel at capturing global graph structure, but what if we need to focus on \u003cstrong\u003elocal patterns\u003c/strong\u003e that determine material properties? This post explores adapting \u003cstrong\u003eTransformer architectures\u003c/strong\u003e to local graph neighborhoods.\u003c/p\u003e\n\u003ch2 id=\"why-local-graph-patterns-matter\"\u003eWhy Local Graph Patterns Matter\u003c/h2\u003e\n\u003ch3 id=\"in-materials-science\"\u003eIn Materials Science\u003c/h3\u003e\n\u003cp\u003eLocal atomic arrangements often determine:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCrystal structure\u003c/strong\u003e stability\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDefect formation\u003c/strong\u003e mechanisms\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePhase transition\u003c/strong\u003e pathways\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMechanical properties\u003c/strong\u003e at nanoscale\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"cegrity\"\u003eCegrity\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ePreserve edge weights and node features\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePositional Encoding for Graphs\u003c/strong\u003e\u003c/p\u003e","title":"Transformer Architecture for Local Graph Structures: A Research Idea"},{"content":"404 - Page Not Found Sorry, the page you\u0026rsquo;re looking for doesn\u0026rsquo;t exist.\nWhat happened? The page you requested could not be found. This might be because:\nThe URL was mistyped The page has been moved or deleted You followed a broken link What can you do? Go back to the homepage Browse all posts Search the site Check the tags If you believe this is an error, please contact me or open an issue on GitHub.\n","permalink":"https://Linlin-resh.github.io/404/","summary":"Page not found","title":"404 - Page Not Found"}]